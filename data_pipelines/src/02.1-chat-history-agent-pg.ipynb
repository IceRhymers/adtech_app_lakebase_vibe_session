{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf63ecd-9099-40b4-aa3a-0700f2576e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: databricks-sdk==0.61.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (0.61.0)\nRequirement already satisfied: pyarrow<20 in /databricks/python3/lib/python3.11/site-packages (14.0.1)\nRequirement already satisfied: databricks-agents==1.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (1.2.0)\nRequirement already satisfied: mlflow<=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (3.1.0)\nRequirement already satisfied: databricks-vectorsearch==0.57 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (0.57)\nRequirement already satisfied: langchain==0.3.27 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (0.3.27)\nRequirement already satisfied: langchain_core==0.3.74 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (0.3.74)\nRequirement already satisfied: databricks-langchain==0.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (0.7.0)\nRequirement already satisfied: bs4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (0.0.2)\nRequirement already satisfied: dotenv in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (0.9.9)\nRequirement already satisfied: psycopg2-binary==2.9.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (2.9.7)\nRequirement already satisfied: pgvector==0.2.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (0.2.5)\nRequirement already satisfied: requests<3,>=2.28.1 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk==0.61.0) (2.31.0)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk==0.61.0) (2.35.0)\nRequirement already satisfied: databricks-connect in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-agents==1.2.0) (16.1.6)\nRequirement already satisfied: dataclasses-json in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-agents==1.2.0) (0.6.7)\nRequirement already satisfied: jinja2>=3.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-agents==1.2.0) (3.1.6)\nRequirement already satisfied: mlflow-skinny<4.0.0,>=3.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-agents==1.2.0) (3.1.0)\nRequirement already satisfied: tenacity>=8.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-agents==1.2.0) (9.1.2)\nRequirement already satisfied: tiktoken>=0.8.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-agents==1.2.0) (0.11.0)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-agents==1.2.0) (4.67.1)\nRequirement already satisfied: urllib3>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-agents==1.2.0) (2.0.7)\nRequirement already satisfied: pydantic>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-agents==1.2.0) (2.11.7)\nRequirement already satisfied: whenever==0.7.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-agents==1.2.0) (0.7.3)\nRequirement already satisfied: boto3>1 in /databricks/python3/lib/python3.11/site-packages (from databricks-agents==1.2.0) (1.34.39)\nRequirement already satisfied: botocore in /databricks/python3/lib/python3.11/site-packages (from databricks-agents==1.2.0) (1.34.39)\nRequirement already satisfied: protobuf<6,>=3.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-vectorsearch==0.57) (4.25.8)\nRequirement already satisfied: deprecation>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-vectorsearch==0.57) (2.1.0)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from langchain==0.3.27) (0.3.9)\nRequirement already satisfied: langsmith>=0.1.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from langchain==0.3.27) (0.4.14)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from langchain==0.3.27) (2.0.43)\nRequirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.3.27) (6.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from langchain_core==0.3.74) (1.33)\nRequirement already satisfied: typing-extensions>=4.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from langchain_core==0.3.74) (4.14.1)\nRequirement already satisfied: packaging>=23.2 in /databricks/python3/lib/python3.11/site-packages (from langchain_core==0.3.74) (23.2)\nRequirement already satisfied: databricks-ai-bridge>=0.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-langchain==0.7.0) (0.7.0)\nRequirement already satisfied: openai>=1.99.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-langchain==0.7.0) (1.99.9)\nRequirement already satisfied: unitycatalog-langchain>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (0.2.0)\nRequirement already satisfied: numpy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from pgvector==0.2.5) (1.26.4)\nRequirement already satisfied: ipython<10,>=8 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk[notebook]) (8.25.0)\nRequirement already satisfied: ipywidgets<9,>=8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-sdk[notebook]) (8.1.7)\nRequirement already satisfied: Flask<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from mlflow<=3.1) (3.1.1)\nRequirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from mlflow<=3.1) (1.16.4)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from mlflow<=3.1) (7.1.0)\nRequirement already satisfied: graphene<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from mlflow<=3.1) (3.4.3)\nRequirement already satisfied: gunicorn<24 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from mlflow<=3.1) (23.0.0)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (3.7.2)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (1.5.3)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (1.3.0)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (1.11.1)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (8.2.1)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (3.0.0)\nRequirement already satisfied: fastapi<1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (0.116.1)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (6.0.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (1.36.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (1.36.0)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (0.5.1)\nRequirement already satisfied: uvicorn<1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (0.35.0)\nRequirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.11/site-packages (from mlflow[databricks]) (12.14.0)\nRequirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow[databricks]) (2.18.2)\nRequirement already satisfied: beautifulsoup4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from bs4) (4.13.4)\nRequirement already satisfied: python-dotenv in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from dotenv) (1.1.1)\nRequirement already satisfied: Mako in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow<=3.1) (1.3.10)\nRequirement already satisfied: azure-core<2.0.0,>=1.28.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (1.32.0)\nRequirement already satisfied: azure-storage-blob<13.0.0,>=12.19.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (12.19.1)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (0.7.2)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.11/site-packages (from boto3>1->databricks-agents==1.2.0) (0.10.0)\nRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.11/site-packages (from boto3>1->databricks-agents==1.2.0) (0.10.3)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.11/site-packages (from botocore->databricks-agents==1.2.0) (2.8.2)\nRequirement already satisfied: tabulate>=0.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-ai-bridge>=0.7.0->databricks-langchain==0.7.0) (0.9.0)\nRequirement already satisfied: googleapis-common-protos>=1.56.4 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (1.65.0)\nRequirement already satisfied: grpcio-status>=1.59.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (1.62.3)\nRequirement already satisfied: grpcio>=1.59.3 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (1.69.0)\nRequirement already satisfied: py4j==0.10.9.7 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (0.10.9.7)\nRequirement already satisfied: setuptools>=68.0.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (75.1.0)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from databricks-connect->databricks-agents==1.2.0) (1.16.0)\nRequirement already satisfied: langchain-openai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0) (0.3.30)\nRequirement already satisfied: httpx in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0) (0.28.1)\nRequirement already satisfied: blinker>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from Flask<4->mlflow<=3.1) (1.9.0)\nRequirement already satisfied: itsdangerous>=2.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from Flask<4->mlflow<=3.1) (2.2.0)\nRequirement already satisfied: markupsafe>=2.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from Flask<4->mlflow<=3.1) (3.0.2)\nRequirement already satisfied: werkzeug>=3.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from Flask<4->mlflow<=3.1) (3.1.3)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.61.0) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.61.0) (4.9)\nRequirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.18.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.4.1)\nRequirement already satisfied: google-resumable-media>=2.7.2 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (1.6.0)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from graphene<4->mlflow<=3.1) (3.2.6)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from graphene<4->mlflow<=3.1) (3.2.0)\nRequirement already satisfied: decorator in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.18.1)\nRequirement already satisfied: matplotlib-inline in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (3.0.48)\nRequirement already satisfied: pygments>=2.4.0 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (2.15.1)\nRequirement already satisfied: stack-data in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.2.0)\nRequirement already satisfied: traitlets>=5.13.0 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (5.13.0)\nRequirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (4.8.0)\nRequirement already satisfied: comm>=0.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from ipywidgets<9,>=8->databricks-sdk[notebook]) (0.2.3)\nRequirement already satisfied: widgetsnbextension~=4.0.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from ipywidgets<9,>=8->databricks-sdk[notebook]) (4.0.14)\nRequirement already satisfied: jupyterlab_widgets~=3.0.15 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from ipywidgets<9,>=8->databricks-sdk[notebook]) (3.0.15)\nRequirement already satisfied: jsonpointer>=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain_core==0.3.74) (3.0.0)\nRequirement already satisfied: orjson>=3.9.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (3.11.2)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.23.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (10.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (3.0.9)\nRequirement already satisfied: anyio<5,>=3.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from openai>=1.99.9->databricks-langchain==0.7.0) (4.10.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.9->databricks-langchain==0.7.0) (1.7.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from openai>=1.99.9->databricks-langchain==0.7.0) (0.10.0)\nRequirement already satisfied: sniffio in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from openai>=1.99.9->databricks-langchain==0.7.0) (1.3.1)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas<3->mlflow<=3.1) (2022.7)\nRequirement already satisfied: annotated-types>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from pydantic>=2->databricks-agents==1.2.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from pydantic>=2->databricks-agents==1.2.0) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from pydantic>=2->databricks-agents==1.2.0) (0.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (2023.7.22)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow<=3.1) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow<=3.1) (2.2.0)\nRequirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.27) (3.2.4)\nRequirement already satisfied: regex>=2022.1.18 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from tiktoken>=0.8.0->databricks-agents==1.2.0) (2025.7.34)\nRequirement already satisfied: langchain-community>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (0.3.27)\nRequirement already satisfied: unitycatalog-ai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (0.3.1)\nRequirement already satisfied: soupsieve>1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.7)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from dataclasses-json->databricks-agents==1.2.0) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from dataclasses-json->databricks-agents==1.2.0) (0.9.0)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow[databricks]) (41.0.3)\nRequirement already satisfied: starlette<0.48.0,>=0.40.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from fastapi<1->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (0.47.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (4.0.11)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=1.30.0->mlflow[databricks]) (1.25.0)\nRequirement already satisfied: httpcore==1.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from httpx->databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from httpcore==1.*->httpx->databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0) (0.16.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (3.11.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /databricks/python3/lib/python3.11/site-packages (from jedi>=0.16->ipython<10,>=8->databricks-sdk[notebook]) (0.8.3)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (3.12.15)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (2.10.1)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (0.4.1)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (0.57b0)\nRequirement already satisfied: ptyprocess>=0.5 in /databricks/python3/lib/python3.11/site-packages (from pexpect>4.3->ipython<10,>=8->databricks-sdk[notebook]) (0.7.0)\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython<10,>=8->databricks-sdk[notebook]) (0.2.5)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk==0.61.0) (0.4.8)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->databricks-agents==1.2.0) (0.4.3)\nRequirement already satisfied: executing in /databricks/python3/lib/python3.11/site-packages (from stack-data->ipython<10,>=8->databricks-sdk[notebook]) (0.8.3)\nRequirement already satisfied: asttokens in /databricks/python3/lib/python3.11/site-packages (from stack-data->ipython<10,>=8->databricks-sdk[notebook]) (2.0.5)\nRequirement already satisfied: pure-eval in /databricks/python3/lib/python3.11/site-packages (from stack-data->ipython<10,>=8->databricks-sdk[notebook]) (0.2.2)\nRequirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.11/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (1.5.6)\nRequirement already satisfied: unitycatalog-client in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (0.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (1.20.1)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow[databricks]) (1.15.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (5.0.1)\nRequirement already satisfied: aiohttp-retry>=2.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages (from unitycatalog-client->unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (2.9.1)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow[databricks]) (2.21)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install 'databricks-sdk==0.61.0' 'pyarrow<20' 'databricks-sdk[notebook]' 'databricks-agents==1.2.0' 'mlflow<=3.1' 'mlflow[databricks]' 'databricks-vectorsearch==0.57' 'langchain==0.3.27' 'langchain_core==0.3.74' 'databricks-langchain==0.7.0' 'bs4' 'dotenv' 'psycopg2-binary==2.9.7' 'pgvector==0.2.5'\n",
    "import os\n",
    "if os.environ.get(\"DATABRICKS_RUNTIME_VERSION\"):\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354c76a9-8be0-4c8d-9983-9e57eb961e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"embedding_model\", \"databricks-gte-large-en\")\n",
    "dbutils.widgets.text(\"database_instance_name\", \"tannerw-adtech-db\")\n",
    "dbutils.widgets.text(\"postgres_database_name\", \"databricks_postgres\")\n",
    "dbutils.widgets.text(\"llm_model_serving_endpoint_name\", \"databricks-claude-3-7-sonnet\")\n",
    "dbutils.widgets.text(\"target_catalog\", \"tanner_wendland\")\n",
    "dbutils.widgets.text(\"target_schema\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99bd51e-5492-4d5d-a971-0ddda14452cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = dbutils.widgets.get(\"embedding_model\")\n",
    "database_instance_name = dbutils.widgets.get(\"database_instance_name\")\n",
    "postgres_database_name = dbutils.widgets.get(\"postgres_database_name\")\n",
    "llm_model_serving_endpoint_name = dbutils.widgets.get(\"llm_model_serving_endpoint_name\")\n",
    "target_catalog = dbutils.widgets.get(\"target_catalog\")\n",
    "target_schema = dbutils.widgets.get(\"target_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "188fb341-aa4b-4717-b2f3-12ef5f50a65c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import os\n",
    "from typing import Optional\n",
    "import uuid\n",
    "\n",
    "host = os.environ.get(\"DATABRICKS_HOST\")\n",
    "token = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "workspace_client = WorkspaceClient(host=host, token=token)\n",
    "\n",
    "def get_postgres_connection(\n",
    "    client: WorkspaceClient,\n",
    "    db_name: str,\n",
    "    database_name: Optional[str] = \"databricks_postgres\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get PostgreSQL connection string using Databricks SDK.\n",
    "\n",
    "    Args:\n",
    "        client (WorkspaceClient): The Databricks workspace client.\n",
    "        db_name (str): The name of the database instance.\n",
    "        database_name (Optional[str], optional): The name of the database to connect to.\n",
    "            Defaults to \"databricks_postgres\".\n",
    "\n",
    "    Returns:\n",
    "        str: PostgreSQL connection string.\n",
    "    \"\"\"\n",
    "    database = client.database.get_database_instance(db_name)\n",
    "    credentials = client.database.generate_database_credential(\n",
    "        instance_names=[db_name],\n",
    "        request_id=str(uuid.uuid4())\n",
    "    )\n",
    "\n",
    "    # Use POSTGRES_GROUP env var as username if set, otherwise use current user\n",
    "    postgres_group = os.getenv('POSTGRES_GROUP')\n",
    "    username = postgres_group if postgres_group else client.current_user.me().user_name\n",
    "\n",
    "    database_info = {\n",
    "        \"host\": database.read_write_dns,\n",
    "        \"port\": \"5432\",\n",
    "        \"database\": database_name,\n",
    "        \"username\": username,\n",
    "        \"password\": credentials.token,\n",
    "        \"ssl_mode\": \"require\"\n",
    "    }\n",
    "\n",
    "    database_url = (\n",
    "        f\"postgresql://{database_info['username']}:{database_info['password']}\"\n",
    "        f\"@{database_info['host']}:{database_info['port']}/\"\n",
    "        f\"{database_info['database']}?sslmode={database_info['ssl_mode']}\"\n",
    "    )\n",
    "\n",
    "    return database_url\n",
    "\n",
    "database_url = get_postgres_connection(workspace_client, database_instance_name, postgres_database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d115d39-47b3-40a6-b143-e4ae3f30c1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585760d5-cc0a-4a30-b069-1b57188e8623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9d15d1-397a-4cdf-b948-41a6ad5555f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain_config = {\n",
    "    \"llm_model_serving_endpoint_name\": llm_model_serving_endpoint_name,\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"database_instance_name\": database_instance_name,\n",
    "    \"postgres_database_name\": postgres_database_name,\n",
    "    \"llm_prompt_template\": \"\"\"You are an assistant that answers questions. You have access to a vector search tool that searches previous conversations, but you should primarily use the immediate conversation context provided.\n",
    "\n",
    "Current conversation context: {context}\n",
    "\n",
    "Instructions:\n",
    "1. First, use the current conversation context above to answer questions\n",
    "2. Only use the vector search tool if the question explicitly refers to information from previous conversations that is NOT in the current context\n",
    "3. If the current conversation context contains sufficient information to answer the question, do NOT use the vector search tool\n",
    "4. The vector search tool should be used sparingly, only when the user is clearly asking about something from their chat history that isn't in the current conversation\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99472b6-acef-49b6-aafd-a5260d654768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Vector Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a545c6a-6753-4240-b196-2b284d419534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "retriever_config = {\n",
    "    \"parameters\": {\n",
    "        \"k\": 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9afa93-e6f1-4616-8b92-b99fa8f143af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "# combine dynamic and static filters for vector search\n",
    "def create_configurable_with_filters(input: Dict, retriever_config: Dict) -> Dict:\n",
    "   \"\"\"\n",
    "   create configurable object with filters.\n",
    "   Args:\n",
    "       input: The input data containing filters.\n",
    "   Returns:\n",
    "       A configurable object with filters added to the search_kwargs.\n",
    "   \"\"\"\n",
    "   if \"custom_inputs\" in input:\n",
    "       filters = input[\"custom_inputs\"][\"filters\"]\n",
    "   else:\n",
    "       filters = {}\n",
    "   print(filters)\n",
    "   configurable = {\n",
    "       \"configurable\": {\n",
    "           \"search_kwargs\": {\n",
    "               \"k\": retriever_config.get(\"parameters\")[\"k\"],\n",
    "               \"filter\": filters\n",
    "           }\n",
    "       }\n",
    "   }\n",
    "   return configurable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bfb5894-e178-45c4-8874-b882e3f46b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9c02e33-0b5b-402b-b659-041490a9c82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, List\n",
    "from databricks_langchain import DatabricksEmbeddings\n",
    "from sqlalchemy import create_engine, text, event\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "# Use psycopg2 driver explicitly for SQLAlchemy\n",
    "database_url_sqlalchemy = database_url.replace(\"postgresql://\", \"postgresql+psycopg2://\")\n",
    "\n",
    "engine = create_engine(database_url_sqlalchemy, pool_pre_ping=True)\n",
    "\n",
    "@event.listens_for(engine, \"connect\")\n",
    "def _register_vector(dbapi_connection, connection_record):\n",
    "    # Register pgvector adapter for psycopg2 so Python lists map to the vector type\n",
    "    register_vector(dbapi_connection)\n",
    "\n",
    "def pg_vector_similarity_search(\n",
    "    embeddings: DatabricksEmbeddings,\n",
    "    query: str,\n",
    "    k: int = 3,\n",
    "    filters: Optional[Dict[str, Any]] = None,\n",
    ") -> str:\n",
    "    # 1) Embed the query\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    # 2) Build where clause from filters\n",
    "    filters = filters or {}\n",
    "    where_conditions: List[str] = []\n",
    "    params: Dict[str, Any] = {}\n",
    "\n",
    "    if \"user_name\" in filters:\n",
    "        where_conditions.append(\"me.user_name = :user_name\")\n",
    "        params[\"user_name\"] = filters[\"user_name\"]\n",
    "\n",
    "    if \"chat_id\" in filters:\n",
    "        where_conditions.append(\"me.chat_id = :chat_id\")\n",
    "        params[\"chat_id\"] = filters[\"chat_id\"]\n",
    "\n",
    "    where_clause = \"\"\n",
    "    if where_conditions:\n",
    "        where_clause = \"WHERE \" + \" AND \".join(where_conditions)\n",
    "\n",
    "    # 3) Query using cosine distance operator (<=>); requires pgvector\n",
    "    sql = text(f\"\"\"\n",
    "        SELECT\n",
    "            ch.message_content,\n",
    "            me.user_name,\n",
    "            me.chat_id,\n",
    "            ch.message_type,\n",
    "            ch.created_at,\n",
    "            ch.message_order,\n",
    "            (me.embedding <=> CAST(:query_embedding AS vector)) AS distance\n",
    "        FROM message_embeddings me\n",
    "        JOIN chat_history ch ON me.message_id = ch.id\n",
    "        {where_clause}\n",
    "        ORDER BY me.embedding <=> CAST(:query_embedding AS vector)\n",
    "        LIMIT :k\n",
    "    \"\"\")\n",
    "\n",
    "    # 4) Execute\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(\n",
    "            sql, {\"query_embedding\": query_embedding, \"k\": k, **params}\n",
    "        ).fetchall()\n",
    "\n",
    "    # 5) Format context for the prompt\n",
    "    passages = [f\"Passage: {r.message_content}\" for r in rows]\n",
    "    return \"\\n\".join(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0de4ba-d9b3-4cef-8105-26490de4b9a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from databricks_langchain.chat_models import ChatDatabricks\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "## Load the chain's configuration\n",
    "model_config = mlflow.models.ModelConfig(development_config=chain_config)\n",
    "\n",
    "def pg_vector_search_with_filters(embeddings: DatabricksEmbeddings, query: str, input_data: dict = None) -> str:\n",
    "    input_data = input_data or {}\n",
    "    filters = input_data.get(\"custom_inputs\", {}).get(\"filters\", {})\n",
    "    k = retriever_config[\"parameters\"][\"k\"]\n",
    "    return pg_vector_similarity_search(embeddings=embeddings, query=query, k=k, filters=filters)\n",
    "\n",
    "vector_search_tool = Tool(\n",
    "    name=\"search_chat_history\",\n",
    "    description=\"Retrieve chat history from Postgres (pgvector) for the current user; use only if the immediate conversation context is insufficient.\",\n",
    "    func=lambda q: pg_vector_search_with_filters(q, {}),  # default no filters\n",
    ")\n",
    "\n",
    "# Updated prompt template for tool-calling agent\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", model_config.get('llm_prompt_template')),\n",
    "    (\"user\", \"{question}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Our foundation model answering the final prompt\n",
    "model = ChatDatabricks(\n",
    "    endpoint=model_config.get(\"llm_model_serving_endpoint_name\"),\n",
    "    extra_params={\"temperature\": 0.01, \"max_tokens\": 500}\n",
    ")\n",
    "\n",
    "embeddings = DatabricksEmbeddings(\n",
    "    endpoint=chain_config[\"embedding_model\"],\n",
    "    token=os.environ.get(\"DATABRICKS_TOKEN\")\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(model, [vector_search_tool], prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=[vector_search_tool], verbose=True)\n",
    "\n",
    "\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "def extract_context_string(chat_messages_array):\n",
    "    return '\\n'.join([f\"Role: {message['role']} - Content: {message['content']}\" for message in chat_messages_array[:-1]])\n",
    "\n",
    "# In your rag_chain_with_tool():\n",
    "def rag_chain_with_tool(input_data: dict) -> str:\n",
    "    user_query = extract_user_query_string(input_data[\"messages\"])\n",
    "    def filtered_vector_search(q: str) -> str:\n",
    "        return pg_vector_search_with_filters(embeddings, q, input_data)\n",
    "\n",
    "    filtered_tool = Tool(\n",
    "        name=\"search_chat_history\",\n",
    "        description=\"Retreive chat history from this vector database for the current user, use this tool to answer questions that may refer to previous conversations\",\n",
    "        func=filtered_vector_search,\n",
    "    )\n",
    "\n",
    "    filtered_agent = create_tool_calling_agent(model, [filtered_tool], prompt)\n",
    "    filtered_agent_executor = AgentExecutor(agent=filtered_agent, tools=[filtered_tool], verbose=True)\n",
    "    context = extract_context_string(input_data[\"messages\"])\n",
    "    result = filtered_agent_executor.invoke({\"question\": user_query, \"context\": context})\n",
    "    return result[\"output\"]\n",
    "\n",
    "# Create a runnable version of the chain\n",
    "chain = RunnableLambda(rag_chain_with_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a985e5-e27b-41c8-a705-82ed8d1ebb9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Document Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5e1c30-cb1b-4436-9a03-01882af81c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n\u001B[32;1m\u001B[1;3m\nInvoking: `search_chat_history` with `all`\nresponded: I'll help you look at your chat history. To provide you with relevant information, I need to search your previous conversations.\n\n\u001B[0m\u001B[36;1m\u001B[1;3mPassage: Based on the information in our current conversation, math rock emerged in the late 1980s and early 1990s. It developed as a subgenre of rock music characterized by complex rhythmic structures, atypical time signatures, and intricate instrumental performances.\n\nThe genre evolved from earlier experimental and progressive rock traditions, with bands like Slint being credited as early pioneers of the sound with their influential album \"Spiderland.\"\nPassage: # Math Rock\n\nMath rock is a subgenre of rock music that emerged in the late 1980s and early 1990s. It's characterized by complex rhythmic structures, atypical time signatures, angular melodies, and intricate instrumental performances. Here are some key aspects of math rock:\n\n## Musical Characteristics\n- **Unusual Time Signatures**: Math rock frequently uses odd time signatures like 7/8, 11/8, or 13/8, or rapidly shifts between different time signatures\n- **Complex Rhythms**: Syncopation, polyrhythms, and rhythmic displacements are common\n- **Technical Instrumentation**: Often features clean, intricate guitar work with extensive use of tapping, hammer-ons/pull-offs, and unusual tunings\n- **Dynamic Shifts**: Abrupt changes in tempo, volume, and intensity\n- **Minimal Vocals**: Many math rock bands are primarily instrumental, though some incorporate vocals\n\n## Influential Bands\n- **Slint**: Often credited as early pioneers with their album \"Spiderland\"\n- **Don Caballero**: Known for their technical precision and complex compositions\n- **Battles**: Combining electronic elements with math rock structures\n- **American Football**: Blending math rock with emo sensibilities\n- **Hella**: Known for extremely technical and frenetic playing\n- **Tricot**: Japanese math rock band with more prominent vocal elements\n\nMath rock has influenced and overlaps with other genres like post-rock, progressive rock, and certain styles of indie rock. The genre continues to evolve with newer bands incorporating elements from electronic music, jazz, and other experimental styles.\nPassage: Does it work with their AI/BI product?\u001B[0m\u001B[32;1m\u001B[1;3mBased on your chat history, I can see that you've previously discussed math rock music. Your history contains detailed information about:\n\n1. Math rock as a subgenre that emerged in the late 1980s and early 1990s\n2. Its musical characteristics including:\n   - Unusual time signatures (7/8, 11/8, 13/8)\n   - Complex rhythms and syncopation\n   - Technical instrumentation with intricate guitar work\n   - Dynamic shifts and minimal vocals\n\n3. Influential bands in the genre:\n   - Slint (pioneers with \"Spiderland\")\n   - Don Caballero\n   - Battles\n   - American Football\n   - Hella\n   - Tricot\n\nThe history also shows that math rock has connections to post-rock, progressive rock, and indie rock, and continues to evolve with influences from electronic music, jazz, and other experimental styles.\n\nIs there anything specific about your chat history you'd like to know more about?\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nBased on your chat history, I can see that you've previously discussed math rock music. Your history contains detailed information about:\n\n1. Math rock as a subgenre that emerged in the late 1980s and early 1990s\n2. Its musical characteristics including:\n   - Unusual time signatures (7/8, 11/8, 13/8)\n   - Complex rhythms and syncopation\n   - Technical instrumentation with intricate guitar work\n   - Dynamic shifts and minimal vocals\n\n3. Influential bands in the genre:\n   - Slint (pioneers with \"Spiderland\")\n   - Don Caballero\n   - Battles\n   - American Football\n   - Hella\n   - Tricot\n\nThe history also shows that math rock has connections to post-rock, progressive rock, and indie rock, and continues to evolve with influences from electronic music, jazz, and other experimental styles.\n\nIs there anything specific about your chat history you'd like to know more about?\n"
     ]
    }
   ],
   "source": [
    "input_example_no_filter = {\"messages\": [ {\"role\": \"user\", \"content\": \"Look at the chat history\"}]}\n",
    "answer_no_filter = chain.invoke(input_example_no_filter)\n",
    "print(answer_no_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3300a0-9b8e-40a5-a8fd-fd3ae12ca393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n\u001B[32;1m\u001B[1;3m\nInvoking: `search_chat_history` with `chat history idea`\nresponded: I don't have enough information from our current conversation to know what specific chat history idea you're referring to. To help you better, I'll need to search your previous conversations.\n\n\u001B[0m\u001B[36;1m\u001B[1;3mPassage: # Common Data Structures Used in Audience Segmentation\n\nBuilding on our discussion about audience segmentation technologies, here are the common data structures used in the audience segmentation process:\n\n## User Profiles\n- Individual-level data structures containing attributes and behaviors\n- Often stored as JSON objects or relational database records\n- Include identifiers, demographics, behaviors, and preferences\n\n## Cookies and Device IDs\n- Small text files or unique identifiers stored on user devices\n- Used to recognize users across sessions and websites\n- Examples: first-party cookies, third-party cookies, mobile advertising IDs\n\n## Taxonomies and Hierarchies\n- Organized classification systems for categorizing audience attributes\n- Allow for both broad and granular segmentation\n- Example: Interest categories organized from general to specific\n\n## Segment Definitions\n- Rules-based or algorithmic definitions of audience groups\n- Can be simple (e.g., \"males aged 25-34\") or complex (combining multiple attributes)\n- Often represented as query expressions or decision trees\n\n## Look-alike Models\n- Statistical representations of similarities between users\n- Often implemented as vector spaces or probabilistic models\n- Used to find new users who resemble existing high-value customers\n\n## Cross-Device Graphs\n- Data structures mapping relationships between different devices\n- Enable consistent identification of users across multiple devices\n- Typically implemented as graph databases\n\n## Event Streams\n- Time-series data representing user actions and behaviors\n- Often processed using stream processing frameworks\n- Used for real-time segmentation and targeting\n\n## Hashed User Identifiers\n- Privacy-compliant representations of user IDs\n- Allow for data matching without exposing personally identifiable information\n- Examples: hashed emails, phone numbers, or device IDs\n\nThese data structures form the foundation of audience segmentation systems, enabling marketers to organize, analyze, and activate audience data effectively across the AdTech ecosystem.\nPassage: # Databases Commonly Used in AdTech\n\nBuilding on our previous discussions about audience segmentation technologies and data structures, here are the types of databases commonly used in AdTech use cases:\n\n## Relational Databases\n- Used for structured data with well-defined relationships\n- Good for transaction processing and data integrity\n- Examples: MySQL, PostgreSQL, Oracle\n- AdTech use: Customer information, campaign management, billing systems\n\n## NoSQL Databases\n- Designed for flexible schemas and horizontal scalability\n- Types include document, key-value, column-family, and graph databases\n- Examples: MongoDB (document), Redis (key-value), Cassandra (column-family), Neo4j (graph)\n- AdTech use: User profiles, real-time bidding data, flexible attribute storage\n\n## Data Warehouses\n- Optimized for analytics and reporting on large datasets\n- Support complex queries across massive amounts of data\n- Examples: Google BigQuery, Amazon Redshift, Snowflake\n- AdTech use: Campaign performance analysis, audience insights, attribution modeling\n\n## Time-Series Databases\n- Specialized for time-stamped or time-series data\n- Optimized for measuring changes over time\n- Examples: InfluxDB, TimescaleDB\n- AdTech use: User behavior tracking, campaign performance over time\n\n## In-Memory Databases\n- Store data primarily in RAM for extremely fast access\n- Used for real-time processing needs\n- Examples: Redis, Memcached, SAP HANA\n- AdTech use: Real-time bidding, ad serving decisions, user targeting\n\n## Graph Databases\n- Specialized for managing highly connected data\n- Efficient at traversing relationships between entities\n- Examples: Neo4j, Amazon Neptune\n- AdTech use: Identity resolution, cross-device mapping, interest networks\n\n## Data Lake Technologies\n- Store vast amounts of raw, unprocessed data\n- Support diverse data types and formats\n- Examples: Apache Hadoop, Amazon S3 with Athena\n- AdTech use: Long-term data storage, machine learning training data\n\n## Hybrid and Multi-Model Databases\n- Combine features of different database types\n- Provide flexibility for various data models\n- Examples: ArangoDB\nPassage: Tell me a few things about databricks genie\u001B[0m\u001B[32;1m\u001B[1;3mBased on your previous conversations, I found that you had discussions about audience segmentation technologies and data structures used in AdTech. You explored topics like:\n\n1. Common data structures used in audience segmentation (user profiles, cookies, taxonomies, segment definitions, look-alike models, etc.)\n2. Different types of databases used in AdTech (relational, NoSQL, data warehouses, time-series databases, etc.)\n\nHowever, I don't see a specific \"chat history idea\" mentioned in the retrieved conversations. There was a brief mention of \"databricks genie\" at the end of one passage, but without more context.\n\nIf you're referring to a different idea or if you could provide more details about what specific idea you're looking for, I'd be happy to help further.\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nBased on your previous conversations, I found that you had discussions about audience segmentation technologies and data structures used in AdTech. You explored topics like:\n\n1. Common data structures used in audience segmentation (user profiles, cookies, taxonomies, segment definitions, look-alike models, etc.)\n2. Different types of databases used in AdTech (relational, NoSQL, data warehouses, time-series databases, etc.)\n\nHowever, I don't see a specific \"chat history idea\" mentioned in the retrieved conversations. There was a brief mention of \"databricks genie\" at the end of one passage, but without more context.\n\nIf you're referring to a different idea or if you could provide more details about what specific idea you're looking for, I'd be happy to help further.\n"
     ]
    }
   ],
   "source": [
    "input_example = {\"messages\": [ {\"role\": \"user\", \"content\": \"What was my chat history idea?\"}], \"custom_inputs\": {\"filters\": {\"user_name\": \"tanner.wendland@databricks.com\"}}}\n",
    "answer = chain.invoke(input_example)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247bc64a-7331-4a60-97fe-52d0360bd338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e706a3d-0b01-44e6-95e1-fcaa22ffaf44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/15 15:37:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n\uD83D\uDD17 View Logged Model at: https://e2-demo-field-eng.cloud.databricks.com/ml/experiments/3426381942480258/models/m-0a31cd9abbd44818bd0e2a2848e45927?o=1444828305810485\nFailed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fc78d5eec50> at 0x7fc7809524c0> was created in a different Context\nFailed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fc78d5eec50> at 0x7fc780944240> was created in a different Context\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n\u001B[32;1m\u001B[1;3m\nInvoking: `search_chat_history` with `chat history idea`\nresponded: I don't have enough information from the current conversation context to determine what specific chat history idea you're referring to. To help you better, I'll need to search your previous conversations.\n\n\u001B[0m"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fc78d5eec50> at 0x7fc78096d680> was created in a different Context\nFailed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fc78d5eec50> at 0x7fc780979540> was created in a different Context\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36;1m\u001B[1;3mPassage: # Common Data Structures Used in Audience Segmentation\n\nBuilding on our discussion about audience segmentation technologies, here are the common data structures used in the audience segmentation process:\n\n## User Profiles\n- Individual-level data structures containing attributes and behaviors\n- Often stored as JSON objects or relational database records\n- Include identifiers, demographics, behaviors, and preferences\n\n## Cookies and Device IDs\n- Small text files or unique identifiers stored on user devices\n- Used to recognize users across sessions and websites\n- Examples: first-party cookies, third-party cookies, mobile advertising IDs\n\n## Taxonomies and Hierarchies\n- Organized classification systems for categorizing audience attributes\n- Allow for both broad and granular segmentation\n- Example: Interest categories organized from general to specific\n\n## Segment Definitions\n- Rules-based or algorithmic definitions of audience groups\n- Can be simple (e.g., \"males aged 25-34\") or complex (combining multiple attributes)\n- Often represented as query expressions or decision trees\n\n## Look-alike Models\n- Statistical representations of similarities between users\n- Often implemented as vector spaces or probabilistic models\n- Used to find new users who resemble existing high-value customers\n\n## Cross-Device Graphs\n- Data structures mapping relationships between different devices\n- Enable consistent identification of users across multiple devices\n- Typically implemented as graph databases\n\n## Event Streams\n- Time-series data representing user actions and behaviors\n- Often processed using stream processing frameworks\n- Used for real-time segmentation and targeting\n\n## Hashed User Identifiers\n- Privacy-compliant representations of user IDs\n- Allow for data matching without exposing personally identifiable information\n- Examples: hashed emails, phone numbers, or device IDs\n\nThese data structures form the foundation of audience segmentation systems, enabling marketers to organize, analyze, and activate audience data effectively across the AdTech ecosystem.\nPassage: # Databases Commonly Used in AdTech\n\nBuilding on our previous discussions about audience segmentation technologies and data structures, here are the types of databases commonly used in AdTech use cases:\n\n## Relational Databases\n- Used for structured data with well-defined relationships\n- Good for transaction processing and data integrity\n- Examples: MySQL, PostgreSQL, Oracle\n- AdTech use: Customer information, campaign management, billing systems\n\n## NoSQL Databases\n- Designed for flexible schemas and horizontal scalability\n- Types include document, key-value, column-family, and graph databases\n- Examples: MongoDB (document), Redis (key-value), Cassandra (column-family), Neo4j (graph)\n- AdTech use: User profiles, real-time bidding data, flexible attribute storage\n\n## Data Warehouses\n- Optimized for analytics and reporting on large datasets\n- Support complex queries across massive amounts of data\n- Examples: Google BigQuery, Amazon Redshift, Snowflake\n- AdTech use: Campaign performance analysis, audience insights, attribution modeling\n\n## Time-Series Databases\n- Specialized for time-stamped or time-series data\n- Optimized for measuring changes over time\n- Examples: InfluxDB, TimescaleDB\n- AdTech use: User behavior tracking, campaign performance over time\n\n## In-Memory Databases\n- Store data primarily in RAM for extremely fast access\n- Used for real-time processing needs\n- Examples: Redis, Memcached, SAP HANA\n- AdTech use: Real-time bidding, ad serving decisions, user targeting\n\n## Graph Databases\n- Specialized for managing highly connected data\n- Efficient at traversing relationships between entities\n- Examples: Neo4j, Amazon Neptune\n- AdTech use: Identity resolution, cross-device mapping, interest networks\n\n## Data Lake Technologies\n- Store vast amounts of raw, unprocessed data\n- Support diverse data types and formats\n- Examples: Apache Hadoop, Amazon S3 with Athena\n- AdTech use: Long-term data storage, machine learning training data\n\n## Hybrid and Multi-Model Databases\n- Combine features of different database types\n- Provide flexibility for various data models\n- Examples: ArangoDB\nPassage: Tell me a few things about databricks genie\u001B[0m\u001B[32;1m\u001B[1;3mBased on your previous conversations, I can see you've discussed several technical topics related to AdTech and data structures, but I don't see a specific \"chat history idea\" mentioned in the search results. \n\nThe search results show discussions about:\n1. Common data structures used in audience segmentation\n2. Types of databases commonly used in AdTech\n3. A brief mention of \"databricks genie\"\n\nIf you're referring to a specific idea you shared in a previous conversation that isn't appearing in these results, could you provide more details about what you're looking for? Or perhaps you're referring to one of these topics that we should explore further?\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/15 15:37:38 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-0a31cd9abbd44818bd0e2a2848e45927\n2025/08/15 15:37:38 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\nFailed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fc78d5eec50> at 0x7fc7809eeac0> was created in a different Context\nFailed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fc78d5eec50> at 0x7fc7809bc4c0> was created in a different Context\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n\u001B[32;1m\u001B[1;3m\nInvoking: `search_chat_history` with `chat history idea`\nresponded: I don't have enough information from the current conversation context to determine what specific chat history idea you're referring to. To help you better, I'll need to search your previous conversations.\n\n\u001B[0m"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fc78d5eec50> at 0x7fc780953700> was created in a different Context\nFailed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-611482fe-0d7d-429e-a351-f1c51a31e0fe/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fc78d5eec50> at 0x7fc780b580c0> was created in a different Context\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36;1m\u001B[1;3mPassage: # Common Data Structures Used in Audience Segmentation\n\nBuilding on our discussion about audience segmentation technologies, here are the common data structures used in the audience segmentation process:\n\n## User Profiles\n- Individual-level data structures containing attributes and behaviors\n- Often stored as JSON objects or relational database records\n- Include identifiers, demographics, behaviors, and preferences\n\n## Cookies and Device IDs\n- Small text files or unique identifiers stored on user devices\n- Used to recognize users across sessions and websites\n- Examples: first-party cookies, third-party cookies, mobile advertising IDs\n\n## Taxonomies and Hierarchies\n- Organized classification systems for categorizing audience attributes\n- Allow for both broad and granular segmentation\n- Example: Interest categories organized from general to specific\n\n## Segment Definitions\n- Rules-based or algorithmic definitions of audience groups\n- Can be simple (e.g., \"males aged 25-34\") or complex (combining multiple attributes)\n- Often represented as query expressions or decision trees\n\n## Look-alike Models\n- Statistical representations of similarities between users\n- Often implemented as vector spaces or probabilistic models\n- Used to find new users who resemble existing high-value customers\n\n## Cross-Device Graphs\n- Data structures mapping relationships between different devices\n- Enable consistent identification of users across multiple devices\n- Typically implemented as graph databases\n\n## Event Streams\n- Time-series data representing user actions and behaviors\n- Often processed using stream processing frameworks\n- Used for real-time segmentation and targeting\n\n## Hashed User Identifiers\n- Privacy-compliant representations of user IDs\n- Allow for data matching without exposing personally identifiable information\n- Examples: hashed emails, phone numbers, or device IDs\n\nThese data structures form the foundation of audience segmentation systems, enabling marketers to organize, analyze, and activate audience data effectively across the AdTech ecosystem.\nPassage: # Databases Commonly Used in AdTech\n\nBuilding on our previous discussions about audience segmentation technologies and data structures, here are the types of databases commonly used in AdTech use cases:\n\n## Relational Databases\n- Used for structured data with well-defined relationships\n- Good for transaction processing and data integrity\n- Examples: MySQL, PostgreSQL, Oracle\n- AdTech use: Customer information, campaign management, billing systems\n\n## NoSQL Databases\n- Designed for flexible schemas and horizontal scalability\n- Types include document, key-value, column-family, and graph databases\n- Examples: MongoDB (document), Redis (key-value), Cassandra (column-family), Neo4j (graph)\n- AdTech use: User profiles, real-time bidding data, flexible attribute storage\n\n## Data Warehouses\n- Optimized for analytics and reporting on large datasets\n- Support complex queries across massive amounts of data\n- Examples: Google BigQuery, Amazon Redshift, Snowflake\n- AdTech use: Campaign performance analysis, audience insights, attribution modeling\n\n## Time-Series Databases\n- Specialized for time-stamped or time-series data\n- Optimized for measuring changes over time\n- Examples: InfluxDB, TimescaleDB\n- AdTech use: User behavior tracking, campaign performance over time\n\n## In-Memory Databases\n- Store data primarily in RAM for extremely fast access\n- Used for real-time processing needs\n- Examples: Redis, Memcached, SAP HANA\n- AdTech use: Real-time bidding, ad serving decisions, user targeting\n\n## Graph Databases\n- Specialized for managing highly connected data\n- Efficient at traversing relationships between entities\n- Examples: Neo4j, Amazon Neptune\n- AdTech use: Identity resolution, cross-device mapping, interest networks\n\n## Data Lake Technologies\n- Store vast amounts of raw, unprocessed data\n- Support diverse data types and formats\n- Examples: Apache Hadoop, Amazon S3 with Athena\n- AdTech use: Long-term data storage, machine learning training data\n\n## Hybrid and Multi-Model Databases\n- Combine features of different database types\n- Provide flexibility for various data models\n- Examples: ArangoDB\nPassage: Tell me a few things about databricks genie\u001B[0m"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/15 15:37:45 INFO mlflow.models.model: Found the following environment variables used during model inference: [DATABRICKS_HOST, DATABRICKS_TOKEN]. Please check if you need to set them when deploying the model. To disable this message, set environment variable `MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING` to `false`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3mBased on your previous conversations, I can see you've discussed several technical topics related to AdTech, data structures for audience segmentation, and database technologies. However, I don't see a specific \"chat history idea\" mentioned in the search results.\n\nIf you're referring to a specific idea you shared in a previous conversation, could you provide more details about what you're looking for? For example, was it related to audience segmentation technologies, database structures, or perhaps something about Databricks Genie that was mentioned?\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<head>\n",
       "  <link\n",
       "    rel=\"stylesheet\"\n",
       "    href=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/xcode.min.css\"\n",
       "  />\n",
       "  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js\"></script>\n",
       "  <script>\n",
       "    hljs.highlightAll();\n",
       "  </script>\n",
       "  <style>\n",
       "    body {\n",
       "      margin: 0;\n",
       "      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto,\n",
       "        Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji,\n",
       "        Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;\n",
       "      -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n",
       "      margin: 0;\n",
       "      font-weight: 400;\n",
       "      font-size: 13px;\n",
       "      line-height: 18px;\n",
       "      color: rgb(17, 23, 28);\n",
       "    }\n",
       "    code {\n",
       "      line-height: 18px;\n",
       "      font-size: 11px;\n",
       "      background: rgb(250, 250, 250) !important;\n",
       "    }\n",
       "    pre {\n",
       "      background: rgb(250, 250, 250);\n",
       "      margin: 0;\n",
       "      display: none;\n",
       "    }\n",
       "    pre.active {\n",
       "      display: unset;\n",
       "    }\n",
       "    button {\n",
       "      white-space: nowrap;\n",
       "      text-align: center;\n",
       "      position: relative;\n",
       "      cursor: pointer;\n",
       "      background: rgba(34, 114, 180, 0) !important;\n",
       "      color: rgb(34, 114, 180) !important;\n",
       "      border-color: rgba(34, 114, 180, 0) !important;\n",
       "      padding: 4px 6px !important;\n",
       "      text-decoration: none !important;\n",
       "      line-height: 20px !important;\n",
       "      box-shadow: none !important;\n",
       "      height: 32px !important;\n",
       "      display: inline-flex !important;\n",
       "      -webkit-box-align: center !important;\n",
       "      align-items: center !important;\n",
       "      -webkit-box-pack: center !important;\n",
       "      justify-content: center !important;\n",
       "      vertical-align: middle !important;\n",
       "    }\n",
       "    p {\n",
       "      margin: 0;\n",
       "      padding: 0;\n",
       "    }\n",
       "    button:hover {\n",
       "      background: rgba(34, 114, 180, 0.08) !important;\n",
       "      color: rgb(14, 83, 139) !important;\n",
       "    }\n",
       "    button:active {\n",
       "      background: rgba(34, 114, 180, 0.16) !important;\n",
       "      color: rgb(4, 53, 93) !important;\n",
       "    }\n",
       "    h1 {\n",
       "      margin-top: 4px;\n",
       "      font-size: 22px;\n",
       "    }\n",
       "    .info {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      line-height: 16px;\n",
       "      color: rgb(95, 114, 129);\n",
       "    }\n",
       "    .tabs {\n",
       "      margin-top: 10px;\n",
       "      border-bottom: 1px solid rgb(209, 217, 225) !important;\n",
       "      display: flex;\n",
       "      line-height: 24px;\n",
       "    }\n",
       "    .tab {\n",
       "      font-size: 13px;\n",
       "      font-weight: 600 !important;\n",
       "      cursor: pointer;\n",
       "      margin: 0 24px 0 2px;\n",
       "      padding-left: 2px;\n",
       "    }\n",
       "    .tab:hover {\n",
       "      color: rgb(14, 83, 139) !important;\n",
       "    }\n",
       "    .tab.active {\n",
       "      border-bottom: 3px solid rgb(34, 114, 180) !important;\n",
       "    }\n",
       "    .link {\n",
       "      margin-left: 12px;\n",
       "      display: inline-block;\n",
       "      text-decoration: none;\n",
       "      color: rgb(34, 114, 180) !important;\n",
       "      font-size: 13px;\n",
       "      font-weight: 400;\n",
       "    }\n",
       "    .link:hover {\n",
       "      color: rgb(14, 83, 139) !important;\n",
       "    }\n",
       "    .link-content {\n",
       "      display: flex;\n",
       "      gap: 6px;\n",
       "      align-items: center;\n",
       "    }\n",
       "    .caret-up {\n",
       "      transform: rotate(180deg);\n",
       "    }\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "  <div style=\"display: flex; align-items: center\">\n",
       "    The logged model is compatible with the Mosaic AI Agent Framework.\n",
       "    <button onclick=\"toggleCode()\">\n",
       "      See how to evaluate the model&nbsp;\n",
       "      <span\n",
       "        role=\"img\"\n",
       "        id=\"caret\"\n",
       "        aria-hidden=\"true\"\n",
       "        class=\"anticon css-6xix1i\"\n",
       "        style=\"font-size: 14px\"\n",
       "        ><svg\n",
       "          xmlns=\"http://www.w3.org/2000/svg\"\n",
       "          width=\"1em\"\n",
       "          height=\"1em\"\n",
       "          fill=\"none\"\n",
       "          viewBox=\"0 0 16 16\"\n",
       "          aria-hidden=\"true\"\n",
       "          focusable=\"false\"\n",
       "          class=\"\"\n",
       "        >\n",
       "          <path\n",
       "            fill=\"currentColor\"\n",
       "            fill-rule=\"evenodd\"\n",
       "            d=\"M8 8.917 10.947 6 12 7.042 8 11 4 7.042 5.053 6z\"\n",
       "            clip-rule=\"evenodd\"\n",
       "          ></path>\n",
       "        </svg>\n",
       "      </span>\n",
       "    </button>\n",
       "  </div>\n",
       "  <div id=\"code\" style=\"display: none\">\n",
       "    <h1>\n",
       "      Agent evaluation\n",
       "      <a\n",
       "        class=\"link\"\n",
       "        href=\"https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook\"\n",
       "        target=\"_blank\"\n",
       "      >\n",
       "        <span class=\"link-content\">\n",
       "          Learn more\n",
       "          <span role=\"img\" aria-hidden=\"true\" class=\"anticon css-6xix1i\"\n",
       "            ><svg\n",
       "              xmlns=\"http://www.w3.org/2000/svg\"\n",
       "              width=\"1em\"\n",
       "              height=\"1em\"\n",
       "              fill=\"none\"\n",
       "              viewBox=\"0 0 16 16\"\n",
       "              aria-hidden=\"true\"\n",
       "              focusable=\"false\"\n",
       "              class=\"\"\n",
       "            >\n",
       "              <path\n",
       "                fill=\"currentColor\"\n",
       "                d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"\n",
       "              ></path>\n",
       "              <path\n",
       "                fill=\"currentColor\"\n",
       "                d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"\n",
       "              ></path></svg></span></span\n",
       "      ></a>\n",
       "    </h1>\n",
       "    <p class=\"info\">\n",
       "      Copy the following code snippet in a notebook cell (right click  copy)\n",
       "    </p>\n",
       "    <div class=\"tabs\">\n",
       "      <div class=\"tab active\" onclick=\"tabClicked(0)\">Using synthetic data</div>\n",
       "      <div class=\"tab\" onclick=\"tabClicked(1)\">Using your own dataset</div>\n",
       "    </div>\n",
       "    <div style=\"height: 472px\">\n",
       "      <pre\n",
       "        class=\"active\"\n",
       "      ><code class=\"language-python\">%pip install -U databricks-agents\n",
       "dbutils.library.restartPython()\n",
       "## Run the above in a separate cell ##\n",
       "\n",
       "from databricks.agents.evals import generate_evals_df\n",
       "import mlflow\n",
       "\n",
       "agent_description = &quot;A chatbot that answers questions about Databricks.&quot;\n",
       "question_guidelines = &quot;&quot;&quot;\n",
       "# User personas\n",
       "- A developer new to the Databricks platform\n",
       "# Example questions\n",
       "- What API lets me parallelize operations over rows of a delta table?\n",
       "&quot;&quot;&quot;\n",
       "# TODO: Spark/Pandas DataFrame with &quot;content&quot; and &quot;doc_uri&quot; columns.\n",
       "docs = spark.table(&quot;catalog.schema.my_table_of_docs&quot;)\n",
       "evals = generate_evals_df(\n",
       "    docs=docs,\n",
       "    num_evals=25,\n",
       "    agent_description=agent_description,\n",
       "    question_guidelines=question_guidelines,\n",
       ")\n",
       "eval_result = mlflow.evaluate(data=evals, model=&quot;models:/m-0a31cd9abbd44818bd0e2a2848e45927&quot;, model_type=&quot;databricks-agent&quot;)\n",
       "</code></pre>\n",
       "\n",
       "      <pre><code class=\"language-python\">%pip install -U databricks-agents\n",
       "dbutils.library.restartPython()\n",
       "## Run the above in a separate cell ##\n",
       "\n",
       "import pandas as pd\n",
       "import mlflow\n",
       "\n",
       "evals = [\n",
       "    {\n",
       "        &quot;request&quot;: {\n",
       "            &quot;messages&quot;: [\n",
       "                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How do I convert a Spark DataFrame to Pandas?&quot;}\n",
       "            ],\n",
       "        },\n",
       "        # Optional, needed for judging correctness.\n",
       "        &quot;expected_facts&quot;: [\n",
       "            &quot;To convert a Spark DataFrame to Pandas, you can use the toPandas() method.&quot;\n",
       "        ],\n",
       "    }\n",
       "]\n",
       "eval_result = mlflow.evaluate(\n",
       "    data=pd.DataFrame.from_records(evals), model=&quot;models:/m-0a31cd9abbd44818bd0e2a2848e45927&quot;, model_type=&quot;databricks-agent&quot;\n",
       ")\n",
       "</code></pre>\n",
       "    </div>\n",
       "  </div>\n",
       "  <script>\n",
       "    var codeShown = false;\n",
       "    function clip(el) {\n",
       "      var range = document.createRange();\n",
       "      range.selectNodeContents(el);\n",
       "      var sel = window.getSelection();\n",
       "      sel.removeAllRanges();\n",
       "      sel.addRange(range);\n",
       "    }\n",
       "\n",
       "    function toggleCode() {\n",
       "      if (codeShown) {\n",
       "        document.getElementById(\"code\").style.display = \"none\";\n",
       "        codeShown = false;\n",
       "      } else {\n",
       "        document.getElementById(\"code\").style.display = \"block\";\n",
       "        clip(document.querySelector(\"pre.active\"));\n",
       "        codeShown = true;\n",
       "      }\n",
       "      document.getElementById(\"caret\").classList.toggle(\"caret-up\");\n",
       "    }\n",
       "\n",
       "    function tabClicked(tabIndex) {\n",
       "      document.querySelectorAll(\".tab\").forEach((tab, index) => {\n",
       "        if (index === tabIndex) {\n",
       "          tab.classList.add(\"active\");\n",
       "        } else {\n",
       "          tab.classList.remove(\"active\");\n",
       "        }\n",
       "      });\n",
       "      document.querySelectorAll(\"pre\").forEach((pre, index) => {\n",
       "        if (index === tabIndex) {\n",
       "          pre.classList.add(\"active\");\n",
       "        } else {\n",
       "          pre.classList.remove(\"active\");\n",
       "        }\n",
       "      });\n",
       "      clip(document.querySelector(\"pre.active\"));\n",
       "    }\n",
       "  </script>\n",
       "</body>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'tanner_wendland.default.chat_history_agent_postgres' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7ee16aa59041c1a2e71bacd6c0698b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1dff4cf44b4cd6adbd3b96dbdf679c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 Created version '7' of model 'tanner_wendland.default.chat_history_agent_postgres': https://e2-demo-field-eng.cloud.databricks.com/explore/data/models/tanner_wendland/default/chat_history_agent_postgres/version/7?o=1444828305810485\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models.resources import DatabricksVectorSearchIndex, DatabricksServingEndpoint\n",
    "\n",
    "chain_file_path = os.path.join(os.getcwd(), 'chain_postgres.py')\n",
    "if not os.path.exists(chain_file_path):\n",
    "    raise FileNotFoundError(f\"Chain file not found at {chain_file_path}\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "os.environ['DATABRICKS_HOST'] = f\"https://{workspace_url}\"\n",
    "os.environ['DATABRICKS_TOKEN'] = dbutils.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=\"adtech_chat_history_agent_postgres\"):\n",
    "  logged_chain_info = mlflow.langchain.log_model(\n",
    "          #Note: In classical ML, MLflow works by serializing the model object.  In generative AI, chains often include Python packages that do not serialize.  Here, we use MLflow's new code-based logging, where we saved our chain under the chain notebook and will use this code instead of trying to serialize the object.\n",
    "          lc_model=os.path.join(os.getcwd(), 'chain_postgres.py'),  # Chain code file e.g., /path/to/the/chain.py \n",
    "          model_config=chain_config, # Chain configuration \n",
    "          artifact_path=\"chain_postgres\", # Required by MLflow, the chain's code/config are saved in this directory\n",
    "          input_example=input_example,\n",
    "          # Specify resources for automatic authentication passthrough\n",
    "          resources=[\n",
    "            DatabricksServingEndpoint(endpoint_name=model_config.get(\"llm_model_serving_endpoint_name\"))\n",
    "          ],\n",
    "          pip_requirements=[\n",
    "            \"mlflow==3.1.0\",\n",
    "            \"databricks-agents==1.2.0\",\n",
    "            \"databricks-langchain==0.7.0\",\n",
    "            \"langchain==0.3.27\",\n",
    "            \"pgvector==0.2.5\",\n",
    "            \"psycopg2-binary==2.9.7\",\n",
    "            \"pydantic==2.11.7\",\n",
    "            \"sqlalchemy==2.0.43\",\n",
    "            \"tornado==6.3.2\",\n",
    "          ]\n",
    "      )\n",
    "\n",
    "model_name = \"chat_history_agent_postgres\"\n",
    "MODEL_NAME_FQN = f\"{target_catalog}.{target_schema}.{model_name}\"\n",
    "# Register to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=MODEL_NAME_FQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0141fcd2-95ff-4d57-bfa4-3c55a38f6946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9639d4-1d86-40b3-87b4-b46c8b7be046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"secert_scope\", \"field-eng\", \"Secret Scope\")\n",
    "dbutils.widgets.text(\"secret_key\", \"app-secret\", \"Secret Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93d99ca7-c565-4092-9f28-aff21fcebfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REDACTED]\n"
     ]
    }
   ],
   "source": [
    "secret_scope = dbutils.widgets.get(\"secert_scope\")\n",
    "secret_key = dbutils.widgets.get(\"secret_key\")\n",
    "\n",
    "secret_value = dbutils.secrets.get(scope=secret_scope, key=secret_key)\n",
    "\n",
    "print(str(secret_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbed8a02-2931-4a9f-aa13-e3f2766bf1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating endpoint tanner_wendland-default-chat_history_agent_postgres...\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedEntityInput\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "\n",
    "version = uc_registered_model_info.version\n",
    "serving_endpoint_name = MODEL_NAME_FQN.replace(\".\", \"-\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "config = {\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "                \"name\": serving_endpoint_name,\n",
    "                \"entity_name\": MODEL_NAME_FQN,\n",
    "                \"entity_version\": version,\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": True,\n",
    "                \"environment_vars\": {\n",
    "                    'DATABRICKS_HOST': workspace_url,\n",
    "                    'DATABRICKS_TOKEN': secret_value\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def does_endpoint_exists(endpoint_name):\n",
    "    try:\n",
    "        workspace_client.serving_endpoints.get(endpoint_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if not does_endpoint_exists(serving_endpoint_name):\n",
    "    print(f\"Creating endpoint {serving_endpoint_name}...\")\n",
    "    workspace_client.serving_endpoints.create_and_wait(\n",
    "        serving_endpoint_name,\n",
    "        config=EndpointCoreConfigInput.from_dict(config)\n",
    "    )\n",
    "else:\n",
    "    print(f\"Updating endpoint {serving_endpoint_name}...\")\n",
    "    workspace_client.serving_endpoints.update_config_and_wait(\n",
    "        serving_endpoint_name,\n",
    "        served_entities=[ServedEntityInput.from_dict(entity) for entity in config['served_entities']]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02.1-chat-history-agent-pg",
   "widgets": {
    "database_instance_name": {
     "currentValue": "tannerw-adtech-db",
     "nuid": "5935864b-337f-4cea-978f-00fcf2a24646",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tannerw-adtech-db",
      "label": null,
      "name": "database_instance_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "tannerw-adtech-db",
      "label": null,
      "name": "database_instance_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "embedding_model": {
     "currentValue": "databricks-gte-large-en",
     "nuid": "96111585-d614-405c-960e-ae589ccc89fc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_model",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_model",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "llm_model_serving_endpoint_name": {
     "currentValue": "databricks-claude-3-7-sonnet",
     "nuid": "4e5299b7-86bb-4511-bd44-8d76be56893e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-claude-3-7-sonnet",
      "label": null,
      "name": "llm_model_serving_endpoint_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-claude-3-7-sonnet",
      "label": null,
      "name": "llm_model_serving_endpoint_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "postgres_database_name": {
     "currentValue": "databricks_postgres",
     "nuid": "e75a0daa-dd0b-412d-b393-29edb1e3b1f1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks_postgres",
      "label": null,
      "name": "postgres_database_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks_postgres",
      "label": null,
      "name": "postgres_database_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "secert_scope": {
     "currentValue": "field-eng",
     "nuid": "e227bce8-9119-4ce0-9bd1-7bb37e315a10",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "field-eng",
      "label": "Secret Scope",
      "name": "secert_scope",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "field-eng",
      "label": "Secret Scope",
      "name": "secert_scope",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "secret_key": {
     "currentValue": "app-secret",
     "nuid": "90c15fd3-85b5-4532-9fb1-cd73d8f32360",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "app-secret",
      "label": "Secret Key",
      "name": "secret_key",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "app-secret",
      "label": "Secret Key",
      "name": "secret_key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_catalog": {
     "currentValue": "tanner_wendland",
     "nuid": "9ce71cf8-98af-4e13-8cc0-71dc8c6cf387",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tanner_wendland",
      "label": null,
      "name": "target_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "tanner_wendland",
      "label": null,
      "name": "target_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_schema": {
     "currentValue": "default",
     "nuid": "eb61b685-72df-4bd5-bf4e-80e589c9adb5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "target_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": null,
      "name": "target_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}