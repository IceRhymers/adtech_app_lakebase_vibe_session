{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf63ecd-9099-40b4-aa3a-0700f2576e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks-sdk==0.61.0\n  Downloading databricks_sdk-0.61.0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: pyarrow<20 in /databricks/python3/lib/python3.11/site-packages (14.0.1)\nCollecting databricks-agents==1.2.0\n  Downloading databricks_agents-1.2.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting mlflow<=3.1\n  Downloading mlflow-3.1.0-py3-none-any.whl.metadata (29 kB)\nCollecting databricks-vectorsearch==0.57\n  Downloading databricks_vectorsearch-0.57-py3-none-any.whl.metadata (2.8 kB)\nCollecting langchain\n  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\nCollecting langchain_core\n  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\nCollecting databricks-langchain\n  Downloading databricks_langchain-0.7.0-py3-none-any.whl.metadata (2.8 kB)\nCollecting bs4\n  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\nCollecting markdownify\n  Downloading markdownify-1.2.0-py3-none-any.whl.metadata (9.9 kB)\nCollecting dotenv\n  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\nCollecting psycopg2-binary==2.9.7\n  Downloading psycopg2_binary-2.9.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nCollecting pgvector\n  Downloading pgvector-0.4.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: requests<3,>=2.28.1 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk==0.61.0) (2.31.0)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk==0.61.0) (2.35.0)\nRequirement already satisfied: databricks-connect in /databricks/python3/lib/python3.11/site-packages (from databricks-agents==1.2.0) (15.4.12)\nCollecting dataclasses-json (from databricks-agents==1.2.0)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nCollecting jinja2>=3.0.0 (from databricks-agents==1.2.0)\n  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\nCollecting mlflow-skinny<4.0.0,>=3.1.0 (from databricks-agents==1.2.0)\n  Downloading mlflow_skinny-3.2.0-py3-none-any.whl.metadata (30 kB)\nCollecting tenacity>=8.5 (from databricks-agents==1.2.0)\n  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\nCollecting tiktoken>=0.8.0 (from databricks-agents==1.2.0)\n  Downloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting tqdm (from databricks-agents==1.2.0)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nCollecting urllib3>=2.0 (from databricks-agents==1.2.0)\n  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting pydantic>=2 (from databricks-agents==1.2.0)\n  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\nCollecting whenever==0.7.3 (from databricks-agents==1.2.0)\n  Downloading whenever-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: boto3>1 in /databricks/python3/lib/python3.11/site-packages (from databricks-agents==1.2.0) (1.34.39)\nRequirement already satisfied: botocore in /databricks/python3/lib/python3.11/site-packages (from databricks-agents==1.2.0) (1.34.39)\nRequirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-vectorsearch==0.57) (5.29.3)\nCollecting deprecation>=2 (from databricks-vectorsearch==0.57)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: numpy>=1.16.6 in /databricks/python3/lib/python3.11/site-packages (from pyarrow<20) (1.23.5)\nRequirement already satisfied: ipython<10,>=8 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk[notebook]) (8.25.0)\nCollecting ipywidgets<9,>=8 (from databricks-sdk[notebook])\n  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\nCollecting mlflow-skinny<4.0.0,>=3.1.0 (from databricks-agents==1.2.0)\n  Downloading mlflow_skinny-3.1.0-py3-none-any.whl.metadata (30 kB)\nCollecting Flask<4 (from mlflow<=3.1)\n  Downloading flask-3.1.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting alembic!=1.10.0,<2 (from mlflow<=3.1)\n  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\nCollecting docker<8,>=4.0.0 (from mlflow<=3.1)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphene<4 (from mlflow<=3.1)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow<=3.1)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (3.7.2)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (1.5.3)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (1.3.0)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (1.11.1)\nCollecting sqlalchemy<3,>=1.4.0 (from mlflow<=3.1)\n  Downloading sqlalchemy-2.0.43-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (8.0.4)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (3.0.0)\nCollecting fastapi<1 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (6.0.0)\nCollecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: packaging<26 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (23.2)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (6.0)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (0.5.1)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (4.10.0)\nCollecting uvicorn<1 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.11/site-packages (from mlflow[databricks]) (12.14.0)\nRequirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow[databricks]) (2.18.2)\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\nCollecting langsmith>=0.1.17 (from langchain)\n  Downloading langsmith-0.4.14-py3-none-any.whl.metadata (14 kB)\nCollecting jsonpatch<2.0,>=1.33 (from langchain_core)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting databricks-ai-bridge>=0.7.0 (from databricks-langchain)\n  Downloading databricks_ai_bridge-0.7.0-py3-none-any.whl.metadata (6.2 kB)\nCollecting databricks-connect (from databricks-agents==1.2.0)\n  Downloading databricks_connect-16.1.6-py2.py3-none-any.whl.metadata (2.6 kB)\nCollecting openai>=1.99.9 (from databricks-langchain)\n  Downloading openai-1.99.9-py3-none-any.whl.metadata (29 kB)\nCollecting unitycatalog-langchain>=0.2.0 (from unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading unitycatalog_langchain-0.2.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting beautifulsoup4 (from bs4)\n  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: six<2,>=1.15 in /usr/lib/python3/dist-packages (from markdownify) (1.16.0)\nCollecting python-dotenv (from dotenv)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow<=3.1)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nCollecting typing-extensions<5,>=4.0.0 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: azure-core<2.0.0,>=1.28.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (1.32.0)\nRequirement already satisfied: azure-storage-blob<13.0.0,>=12.19.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (12.19.1)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (0.7.2)\nCollecting soupsieve>1.2 (from beautifulsoup4->bs4)\n  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.11/site-packages (from boto3>1->databricks-agents==1.2.0) (0.10.0)\nRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.11/site-packages (from boto3>1->databricks-agents==1.2.0) (0.10.3)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.11/site-packages (from botocore->databricks-agents==1.2.0) (2.8.2)\nCollecting urllib3>=2.0 (from databricks-agents==1.2.0)\n  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\nCollecting tabulate>=0.9.0 (from databricks-ai-bridge>=0.7.0->databricks-langchain)\n  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\nRequirement already satisfied: googleapis-common-protos>=1.56.4 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (1.65.0)\nRequirement already satisfied: grpcio-status>=1.59.3 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (1.69.0)\nRequirement already satisfied: grpcio>=1.59.3 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (1.69.0)\nRequirement already satisfied: py4j==0.10.9.7 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (0.10.9.7)\nRequirement already satisfied: setuptools>=68.0.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (75.1.0)\nCollecting langchain-openai (from databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0)\n  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\nCollecting httpx (from databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting blinker>=1.9.0 (from Flask<4->mlflow<=3.1)\n  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting click<9,>=7.0 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\nCollecting itsdangerous>=2.2.0 (from Flask<4->mlflow<=3.1)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nCollecting markupsafe>=2.1.1 (from Flask<4->mlflow<=3.1)\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow<=3.1)\n  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.61.0) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.61.0) (4.9)\nRequirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.18.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.4.1)\nRequirement already satisfied: google-resumable-media>=2.7.2 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (1.6.0)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow<=3.1)\n  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow<=3.1)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: decorator in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.18.1)\nRequirement already satisfied: matplotlib-inline in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (3.0.48)\nRequirement already satisfied: pygments>=2.4.0 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (2.15.1)\nRequirement already satisfied: stack-data in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.2.0)\nRequirement already satisfied: traitlets>=5.13.0 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (5.13.0)\nRequirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (4.8.0)\nCollecting comm>=0.1.3 (from ipywidgets<9,>=8->databricks-sdk[notebook])\n  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\nCollecting widgetsnbextension~=4.0.14 (from ipywidgets<9,>=8->databricks-sdk[notebook])\n  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\nCollecting jupyterlab_widgets~=3.0.15 (from ipywidgets<9,>=8->databricks-sdk[notebook])\n  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\nCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\nCollecting orjson>=3.9.14 (from langsmith>=0.1.17->langchain)\n  Downloading orjson-3.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\nCollecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (10.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (3.0.9)\nCollecting anyio<5,>=3.5.0 (from openai>=1.99.9->databricks-langchain)\n  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.9->databricks-langchain) (1.7.0)\nCollecting jiter<1,>=0.4.0 (from openai>=1.99.9->databricks-langchain)\n  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nCollecting sniffio (from openai>=1.99.9->databricks-langchain)\n  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas<3->mlflow<=3.1) (2022.7)\nCollecting annotated-types>=0.6.0 (from pydantic>=2->databricks-agents==1.2.0)\n  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\nCollecting pydantic-core==2.33.2 (from pydantic>=2->databricks-agents==1.2.0)\n  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic>=2->databricks-agents==1.2.0)\n  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (2023.7.22)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow<=3.1) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow<=3.1) (2.2.0)\nCollecting greenlet>=1 (from sqlalchemy<3,>=1.4.0->mlflow<=3.1)\n  Downloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nCollecting regex>=2022.1.18 (from tiktoken>=0.8.0->databricks-agents==1.2.0)\n  Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\nCollecting langchain-community>=0.2.0 (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\nCollecting unitycatalog-ai (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading unitycatalog_ai-0.3.1-py3-none-any.whl.metadata (31 kB)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->databricks-agents==1.2.0)\n  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json->databricks-agents==1.2.0)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow[databricks]) (41.0.3)\nCollecting starlette<0.48.0,>=0.40.0 (from fastapi<1->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (4.0.11)\nCollecting protobuf<6,>=3.12.0 (from databricks-vectorsearch==0.57)\n  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=1.30.0->mlflow[databricks]) (1.25.0)\nINFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\nCollecting grpcio-status>=1.59.3 (from databricks-connect->databricks-agents==1.2.0)\n  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\nINFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\nCollecting httpcore==1.* (from httpx->databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0)\n  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nCollecting h11>=0.16 (from httpcore==1.*->httpx->databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0)\n  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (3.11.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /databricks/python3/lib/python3.11/site-packages (from jedi>=0.16->ipython<10,>=8->databricks-sdk[notebook]) (0.8.3)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting numpy>=1.16.6 (from pyarrow<20)\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nCollecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: ptyprocess>=0.5 in /databricks/python3/lib/python3.11/site-packages (from pexpect>4.3->ipython<10,>=8->databricks-sdk[notebook]) (0.7.0)\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython<10,>=8->databricks-sdk[notebook]) (0.2.5)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk==0.61.0) (0.4.8)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->databricks-agents==1.2.0) (0.4.3)\nRequirement already satisfied: executing in /databricks/python3/lib/python3.11/site-packages (from stack-data->ipython<10,>=8->databricks-sdk[notebook]) (0.8.3)\nRequirement already satisfied: asttokens in /databricks/python3/lib/python3.11/site-packages (from stack-data->ipython<10,>=8->databricks-sdk[notebook]) (2.0.5)\nRequirement already satisfied: pure-eval in /databricks/python3/lib/python3.11/site-packages (from stack-data->ipython<10,>=8->databricks-sdk[notebook]) (0.2.2)\nRequirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.11/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain) (1.5.6)\nCollecting unitycatalog-client (from unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading unitycatalog_client-0.3.0-py3-none-any.whl.metadata (7.8 kB)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\nCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow[databricks]) (1.15.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (5.0.1)\nCollecting aiohttp-retry>=2.8.3 (from unitycatalog-client->unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain)\n  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow[databricks]) (2.21)\nDownloading databricks_sdk-0.61.0-py3-none-any.whl (680 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/680.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m680.6/680.6 kB\u001B[0m \u001B[31m22.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading databricks_agents-1.2.0-py3-none-any.whl (195 kB)\nDownloading databricks_vectorsearch-0.57-py3-none-any.whl (16 kB)\nDownloading psycopg2_binary-2.9.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m96.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading whenever-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (418 kB)\nDownloading mlflow-3.1.0-py3-none-any.whl (24.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/24.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m24.6/24.7 MB\u001B[0m \u001B[31m124.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.7/24.7 MB\u001B[0m \u001B[31m95.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_skinny-3.1.0-py3-none-any.whl (1.9 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.9 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m90.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m87.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\nDownloading databricks_langchain-0.7.0-py3-none-any.whl (26 kB)\nDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\nDownloading markdownify-1.2.0-py3-none-any.whl (15 kB)\nDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\nDownloading pgvector-0.4.1-py3-none-any.whl (27 kB)\nDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\nDownloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\nDownloading databricks_ai_bridge-0.7.0-py3-none-any.whl (18 kB)\nDownloading databricks_connect-16.1.6-py2.py3-none-any.whl (2.4 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.4/2.4 MB\u001B[0m \u001B[31m130.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nDownloading docker-7.1.0-py3-none-any.whl (147 kB)\nDownloading flask-3.1.1-py3-none-any.whl (103 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\nDownloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\nDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\nDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\nDownloading langsmith-0.4.14-py3-none-any.whl (373 kB)\nDownloading openai-1.99.9-py3-none-any.whl (786 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/786.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m786.8/786.8 kB\u001B[0m \u001B[31m71.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\nDownloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m174.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading sqlalchemy-2.0.43-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m93.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\nDownloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m116.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\nDownloading unitycatalog_langchain-0.2.0-py3-none-any.whl (5.4 kB)\nDownloading urllib3-2.0.7-py3-none-any.whl (124 kB)\nDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nDownloading anyio-4.10.0-py3-none-any.whl (107 kB)\nDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\nDownloading click-8.2.1-py3-none-any.whl (102 kB)\nDownloading comm-0.2.3-py3-none-any.whl (7.3 kB)\nDownloading fastapi-0.116.1-py3-none-any.whl (95 kB)\nDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\nDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\nDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (587 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/587.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m587.7/587.7 kB\u001B[0m \u001B[31m41.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\nDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\nDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\nDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\nDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\nDownloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\nDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m98.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/18.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.3/18.3 MB\u001B[0m \u001B[31m150.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\nDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\nDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\nDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\nDownloading orjson-3.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\nDownloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/798.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m798.9/798.9 kB\u001B[0m \u001B[31m82.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\nDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\nDownloading soupsieve-2.7-py3-none-any.whl (36 kB)\nDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\nDownloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\nDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\nDownloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m109.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\nDownloading unitycatalog_ai-0.3.1-py3-none-any.whl (66 kB)\nDownloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m148.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\nDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\nDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\nDownloading starlette-0.47.2-py3-none-any.whl (72 kB)\nDownloading unitycatalog_client-0.3.0-py3-none-any.whl (159 kB)\nDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading attrs-25.3.0-py3-none-any.whl (63 kB)\nDownloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\nDownloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\nDownloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\nDownloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\nInstalling collected packages: widgetsnbextension, whenever, urllib3, typing-extensions, tqdm, tenacity, tabulate, soupsieve, sniffio, regex, python-dotenv, psycopg2-binary, protobuf, propcache, orjson, numpy, multidict, marshmallow, markupsafe, jupyterlab_widgets, jsonpointer, jiter, itsdangerous, httpx-sse, h11, gunicorn, greenlet, graphql-core, frozenlist, deprecation, comm, click, blinker, attrs, annotated-types, aiohappyeyeballs, yarl, werkzeug, uvicorn, typing-inspection, typing-inspect, sqlalchemy, pydantic-core, pgvector, opentelemetry-api, Mako, jsonpatch, jinja2, httpcore, graphql-relay, dotenv, beautifulsoup4, anyio, aiosignal, tiktoken, starlette, requests-toolbelt, pydantic, opentelemetry-semantic-conventions, markdownify, httpx, grpcio-status, graphene, Flask, docker, dataclasses-json, databricks-sdk, bs4, alembic, aiohttp, pydantic-settings, opentelemetry-sdk, openai, langsmith, ipywidgets, fastapi, databricks-connect, aiohttp-retry, unitycatalog-client, mlflow-skinny, langchain_core, unitycatalog-ai, mlflow, langchain-text-splitters, langchain-openai, databricks-vectorsearch, databricks-ai-bridge, langchain, langchain-community, databricks-agents, unitycatalog-langchain, databricks-langchain\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.16\n    Not uninstalling urllib3 at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'urllib3'. No files were found to uninstall.\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.10.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 8.2.2\n    Not uninstalling tenacity at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'tenacity'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 5.29.3\n    Not uninstalling protobuf at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'numpy'. No files were found to uninstall.\n  Attempting uninstall: comm\n    Found existing installation: comm 0.1.2\n    Not uninstalling comm at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'comm'. No files were found to uninstall.\n  Attempting uninstall: click\n    Found existing installation: click 8.0.4\n    Not uninstalling click at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'click'. No files were found to uninstall.\n  Attempting uninstall: blinker\n    Found existing installation: blinker 1.4\n    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'blinker'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.6\n    Not uninstalling pydantic at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'pydantic'. No files were found to uninstall.\n  Attempting uninstall: grpcio-status\n    Found existing installation: grpcio-status 1.69.0\n    Not uninstalling grpcio-status at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'grpcio-status'. No files were found to uninstall.\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.40.0\n    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n  Attempting uninstall: ipywidgets\n    Found existing installation: ipywidgets 7.7.2\n    Not uninstalling ipywidgets at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'ipywidgets'. No files were found to uninstall.\n  Attempting uninstall: databricks-connect\n    Found existing installation: databricks-connect 15.4.12\n    Not uninstalling databricks-connect at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'databricks-connect'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.11.4\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\nSuccessfully installed Flask-3.1.1 Mako-1.3.10 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiohttp-retry-2.9.1 aiosignal-1.4.0 alembic-1.16.4 annotated-types-0.7.0 anyio-4.10.0 attrs-25.3.0 beautifulsoup4-4.13.4 blinker-1.9.0 bs4-0.0.2 click-8.2.1 comm-0.2.3 databricks-agents-1.2.0 databricks-ai-bridge-0.7.0 databricks-connect-16.1.6 databricks-langchain-0.7.0 databricks-sdk-0.61.0 databricks-vectorsearch-0.57 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 dotenv-0.9.9 fastapi-0.116.1 frozenlist-1.7.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 greenlet-3.2.4 grpcio-status-1.62.3 gunicorn-23.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.1 ipywidgets-8.1.7 itsdangerous-2.2.0 jinja2-3.1.6 jiter-0.10.0 jsonpatch-1.33 jsonpointer-3.0.0 jupyterlab_widgets-3.0.15 langchain-0.3.27 langchain-community-0.3.27 langchain-openai-0.3.30 langchain-text-splitters-0.3.9 langchain_core-0.3.74 langsmith-0.4.14 markdownify-1.2.0 markupsafe-3.0.2 marshmallow-3.26.1 mlflow-3.1.0 mlflow-skinny-3.1.0 multidict-6.6.4 numpy-1.26.4 openai-1.99.9 opentelemetry-api-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 orjson-3.11.2 pgvector-0.4.1 propcache-0.3.2 protobuf-4.25.8 psycopg2-binary-2.9.7 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 regex-2025.7.34 requests-toolbelt-1.0.0 sniffio-1.3.1 soupsieve-2.7 sqlalchemy-2.0.43 starlette-0.47.2 tabulate-0.9.0 tenacity-9.1.2 tiktoken-0.11.0 tqdm-4.67.1 typing-extensions-4.14.1 typing-inspect-0.9.0 typing-inspection-0.4.1 unitycatalog-ai-0.3.1 unitycatalog-client-0.3.0 unitycatalog-langchain-0.2.0 urllib3-2.0.7 uvicorn-0.35.0 werkzeug-3.1.3 whenever-0.7.3 widgetsnbextension-4.0.14 yarl-1.20.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install 'databricks-sdk==0.61.0' 'pyarrow<20' 'databricks-sdk[notebook]' 'databricks-agents==1.2.0' 'mlflow<=3.1' 'mlflow[databricks]' 'databricks-vectorsearch==0.57' 'langchain' 'langchain_core' 'databricks-langchain' 'bs4' 'markdownify' 'dotenv' 'psycopg2-binary==2.9.7' 'pgvector'\n",
    "import os\n",
    "if os.environ.get(\"DATABRICKS_RUNTIME_VERSION\"):\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354c76a9-8be0-4c8d-9983-9e57eb961e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"embedding_model\", \"databricks-gte-large-en\")\n",
    "dbutils.widgets.text(\"database_instance_name\", \"tannerw-adtech-db\")\n",
    "dbutils.widgets.text(\"postgres_database_name\", \"databricks_postgres\")\n",
    "dbutils.widgets.text(\"llm_model_serving_endpoint_name\", \"databricks-claude-3-7-sonnet\")\n",
    "dbutils.widgets.text(\"target_catalog\", \"tanner_wendland\")\n",
    "dbutils.widgets.text(\"target_schema\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99bd51e-5492-4d5d-a971-0ddda14452cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = dbutils.widgets.get(\"embedding_model\")\n",
    "database_instance_name = dbutils.widgets.get(\"database_instance_name\")\n",
    "postgres_database_name = dbutils.widgets.get(\"postgres_database_name\")\n",
    "llm_model_serving_endpoint_name = dbutils.widgets.get(\"llm_model_serving_endpoint_name\")\n",
    "target_catalog = dbutils.widgets.get(\"target_catalog\")\n",
    "target_schema = dbutils.widgets.get(\"target_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "188fb341-aa4b-4717-b2f3-12ef5f50a65c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import os\n",
    "from typing import Optional\n",
    "import uuid\n",
    "\n",
    "host = os.environ.get(\"DATABRICKS_HOST\")\n",
    "token = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "workspace_client = WorkspaceClient(host=host, token=token)\n",
    "\n",
    "def get_postgres_connection(\n",
    "    client: WorkspaceClient,\n",
    "    db_name: str,\n",
    "    database_name: Optional[str] = \"databricks_postgres\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get PostgreSQL connection string using Databricks SDK.\n",
    "\n",
    "    Args:\n",
    "        client (WorkspaceClient): The Databricks workspace client.\n",
    "        db_name (str): The name of the database instance.\n",
    "        database_name (Optional[str], optional): The name of the database to connect to.\n",
    "            Defaults to \"databricks_postgres\".\n",
    "\n",
    "    Returns:\n",
    "        str: PostgreSQL connection string.\n",
    "    \"\"\"\n",
    "    database = client.database.get_database_instance(db_name)\n",
    "    credentials = client.database.generate_database_credential(\n",
    "        instance_names=[db_name],\n",
    "        request_id=str(uuid.uuid4())\n",
    "    )\n",
    "\n",
    "    # Use POSTGRES_GROUP env var as username if set, otherwise use current user\n",
    "    postgres_group = os.getenv('POSTGRES_GROUP')\n",
    "    username = postgres_group if postgres_group else client.current_user.me().user_name\n",
    "\n",
    "    database_info = {\n",
    "        \"host\": database.read_write_dns,\n",
    "        \"port\": \"5432\",\n",
    "        \"database\": database_name,\n",
    "        \"username\": username,\n",
    "        \"password\": credentials.token,\n",
    "        \"ssl_mode\": \"require\"\n",
    "    }\n",
    "\n",
    "    database_url = (\n",
    "        f\"postgresql://{database_info['username']}:{database_info['password']}\"\n",
    "        f\"@{database_info['host']}:{database_info['port']}/\"\n",
    "        f\"{database_info['database']}?sslmode={database_info['ssl_mode']}\"\n",
    "    )\n",
    "\n",
    "    return database_url\n",
    "\n",
    "database_url = get_postgres_connection(workspace_client, database_instance_name, postgres_database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d115d39-47b3-40a6-b143-e4ae3f30c1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585760d5-cc0a-4a30-b069-1b57188e8623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9d15d1-397a-4cdf-b948-41a6ad5555f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain_config = {\n",
    "    \"llm_model_serving_endpoint_name\": llm_model_serving_endpoint_name,\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"database_instance_name\": database_instance_name,\n",
    "    \"postgres_database_name\": postgres_database_name,\n",
    "    \"llm_prompt_template\": \"\"\"You are an assistant that answers questions. You have access to a vector search tool that searches previous conversations, but you should primarily use the immediate conversation context provided.\n",
    "\n",
    "Current conversation context: {context}\n",
    "\n",
    "Instructions:\n",
    "1. First, use the current conversation context above to answer questions\n",
    "2. Only use the vector search tool if the question explicitly refers to information from previous conversations that is NOT in the current context\n",
    "3. If the current conversation context contains sufficient information to answer the question, do NOT use the vector search tool\n",
    "4. The vector search tool should be used sparingly, only when the user is clearly asking about something from their chat history that isn't in the current conversation\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99472b6-acef-49b6-aafd-a5260d654768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Vector Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a545c6a-6753-4240-b196-2b284d419534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "retriever_config = {\n",
    "    \"parameters\": {\n",
    "        \"k\": 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9afa93-e6f1-4616-8b92-b99fa8f143af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "# combine dynamic and static filters for vector search\n",
    "def create_configurable_with_filters(input: Dict, retriever_config: Dict) -> Dict:\n",
    "   \"\"\"\n",
    "   create configurable object with filters.\n",
    "   Args:\n",
    "       input: The input data containing filters.\n",
    "   Returns:\n",
    "       A configurable object with filters added to the search_kwargs.\n",
    "   \"\"\"\n",
    "   if \"custom_inputs\" in input:\n",
    "       filters = input[\"custom_inputs\"][\"filters\"]\n",
    "   else:\n",
    "       filters = {}\n",
    "   print(filters)\n",
    "   configurable = {\n",
    "       \"configurable\": {\n",
    "           \"search_kwargs\": {\n",
    "               \"k\": retriever_config.get(\"parameters\")[\"k\"],\n",
    "               \"filter\": filters\n",
    "           }\n",
    "       }\n",
    "   }\n",
    "   return configurable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bfb5894-e178-45c4-8874-b882e3f46b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9c02e33-0b5b-402b-b659-041490a9c82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, List\n",
    "from databricks_langchain import DatabricksEmbeddings\n",
    "from sqlalchemy import create_engine, text, event\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "# Use psycopg2 driver explicitly for SQLAlchemy\n",
    "database_url_sqlalchemy = database_url.replace(\"postgresql://\", \"postgresql+psycopg2://\")\n",
    "\n",
    "engine = create_engine(database_url_sqlalchemy, pool_pre_ping=True)\n",
    "\n",
    "@event.listens_for(engine, \"connect\")\n",
    "def _register_vector(dbapi_connection, connection_record):\n",
    "    # Register pgvector adapter for psycopg2 so Python lists map to the vector type\n",
    "    register_vector(dbapi_connection)\n",
    "\n",
    "def pg_vector_similarity_search(\n",
    "    embeddings: DatabricksEmbeddings,\n",
    "    query: str,\n",
    "    k: int = 3,\n",
    "    filters: Optional[Dict[str, Any]] = None,\n",
    ") -> str:\n",
    "    # 1) Embed the query\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    # 2) Build where clause from filters\n",
    "    filters = filters or {}\n",
    "    where_conditions: List[str] = []\n",
    "    params: Dict[str, Any] = {}\n",
    "\n",
    "    if \"user_name\" in filters:\n",
    "        where_conditions.append(\"me.user_name = :user_name\")\n",
    "        params[\"user_name\"] = filters[\"user_name\"]\n",
    "\n",
    "    if \"chat_id\" in filters:\n",
    "        where_conditions.append(\"me.chat_id = :chat_id\")\n",
    "        params[\"chat_id\"] = filters[\"chat_id\"]\n",
    "\n",
    "    where_clause = \"\"\n",
    "    if where_conditions:\n",
    "        where_clause = \"WHERE \" + \" AND \".join(where_conditions)\n",
    "\n",
    "    # 3) Query using cosine distance operator (<=>); requires pgvector\n",
    "    sql = text(f\"\"\"\n",
    "        SELECT\n",
    "            ch.message_content,\n",
    "            me.user_name,\n",
    "            me.chat_id,\n",
    "            ch.message_type,\n",
    "            ch.created_at,\n",
    "            ch.message_order,\n",
    "            (me.embedding <=> CAST(:query_embedding AS vector)) AS distance\n",
    "        FROM message_embeddings me\n",
    "        JOIN chat_history ch ON me.message_id = ch.id\n",
    "        {where_clause}\n",
    "        ORDER BY me.embedding <=> CAST(:query_embedding AS vector)\n",
    "        LIMIT :k\n",
    "    \"\"\")\n",
    "\n",
    "    # 4) Execute\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(\n",
    "            sql, {\"query_embedding\": query_embedding, \"k\": k, **params}\n",
    "        ).fetchall()\n",
    "\n",
    "    # 5) Format context for the prompt\n",
    "    passages = [f\"Passage: {r.message_content}\" for r in rows]\n",
    "    return \"\\n\".join(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0de4ba-d9b3-4cef-8105-26490de4b9a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from databricks_langchain.chat_models import ChatDatabricks\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "## Load the chain's configuration\n",
    "model_config = mlflow.models.ModelConfig(development_config=chain_config)\n",
    "\n",
    "def pg_vector_search_with_filters(embeddings: DatabricksEmbeddings, query: str, input_data: dict = None) -> str:\n",
    "    input_data = input_data or {}\n",
    "    filters = input_data.get(\"custom_inputs\", {}).get(\"filters\", {})\n",
    "    k = retriever_config[\"parameters\"][\"k\"]\n",
    "    return pg_vector_similarity_search(embeddings=embeddings, query=query, k=k, filters=filters)\n",
    "\n",
    "vector_search_tool = Tool(\n",
    "    name=\"search_chat_history\",\n",
    "    description=\"Retrieve chat history from Postgres (pgvector) for the current user; use only if the immediate conversation context is insufficient.\",\n",
    "    func=lambda q: pg_vector_search_with_filters(q, {}),  # default no filters\n",
    ")\n",
    "\n",
    "# Updated prompt template for tool-calling agent\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", model_config.get('llm_prompt_template')),\n",
    "    (\"user\", \"{question}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Our foundation model answering the final prompt\n",
    "model = ChatDatabricks(\n",
    "    endpoint=model_config.get(\"llm_model_serving_endpoint_name\"),\n",
    "    extra_params={\"temperature\": 0.01, \"max_tokens\": 500}\n",
    ")\n",
    "\n",
    "embeddings = DatabricksEmbeddings(\n",
    "    endpoint=chain_config[\"embedding_model\"],\n",
    "    token=os.environ.get(\"DATABRICKS_TOKEN\")\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(model, [vector_search_tool], prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=[vector_search_tool], verbose=True)\n",
    "\n",
    "\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "def extract_context_string(chat_messages_array):\n",
    "    return '\\n'.join([f\"Role: {message['role']} - Content: {message['content']}\" for message in chat_messages_array[:-1]])\n",
    "\n",
    "# In your rag_chain_with_tool():\n",
    "def rag_chain_with_tool(input_data: dict) -> str:\n",
    "    user_query = extract_user_query_string(input_data[\"messages\"])\n",
    "    def filtered_vector_search(q: str) -> str:\n",
    "        return pg_vector_search_with_filters(embeddings, q, input_data)\n",
    "\n",
    "    filtered_tool = Tool(\n",
    "        name=\"search_chat_history\",\n",
    "        description=\"Retreive chat history from this vector database for the current user, use this tool to answer questions that may refer to previous conversations\",\n",
    "        func=filtered_vector_search,\n",
    "    )\n",
    "\n",
    "    filtered_agent = create_tool_calling_agent(model, [filtered_tool], prompt)\n",
    "    filtered_agent_executor = AgentExecutor(agent=filtered_agent, tools=[filtered_tool], verbose=True)\n",
    "    context = extract_context_string(input_data[\"messages\"])\n",
    "    result = filtered_agent_executor.invoke({\"question\": user_query, \"context\": context})\n",
    "    return result[\"output\"]\n",
    "\n",
    "# Create a runnable version of the chain\n",
    "chain = RunnableLambda(rag_chain_with_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a985e5-e27b-41c8-a705-82ed8d1ebb9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Document Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5e1c30-cb1b-4436-9a03-01882af81c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n\u001B[32;1m\u001B[1;3m\nInvoking: `search_chat_history` with `all`\nresponded: I'll help you look at your chat history. To provide you with relevant information, I need to search your previous conversations.\n\n\u001B[0m\u001B[36;1m\u001B[1;3mPassage: Based on the information in our current conversation, math rock emerged in the late 1980s and early 1990s. It developed as a subgenre of rock music characterized by complex rhythmic structures, atypical time signatures, and intricate instrumental performances.\n\nThe genre evolved from earlier experimental and progressive rock traditions, with bands like Slint being credited as early pioneers of the sound with their influential album \"Spiderland.\"\nPassage: # Math Rock\n\nMath rock is a subgenre of rock music that emerged in the late 1980s and early 1990s. It's characterized by complex rhythmic structures, atypical time signatures, angular melodies, and intricate instrumental performances. Here are some key aspects of math rock:\n\n## Musical Characteristics\n- **Unusual Time Signatures**: Math rock frequently uses odd time signatures like 7/8, 11/8, or 13/8, or rapidly shifts between different time signatures\n- **Complex Rhythms**: Syncopation, polyrhythms, and rhythmic displacements are common\n- **Technical Instrumentation**: Often features clean, intricate guitar work with extensive use of tapping, hammer-ons/pull-offs, and unusual tunings\n- **Dynamic Shifts**: Abrupt changes in tempo, volume, and intensity\n- **Minimal Vocals**: Many math rock bands are primarily instrumental, though some incorporate vocals\n\n## Influential Bands\n- **Slint**: Often credited as early pioneers with their album \"Spiderland\"\n- **Don Caballero**: Known for their technical precision and complex compositions\n- **Battles**: Combining electronic elements with math rock structures\n- **American Football**: Blending math rock with emo sensibilities\n- **Hella**: Known for extremely technical and frenetic playing\n- **Tricot**: Japanese math rock band with more prominent vocal elements\n\nMath rock has influenced and overlaps with other genres like post-rock, progressive rock, and certain styles of indie rock. The genre continues to evolve with newer bands incorporating elements from electronic music, jazz, and other experimental styles.\nPassage: Does it work with their AI/BI product?\u001B[0m\u001B[32;1m\u001B[1;3mBased on your chat history, I can see that you've previously discussed math rock music. Your history contains detailed information about:\n\n1. The origins of math rock (emerged in the late 1980s and early 1990s)\n2. Musical characteristics of math rock, including:\n   - Unusual time signatures (7/8, 11/8, 13/8)\n   - Complex rhythms and syncopation\n   - Technical instrumentation with intricate guitar work\n   - Dynamic shifts and minimal vocals\n\n3. Influential math rock bands such as:\n   - Slint (pioneers with their album \"Spiderland\")\n   - Don Caballero\n   - Battles\n   - American Football\n   - Hella\n   - Tricot\n\nThe history also shows that math rock has connections to other genres like post-rock, progressive rock, and indie rock, and continues to evolve with influences from electronic music, jazz, and other experimental styles.\n\nIs there anything specific about your chat history you'd like to know more about?\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nBased on your chat history, I can see that you've previously discussed math rock music. Your history contains detailed information about:\n\n1. The origins of math rock (emerged in the late 1980s and early 1990s)\n2. Musical characteristics of math rock, including:\n   - Unusual time signatures (7/8, 11/8, 13/8)\n   - Complex rhythms and syncopation\n   - Technical instrumentation with intricate guitar work\n   - Dynamic shifts and minimal vocals\n\n3. Influential math rock bands such as:\n   - Slint (pioneers with their album \"Spiderland\")\n   - Don Caballero\n   - Battles\n   - American Football\n   - Hella\n   - Tricot\n\nThe history also shows that math rock has connections to other genres like post-rock, progressive rock, and indie rock, and continues to evolve with influences from electronic music, jazz, and other experimental styles.\n\nIs there anything specific about your chat history you'd like to know more about?\n"
     ]
    }
   ],
   "source": [
    "input_example_no_filter = {\"messages\": [ {\"role\": \"user\", \"content\": \"Look at the chat history\"}]}\n",
    "answer_no_filter = chain.invoke(input_example_no_filter)\n",
    "print(answer_no_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3300a0-9b8e-40a5-a8fd-fd3ae12ca393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n\u001B[32;1m\u001B[1;3m\nInvoking: `search_chat_history` with `chat history idea`\nresponded: I don't have enough information from our current conversation to know what specific chat history idea you're referring to. To help you better, I'll need to search your previous conversations.\n\n\u001B[0m\u001B[36;1m\u001B[1;3mPassage: # Common Data Structures Used in Audience Segmentation\n\nBuilding on our discussion about audience segmentation technologies, here are the common data structures used in the audience segmentation process:\n\n## User Profiles\n- Individual-level data structures containing attributes and behaviors\n- Often stored as JSON objects or relational database records\n- Include identifiers, demographics, behaviors, and preferences\n\n## Cookies and Device IDs\n- Small text files or unique identifiers stored on user devices\n- Used to recognize users across sessions and websites\n- Examples: first-party cookies, third-party cookies, mobile advertising IDs\n\n## Taxonomies and Hierarchies\n- Organized classification systems for categorizing audience attributes\n- Allow for both broad and granular segmentation\n- Example: Interest categories organized from general to specific\n\n## Segment Definitions\n- Rules-based or algorithmic definitions of audience groups\n- Can be simple (e.g., \"males aged 25-34\") or complex (combining multiple attributes)\n- Often represented as query expressions or decision trees\n\n## Look-alike Models\n- Statistical representations of similarities between users\n- Often implemented as vector spaces or probabilistic models\n- Used to find new users who resemble existing high-value customers\n\n## Cross-Device Graphs\n- Data structures mapping relationships between different devices\n- Enable consistent identification of users across multiple devices\n- Typically implemented as graph databases\n\n## Event Streams\n- Time-series data representing user actions and behaviors\n- Often processed using stream processing frameworks\n- Used for real-time segmentation and targeting\n\n## Hashed User Identifiers\n- Privacy-compliant representations of user IDs\n- Allow for data matching without exposing personally identifiable information\n- Examples: hashed emails, phone numbers, or device IDs\n\nThese data structures form the foundation of audience segmentation systems, enabling marketers to organize, analyze, and activate audience data effectively across the AdTech ecosystem.\nPassage: # Databases Commonly Used in AdTech\n\nBuilding on our previous discussions about audience segmentation technologies and data structures, here are the types of databases commonly used in AdTech use cases:\n\n## Relational Databases\n- Used for structured data with well-defined relationships\n- Good for transaction processing and data integrity\n- Examples: MySQL, PostgreSQL, Oracle\n- AdTech use: Customer information, campaign management, billing systems\n\n## NoSQL Databases\n- Designed for flexible schemas and horizontal scalability\n- Types include document, key-value, column-family, and graph databases\n- Examples: MongoDB (document), Redis (key-value), Cassandra (column-family), Neo4j (graph)\n- AdTech use: User profiles, real-time bidding data, flexible attribute storage\n\n## Data Warehouses\n- Optimized for analytics and reporting on large datasets\n- Support complex queries across massive amounts of data\n- Examples: Google BigQuery, Amazon Redshift, Snowflake\n- AdTech use: Campaign performance analysis, audience insights, attribution modeling\n\n## Time-Series Databases\n- Specialized for time-stamped or time-series data\n- Optimized for measuring changes over time\n- Examples: InfluxDB, TimescaleDB\n- AdTech use: User behavior tracking, campaign performance over time\n\n## In-Memory Databases\n- Store data primarily in RAM for extremely fast access\n- Used for real-time processing needs\n- Examples: Redis, Memcached, SAP HANA\n- AdTech use: Real-time bidding, ad serving decisions, user targeting\n\n## Graph Databases\n- Specialized for managing highly connected data\n- Efficient at traversing relationships between entities\n- Examples: Neo4j, Amazon Neptune\n- AdTech use: Identity resolution, cross-device mapping, interest networks\n\n## Data Lake Technologies\n- Store vast amounts of raw, unprocessed data\n- Support diverse data types and formats\n- Examples: Apache Hadoop, Amazon S3 with Athena\n- AdTech use: Long-term data storage, machine learning training data\n\n## Hybrid and Multi-Model Databases\n- Combine features of different database types\n- Provide flexibility for various data models\n- Examples: ArangoDB\nPassage: Tell me a few things about databricks genie\u001B[0m\u001B[32;1m\u001B[1;3mBased on your previous conversations, I found that you've discussed several ideas related to data structures and databases used in audience segmentation and AdTech. However, I don't see a specific \"chat history idea\" mentioned in the search results.\n\nThe search results show detailed discussions about:\n1. Common data structures used in audience segmentation (user profiles, cookies, taxonomies, etc.)\n2. Various types of databases used in AdTech (relational, NoSQL, data warehouses, etc.)\n\nThere was also a mention of \"databricks genie\" in one of your previous conversations, but without more context.\n\nCould you please clarify which specific idea from your chat history you're referring to? This would help me provide you with more accurate information.\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nBased on your previous conversations, I found that you've discussed several ideas related to data structures and databases used in audience segmentation and AdTech. However, I don't see a specific \"chat history idea\" mentioned in the search results.\n\nThe search results show detailed discussions about:\n1. Common data structures used in audience segmentation (user profiles, cookies, taxonomies, etc.)\n2. Various types of databases used in AdTech (relational, NoSQL, data warehouses, etc.)\n\nThere was also a mention of \"databricks genie\" in one of your previous conversations, but without more context.\n\nCould you please clarify which specific idea from your chat history you're referring to? This would help me provide you with more accurate information.\n"
     ]
    }
   ],
   "source": [
    "input_example = {\"messages\": [ {\"role\": \"user\", \"content\": \"What was my chat history idea?\"}], \"custom_inputs\": {\"filters\": {\"user_name\": \"tanner.wendland@databricks.com\"}}}\n",
    "answer = chain.invoke(input_example)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247bc64a-7331-4a60-97fe-52d0360bd338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e706a3d-0b01-44e6-95e1-fcaa22ffaf44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/15 14:55:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n\uD83D\uDD17 View Logged Model at: https://e2-demo-field-eng.cloud.databricks.com/ml/experiments/3426381942480258/models/m-7fef0fd622ec4243ad22c25f25ba3db4?o=1444828305810485\nFailed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fa564496660> at 0x7fa5580ef0c0> was created in a different Context\nFailed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fa564496660> at 0x7fa5580ef480> was created in a different Context\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n\u001B[32;1m\u001B[1;3m\nInvoking: `search_chat_history` with `chat history idea`\nresponded: I don't have enough information from the current conversation context to determine what specific chat history idea you're referring to. To help you better, I'll need to search your previous conversations.\n\n\u001B[0m"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fa564496660> at 0x7fa557f13f00> was created in a different Context\nFailed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fa564496660> at 0x7fa557f1d580> was created in a different Context\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36;1m\u001B[1;3mPassage: # Common Data Structures Used in Audience Segmentation\n\nBuilding on our discussion about audience segmentation technologies, here are the common data structures used in the audience segmentation process:\n\n## User Profiles\n- Individual-level data structures containing attributes and behaviors\n- Often stored as JSON objects or relational database records\n- Include identifiers, demographics, behaviors, and preferences\n\n## Cookies and Device IDs\n- Small text files or unique identifiers stored on user devices\n- Used to recognize users across sessions and websites\n- Examples: first-party cookies, third-party cookies, mobile advertising IDs\n\n## Taxonomies and Hierarchies\n- Organized classification systems for categorizing audience attributes\n- Allow for both broad and granular segmentation\n- Example: Interest categories organized from general to specific\n\n## Segment Definitions\n- Rules-based or algorithmic definitions of audience groups\n- Can be simple (e.g., \"males aged 25-34\") or complex (combining multiple attributes)\n- Often represented as query expressions or decision trees\n\n## Look-alike Models\n- Statistical representations of similarities between users\n- Often implemented as vector spaces or probabilistic models\n- Used to find new users who resemble existing high-value customers\n\n## Cross-Device Graphs\n- Data structures mapping relationships between different devices\n- Enable consistent identification of users across multiple devices\n- Typically implemented as graph databases\n\n## Event Streams\n- Time-series data representing user actions and behaviors\n- Often processed using stream processing frameworks\n- Used for real-time segmentation and targeting\n\n## Hashed User Identifiers\n- Privacy-compliant representations of user IDs\n- Allow for data matching without exposing personally identifiable information\n- Examples: hashed emails, phone numbers, or device IDs\n\nThese data structures form the foundation of audience segmentation systems, enabling marketers to organize, analyze, and activate audience data effectively across the AdTech ecosystem.\nPassage: # Databases Commonly Used in AdTech\n\nBuilding on our previous discussions about audience segmentation technologies and data structures, here are the types of databases commonly used in AdTech use cases:\n\n## Relational Databases\n- Used for structured data with well-defined relationships\n- Good for transaction processing and data integrity\n- Examples: MySQL, PostgreSQL, Oracle\n- AdTech use: Customer information, campaign management, billing systems\n\n## NoSQL Databases\n- Designed for flexible schemas and horizontal scalability\n- Types include document, key-value, column-family, and graph databases\n- Examples: MongoDB (document), Redis (key-value), Cassandra (column-family), Neo4j (graph)\n- AdTech use: User profiles, real-time bidding data, flexible attribute storage\n\n## Data Warehouses\n- Optimized for analytics and reporting on large datasets\n- Support complex queries across massive amounts of data\n- Examples: Google BigQuery, Amazon Redshift, Snowflake\n- AdTech use: Campaign performance analysis, audience insights, attribution modeling\n\n## Time-Series Databases\n- Specialized for time-stamped or time-series data\n- Optimized for measuring changes over time\n- Examples: InfluxDB, TimescaleDB\n- AdTech use: User behavior tracking, campaign performance over time\n\n## In-Memory Databases\n- Store data primarily in RAM for extremely fast access\n- Used for real-time processing needs\n- Examples: Redis, Memcached, SAP HANA\n- AdTech use: Real-time bidding, ad serving decisions, user targeting\n\n## Graph Databases\n- Specialized for managing highly connected data\n- Efficient at traversing relationships between entities\n- Examples: Neo4j, Amazon Neptune\n- AdTech use: Identity resolution, cross-device mapping, interest networks\n\n## Data Lake Technologies\n- Store vast amounts of raw, unprocessed data\n- Support diverse data types and formats\n- Examples: Apache Hadoop, Amazon S3 with Athena\n- AdTech use: Long-term data storage, machine learning training data\n\n## Hybrid and Multi-Model Databases\n- Combine features of different database types\n- Provide flexibility for various data models\n- Examples: ArangoDB\nPassage: Tell me a few things about databricks genie\u001B[0m\u001B[32;1m\u001B[1;3mBased on your previous conversations, I found that you had discussions about audience segmentation technologies and data structures used in AdTech. However, I don't see a specific \"chat history idea\" mentioned in the retrieved information.\n\nThe search results show detailed discussions about:\n1. Common data structures used in audience segmentation (user profiles, cookies, taxonomies, etc.)\n2. Types of databases commonly used in AdTech (relational, NoSQL, data warehouses, etc.)\n3. A brief mention of \"databricks genie\"\n\nIf you're referring to a specific idea you shared in a previous conversation, could you provide more details about what you're looking for? Or perhaps you're referring to something else that wasn't captured in the search results?\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/15 14:56:26 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-7fef0fd622ec4243ad22c25f25ba3db4\n2025/08/15 14:56:26 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\nFailed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fa564496660> at 0x7fa559728280> was created in a different Context\nFailed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fa564496660> at 0x7fa557f21280> was created in a different Context\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n\u001B[32;1m\u001B[1;3m\nInvoking: `search_chat_history` with `chat history idea`\nresponded: I don't have enough information from the current conversation context to determine what specific chat history idea you're referring to. To help you with this question, I'll need to search your previous conversations.\n\n\u001B[0m"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fa564496660> at 0x7fa558154180> was created in a different Context\nFailed to detach context\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f59edc39-7232-4308-a77a-33b76bd2f69a/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7fa564496660> at 0x7fa558156100> was created in a different Context\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36;1m\u001B[1;3mPassage: # Common Data Structures Used in Audience Segmentation\n\nBuilding on our discussion about audience segmentation technologies, here are the common data structures used in the audience segmentation process:\n\n## User Profiles\n- Individual-level data structures containing attributes and behaviors\n- Often stored as JSON objects or relational database records\n- Include identifiers, demographics, behaviors, and preferences\n\n## Cookies and Device IDs\n- Small text files or unique identifiers stored on user devices\n- Used to recognize users across sessions and websites\n- Examples: first-party cookies, third-party cookies, mobile advertising IDs\n\n## Taxonomies and Hierarchies\n- Organized classification systems for categorizing audience attributes\n- Allow for both broad and granular segmentation\n- Example: Interest categories organized from general to specific\n\n## Segment Definitions\n- Rules-based or algorithmic definitions of audience groups\n- Can be simple (e.g., \"males aged 25-34\") or complex (combining multiple attributes)\n- Often represented as query expressions or decision trees\n\n## Look-alike Models\n- Statistical representations of similarities between users\n- Often implemented as vector spaces or probabilistic models\n- Used to find new users who resemble existing high-value customers\n\n## Cross-Device Graphs\n- Data structures mapping relationships between different devices\n- Enable consistent identification of users across multiple devices\n- Typically implemented as graph databases\n\n## Event Streams\n- Time-series data representing user actions and behaviors\n- Often processed using stream processing frameworks\n- Used for real-time segmentation and targeting\n\n## Hashed User Identifiers\n- Privacy-compliant representations of user IDs\n- Allow for data matching without exposing personally identifiable information\n- Examples: hashed emails, phone numbers, or device IDs\n\nThese data structures form the foundation of audience segmentation systems, enabling marketers to organize, analyze, and activate audience data effectively across the AdTech ecosystem.\nPassage: # Databases Commonly Used in AdTech\n\nBuilding on our previous discussions about audience segmentation technologies and data structures, here are the types of databases commonly used in AdTech use cases:\n\n## Relational Databases\n- Used for structured data with well-defined relationships\n- Good for transaction processing and data integrity\n- Examples: MySQL, PostgreSQL, Oracle\n- AdTech use: Customer information, campaign management, billing systems\n\n## NoSQL Databases\n- Designed for flexible schemas and horizontal scalability\n- Types include document, key-value, column-family, and graph databases\n- Examples: MongoDB (document), Redis (key-value), Cassandra (column-family), Neo4j (graph)\n- AdTech use: User profiles, real-time bidding data, flexible attribute storage\n\n## Data Warehouses\n- Optimized for analytics and reporting on large datasets\n- Support complex queries across massive amounts of data\n- Examples: Google BigQuery, Amazon Redshift, Snowflake\n- AdTech use: Campaign performance analysis, audience insights, attribution modeling\n\n## Time-Series Databases\n- Specialized for time-stamped or time-series data\n- Optimized for measuring changes over time\n- Examples: InfluxDB, TimescaleDB\n- AdTech use: User behavior tracking, campaign performance over time\n\n## In-Memory Databases\n- Store data primarily in RAM for extremely fast access\n- Used for real-time processing needs\n- Examples: Redis, Memcached, SAP HANA\n- AdTech use: Real-time bidding, ad serving decisions, user targeting\n\n## Graph Databases\n- Specialized for managing highly connected data\n- Efficient at traversing relationships between entities\n- Examples: Neo4j, Amazon Neptune\n- AdTech use: Identity resolution, cross-device mapping, interest networks\n\n## Data Lake Technologies\n- Store vast amounts of raw, unprocessed data\n- Support diverse data types and formats\n- Examples: Apache Hadoop, Amazon S3 with Athena\n- AdTech use: Long-term data storage, machine learning training data\n\n## Hybrid and Multi-Model Databases\n- Combine features of different database types\n- Provide flexibility for various data models\n- Examples: ArangoDB\nPassage: Tell me a few things about databricks genie\u001B[0m"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/15 14:56:34 INFO mlflow.models.model: Found the following environment variables used during model inference: [DATABRICKS_HOST, DATABRICKS_TOKEN]. Please check if you need to set them when deploying the model. To disable this message, set environment variable `MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING` to `false`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3mBased on your previous conversations, I can see you've discussed several technical topics related to AdTech, data structures for audience segmentation, and database technologies. However, I don't see a specific \"chat history idea\" mentioned in the search results.\n\nThe search shows detailed discussions about:\n1. Common data structures used in audience segmentation\n2. Types of databases commonly used in AdTech\n3. A brief mention of \"databricks genie\"\n\nIf you're referring to a specific idea you shared in a previous conversation that isn't appearing in these results, could you provide more details about what you're looking for? Or perhaps you're referring to one of these topics we just found?\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<head>\n",
       "  <link\n",
       "    rel=\"stylesheet\"\n",
       "    href=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/xcode.min.css\"\n",
       "  />\n",
       "  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js\"></script>\n",
       "  <script>\n",
       "    hljs.highlightAll();\n",
       "  </script>\n",
       "  <style>\n",
       "    body {\n",
       "      margin: 0;\n",
       "      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto,\n",
       "        Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji,\n",
       "        Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;\n",
       "      -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n",
       "      margin: 0;\n",
       "      font-weight: 400;\n",
       "      font-size: 13px;\n",
       "      line-height: 18px;\n",
       "      color: rgb(17, 23, 28);\n",
       "    }\n",
       "    code {\n",
       "      line-height: 18px;\n",
       "      font-size: 11px;\n",
       "      background: rgb(250, 250, 250) !important;\n",
       "    }\n",
       "    pre {\n",
       "      background: rgb(250, 250, 250);\n",
       "      margin: 0;\n",
       "      display: none;\n",
       "    }\n",
       "    pre.active {\n",
       "      display: unset;\n",
       "    }\n",
       "    button {\n",
       "      white-space: nowrap;\n",
       "      text-align: center;\n",
       "      position: relative;\n",
       "      cursor: pointer;\n",
       "      background: rgba(34, 114, 180, 0) !important;\n",
       "      color: rgb(34, 114, 180) !important;\n",
       "      border-color: rgba(34, 114, 180, 0) !important;\n",
       "      padding: 4px 6px !important;\n",
       "      text-decoration: none !important;\n",
       "      line-height: 20px !important;\n",
       "      box-shadow: none !important;\n",
       "      height: 32px !important;\n",
       "      display: inline-flex !important;\n",
       "      -webkit-box-align: center !important;\n",
       "      align-items: center !important;\n",
       "      -webkit-box-pack: center !important;\n",
       "      justify-content: center !important;\n",
       "      vertical-align: middle !important;\n",
       "    }\n",
       "    p {\n",
       "      margin: 0;\n",
       "      padding: 0;\n",
       "    }\n",
       "    button:hover {\n",
       "      background: rgba(34, 114, 180, 0.08) !important;\n",
       "      color: rgb(14, 83, 139) !important;\n",
       "    }\n",
       "    button:active {\n",
       "      background: rgba(34, 114, 180, 0.16) !important;\n",
       "      color: rgb(4, 53, 93) !important;\n",
       "    }\n",
       "    h1 {\n",
       "      margin-top: 4px;\n",
       "      font-size: 22px;\n",
       "    }\n",
       "    .info {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      line-height: 16px;\n",
       "      color: rgb(95, 114, 129);\n",
       "    }\n",
       "    .tabs {\n",
       "      margin-top: 10px;\n",
       "      border-bottom: 1px solid rgb(209, 217, 225) !important;\n",
       "      display: flex;\n",
       "      line-height: 24px;\n",
       "    }\n",
       "    .tab {\n",
       "      font-size: 13px;\n",
       "      font-weight: 600 !important;\n",
       "      cursor: pointer;\n",
       "      margin: 0 24px 0 2px;\n",
       "      padding-left: 2px;\n",
       "    }\n",
       "    .tab:hover {\n",
       "      color: rgb(14, 83, 139) !important;\n",
       "    }\n",
       "    .tab.active {\n",
       "      border-bottom: 3px solid rgb(34, 114, 180) !important;\n",
       "    }\n",
       "    .link {\n",
       "      margin-left: 12px;\n",
       "      display: inline-block;\n",
       "      text-decoration: none;\n",
       "      color: rgb(34, 114, 180) !important;\n",
       "      font-size: 13px;\n",
       "      font-weight: 400;\n",
       "    }\n",
       "    .link:hover {\n",
       "      color: rgb(14, 83, 139) !important;\n",
       "    }\n",
       "    .link-content {\n",
       "      display: flex;\n",
       "      gap: 6px;\n",
       "      align-items: center;\n",
       "    }\n",
       "    .caret-up {\n",
       "      transform: rotate(180deg);\n",
       "    }\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "  <div style=\"display: flex; align-items: center\">\n",
       "    The logged model is compatible with the Mosaic AI Agent Framework.\n",
       "    <button onclick=\"toggleCode()\">\n",
       "      See how to evaluate the model&nbsp;\n",
       "      <span\n",
       "        role=\"img\"\n",
       "        id=\"caret\"\n",
       "        aria-hidden=\"true\"\n",
       "        class=\"anticon css-6xix1i\"\n",
       "        style=\"font-size: 14px\"\n",
       "        ><svg\n",
       "          xmlns=\"http://www.w3.org/2000/svg\"\n",
       "          width=\"1em\"\n",
       "          height=\"1em\"\n",
       "          fill=\"none\"\n",
       "          viewBox=\"0 0 16 16\"\n",
       "          aria-hidden=\"true\"\n",
       "          focusable=\"false\"\n",
       "          class=\"\"\n",
       "        >\n",
       "          <path\n",
       "            fill=\"currentColor\"\n",
       "            fill-rule=\"evenodd\"\n",
       "            d=\"M8 8.917 10.947 6 12 7.042 8 11 4 7.042 5.053 6z\"\n",
       "            clip-rule=\"evenodd\"\n",
       "          ></path>\n",
       "        </svg>\n",
       "      </span>\n",
       "    </button>\n",
       "  </div>\n",
       "  <div id=\"code\" style=\"display: none\">\n",
       "    <h1>\n",
       "      Agent evaluation\n",
       "      <a\n",
       "        class=\"link\"\n",
       "        href=\"https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook\"\n",
       "        target=\"_blank\"\n",
       "      >\n",
       "        <span class=\"link-content\">\n",
       "          Learn more\n",
       "          <span role=\"img\" aria-hidden=\"true\" class=\"anticon css-6xix1i\"\n",
       "            ><svg\n",
       "              xmlns=\"http://www.w3.org/2000/svg\"\n",
       "              width=\"1em\"\n",
       "              height=\"1em\"\n",
       "              fill=\"none\"\n",
       "              viewBox=\"0 0 16 16\"\n",
       "              aria-hidden=\"true\"\n",
       "              focusable=\"false\"\n",
       "              class=\"\"\n",
       "            >\n",
       "              <path\n",
       "                fill=\"currentColor\"\n",
       "                d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"\n",
       "              ></path>\n",
       "              <path\n",
       "                fill=\"currentColor\"\n",
       "                d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"\n",
       "              ></path></svg></span></span\n",
       "      ></a>\n",
       "    </h1>\n",
       "    <p class=\"info\">\n",
       "      Copy the following code snippet in a notebook cell (right click → copy)\n",
       "    </p>\n",
       "    <div class=\"tabs\">\n",
       "      <div class=\"tab active\" onclick=\"tabClicked(0)\">Using synthetic data</div>\n",
       "      <div class=\"tab\" onclick=\"tabClicked(1)\">Using your own dataset</div>\n",
       "    </div>\n",
       "    <div style=\"height: 472px\">\n",
       "      <pre\n",
       "        class=\"active\"\n",
       "      ><code class=\"language-python\">%pip install -U databricks-agents\n",
       "dbutils.library.restartPython()\n",
       "## Run the above in a separate cell ##\n",
       "\n",
       "from databricks.agents.evals import generate_evals_df\n",
       "import mlflow\n",
       "\n",
       "agent_description = &quot;A chatbot that answers questions about Databricks.&quot;\n",
       "question_guidelines = &quot;&quot;&quot;\n",
       "# User personas\n",
       "- A developer new to the Databricks platform\n",
       "# Example questions\n",
       "- What API lets me parallelize operations over rows of a delta table?\n",
       "&quot;&quot;&quot;\n",
       "# TODO: Spark/Pandas DataFrame with &quot;content&quot; and &quot;doc_uri&quot; columns.\n",
       "docs = spark.table(&quot;catalog.schema.my_table_of_docs&quot;)\n",
       "evals = generate_evals_df(\n",
       "    docs=docs,\n",
       "    num_evals=25,\n",
       "    agent_description=agent_description,\n",
       "    question_guidelines=question_guidelines,\n",
       ")\n",
       "eval_result = mlflow.evaluate(data=evals, model=&quot;models:/m-7fef0fd622ec4243ad22c25f25ba3db4&quot;, model_type=&quot;databricks-agent&quot;)\n",
       "</code></pre>\n",
       "\n",
       "      <pre><code class=\"language-python\">%pip install -U databricks-agents\n",
       "dbutils.library.restartPython()\n",
       "## Run the above in a separate cell ##\n",
       "\n",
       "import pandas as pd\n",
       "import mlflow\n",
       "\n",
       "evals = [\n",
       "    {\n",
       "        &quot;request&quot;: {\n",
       "            &quot;messages&quot;: [\n",
       "                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How do I convert a Spark DataFrame to Pandas?&quot;}\n",
       "            ],\n",
       "        },\n",
       "        # Optional, needed for judging correctness.\n",
       "        &quot;expected_facts&quot;: [\n",
       "            &quot;To convert a Spark DataFrame to Pandas, you can use the toPandas() method.&quot;\n",
       "        ],\n",
       "    }\n",
       "]\n",
       "eval_result = mlflow.evaluate(\n",
       "    data=pd.DataFrame.from_records(evals), model=&quot;models:/m-7fef0fd622ec4243ad22c25f25ba3db4&quot;, model_type=&quot;databricks-agent&quot;\n",
       ")\n",
       "</code></pre>\n",
       "    </div>\n",
       "  </div>\n",
       "  <script>\n",
       "    var codeShown = false;\n",
       "    function clip(el) {\n",
       "      var range = document.createRange();\n",
       "      range.selectNodeContents(el);\n",
       "      var sel = window.getSelection();\n",
       "      sel.removeAllRanges();\n",
       "      sel.addRange(range);\n",
       "    }\n",
       "\n",
       "    function toggleCode() {\n",
       "      if (codeShown) {\n",
       "        document.getElementById(\"code\").style.display = \"none\";\n",
       "        codeShown = false;\n",
       "      } else {\n",
       "        document.getElementById(\"code\").style.display = \"block\";\n",
       "        clip(document.querySelector(\"pre.active\"));\n",
       "        codeShown = true;\n",
       "      }\n",
       "      document.getElementById(\"caret\").classList.toggle(\"caret-up\");\n",
       "    }\n",
       "\n",
       "    function tabClicked(tabIndex) {\n",
       "      document.querySelectorAll(\".tab\").forEach((tab, index) => {\n",
       "        if (index === tabIndex) {\n",
       "          tab.classList.add(\"active\");\n",
       "        } else {\n",
       "          tab.classList.remove(\"active\");\n",
       "        }\n",
       "      });\n",
       "      document.querySelectorAll(\"pre\").forEach((pre, index) => {\n",
       "        if (index === tabIndex) {\n",
       "          pre.classList.add(\"active\");\n",
       "        } else {\n",
       "          pre.classList.remove(\"active\");\n",
       "        }\n",
       "      });\n",
       "      clip(document.querySelector(\"pre.active\"));\n",
       "    }\n",
       "  </script>\n",
       "</body>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'tanner_wendland.default.chat_history_agent_postgres' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabf324f96f8410da2168ef83528ceb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9813eb01cc44b0a495b1e32696e2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 Created version '4' of model 'tanner_wendland.default.chat_history_agent_postgres': https://e2-demo-field-eng.cloud.databricks.com/explore/data/models/tanner_wendland/default/chat_history_agent_postgres/version/4?o=1444828305810485\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models.resources import DatabricksVectorSearchIndex, DatabricksServingEndpoint\n",
    "\n",
    "chain_file_path = os.path.join(os.getcwd(), 'chain_postgres.py')\n",
    "if not os.path.exists(chain_file_path):\n",
    "    raise FileNotFoundError(f\"Chain file not found at {chain_file_path}\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "os.environ['DATABRICKS_HOST'] = f\"https://{workspace_url}\"\n",
    "os.environ['DATABRICKS_TOKEN'] = dbutils.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=\"adtech_chat_history_agent_postgres\"):\n",
    "  logged_chain_info = mlflow.langchain.log_model(\n",
    "          #Note: In classical ML, MLflow works by serializing the model object.  In generative AI, chains often include Python packages that do not serialize.  Here, we use MLflow's new code-based logging, where we saved our chain under the chain notebook and will use this code instead of trying to serialize the object.\n",
    "          lc_model=os.path.join(os.getcwd(), 'chain_postgres.py'),  # Chain code file e.g., /path/to/the/chain.py \n",
    "          model_config=chain_config, # Chain configuration \n",
    "          artifact_path=\"chain_postgres\", # Required by MLflow, the chain's code/config are saved in this directory\n",
    "          input_example=input_example,\n",
    "          # Specify resources for automatic authentication passthrough\n",
    "          resources=[\n",
    "            DatabricksServingEndpoint(endpoint_name=model_config.get(\"llm_model_serving_endpoint_name\"))\n",
    "          ]\n",
    "      )\n",
    "\n",
    "model_name = \"chat_history_agent_postgres\"\n",
    "MODEL_NAME_FQN = f\"{target_catalog}.{target_schema}.{model_name}\"\n",
    "# Register to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=MODEL_NAME_FQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0141fcd2-95ff-4d57-bfa4-3c55a38f6946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9639d4-1d86-40b3-87b4-b46c8b7be046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"secert_scope\", \"field-eng\", \"Secret Scope\")\n",
    "dbutils.widgets.text(\"secret_key\", \"app-secret\", \"Secret Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93d99ca7-c565-4092-9f28-aff21fcebfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret_scope = dbutils.widgets.get(\"secert_scope\")\n",
    "secret_key = dbutils.widgets.get(\"secret_key\")\n",
    "\n",
    "secret_value = dbutils.secrets.get(scope=secret_scope, key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8163698188724699,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbed8a02-2931-4a9f-aa13-e3f2766bf1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating endpoint tanner_wendland-default-chat_history_agent_postgres...\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedEntityInput\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "\n",
    "version = uc_registered_model_info.version\n",
    "serving_endpoint_name = MODEL_NAME_FQN.replace(\".\", \"-\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "config = {\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "                \"name\": serving_endpoint_name,\n",
    "                \"entity_name\": MODEL_NAME_FQN,\n",
    "                \"entity_version\": version,\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": True,\n",
    "                \"environment_vars\": {\n",
    "                    'DATABRICKS_HOST': workspace_url,\n",
    "                    'DATABRICKS_TOKEN': secret_value\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def does_endpoint_exists(endpoint_name):\n",
    "    try:\n",
    "        workspace_client.serving_endpoints.get(endpoint_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if not does_endpoint_exists(serving_endpoint_name):\n",
    "    print(f\"Creating endpoint {serving_endpoint_name}...\")\n",
    "    workspace_client.serving_endpoints.create_and_wait(\n",
    "        serving_endpoint_name,\n",
    "        config=EndpointCoreConfigInput.from_dict(config)\n",
    "    )\n",
    "else:\n",
    "    print(f\"Updating endpoint {serving_endpoint_name}...\")\n",
    "    workspace_client.serving_endpoints.update_config_and_wait(\n",
    "        serving_endpoint_name,\n",
    "        served_entities=[ServedEntityInput.from_dict(entity) for entity in config['served_entities']]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02.1-chat-history-agent-pg",
   "widgets": {
    "database_instance_name": {
     "currentValue": "tannerw-adtech-db",
     "nuid": "5935864b-337f-4cea-978f-00fcf2a24646",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tannerw-adtech-db",
      "label": null,
      "name": "database_instance_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "tannerw-adtech-db",
      "label": null,
      "name": "database_instance_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "embedding_model": {
     "currentValue": "databricks-gte-large-en",
     "nuid": "96111585-d614-405c-960e-ae589ccc89fc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_model",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_model",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "llm_model_serving_endpoint_name": {
     "currentValue": "databricks-claude-3-7-sonnet",
     "nuid": "4e5299b7-86bb-4511-bd44-8d76be56893e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-claude-3-7-sonnet",
      "label": null,
      "name": "llm_model_serving_endpoint_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-claude-3-7-sonnet",
      "label": null,
      "name": "llm_model_serving_endpoint_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "postgres_database_name": {
     "currentValue": "databricks_postgres",
     "nuid": "e75a0daa-dd0b-412d-b393-29edb1e3b1f1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks_postgres",
      "label": null,
      "name": "postgres_database_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks_postgres",
      "label": null,
      "name": "postgres_database_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "secert_scope": {
     "currentValue": "field-eng",
     "nuid": "e227bce8-9119-4ce0-9bd1-7bb37e315a10",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "field-eng",
      "label": "Secret Scope",
      "name": "secert_scope",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "field-eng",
      "label": "Secret Scope",
      "name": "secert_scope",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "secret_key": {
     "currentValue": "app-secret",
     "nuid": "90c15fd3-85b5-4532-9fb1-cd73d8f32360",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "app-secret",
      "label": "Secret Key",
      "name": "secret_key",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "app-secret",
      "label": "Secret Key",
      "name": "secret_key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_catalog": {
     "currentValue": "tanner_wendland",
     "nuid": "9ce71cf8-98af-4e13-8cc0-71dc8c6cf387",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tanner_wendland",
      "label": null,
      "name": "target_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "tanner_wendland",
      "label": null,
      "name": "target_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_schema": {
     "currentValue": "default",
     "nuid": "eb61b685-72df-4bd5-bf4e-80e589c9adb5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "target_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": null,
      "name": "target_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}