{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf63ecd-9099-40b4-aa3a-0700f2576e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks-sdk==0.61.0\n  Downloading databricks_sdk-0.61.0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: pyarrow<20 in /databricks/python3/lib/python3.11/site-packages (14.0.1)\nCollecting databricks-agents==1.2.0\n  Downloading databricks_agents-1.2.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting mlflow<=3.1\n  Downloading mlflow-3.1.0-py3-none-any.whl.metadata (29 kB)\nCollecting databricks-vectorsearch==0.57\n  Downloading databricks_vectorsearch-0.57-py3-none-any.whl.metadata (2.8 kB)\nCollecting langchain==0.3.27\n  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\nCollecting langchain-mcp\n  Downloading langchain_mcp-0.2.1-py3-none-any.whl.metadata (1.7 kB)\nCollecting langchain_core==0.3.74\n  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\nCollecting databricks-langchain==0.7.0\n  Downloading databricks_langchain-0.7.0-py3-none-any.whl.metadata (2.8 kB)\nCollecting bs4\n  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\nCollecting dotenv\n  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\nCollecting psycopg2-binary==2.9.7\n  Downloading psycopg2_binary-2.9.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nCollecting pgvector==0.2.5\n  Downloading pgvector-0.2.5-py2.py3-none-any.whl.metadata (9.9 kB)\nCollecting langgraph==0.3.4\n  Downloading langgraph-0.3.4-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: requests<3,>=2.28.1 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk==0.61.0) (2.31.0)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk==0.61.0) (2.35.0)\nRequirement already satisfied: databricks-connect in /databricks/python3/lib/python3.11/site-packages (from databricks-agents==1.2.0) (15.4.12)\nCollecting dataclasses-json (from databricks-agents==1.2.0)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nCollecting jinja2>=3.0.0 (from databricks-agents==1.2.0)\n  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\nCollecting mlflow-skinny<4.0.0,>=3.1.0 (from databricks-agents==1.2.0)\n  Downloading mlflow_skinny-3.3.1-py3-none-any.whl.metadata (31 kB)\nCollecting tenacity>=8.5 (from databricks-agents==1.2.0)\n  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\nCollecting tiktoken>=0.8.0 (from databricks-agents==1.2.0)\n  Downloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting tqdm (from databricks-agents==1.2.0)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nCollecting urllib3>=2.0 (from databricks-agents==1.2.0)\n  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting pydantic>=2 (from databricks-agents==1.2.0)\n  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\nCollecting whenever==0.7.3 (from databricks-agents==1.2.0)\n  Downloading whenever-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: boto3>1 in /databricks/python3/lib/python3.11/site-packages (from databricks-agents==1.2.0) (1.34.39)\nRequirement already satisfied: botocore in /databricks/python3/lib/python3.11/site-packages (from databricks-agents==1.2.0) (1.34.39)\nRequirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-vectorsearch==0.57) (5.29.3)\nCollecting deprecation>=2 (from databricks-vectorsearch==0.57)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain==0.3.27)\n  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\nCollecting langsmith>=0.1.17 (from langchain==0.3.27)\n  Downloading langsmith-0.4.15-py3-none-any.whl.metadata (14 kB)\nCollecting SQLAlchemy<3,>=1.4 (from langchain==0.3.27)\n  Downloading sqlalchemy-2.0.43-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.3.27) (6.0)\nCollecting jsonpatch<2.0,>=1.33 (from langchain_core==0.3.74)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /databricks/python3/lib/python3.11/site-packages (from langchain_core==0.3.74) (4.10.0)\nRequirement already satisfied: packaging>=23.2 in /databricks/python3/lib/python3.11/site-packages (from langchain_core==0.3.74) (23.2)\nCollecting databricks-ai-bridge>=0.7.0 (from databricks-langchain==0.7.0)\n  Downloading databricks_ai_bridge-0.7.0-py3-none-any.whl.metadata (6.2 kB)\nCollecting databricks-connect (from databricks-agents==1.2.0)\n  Downloading databricks_connect-16.1.6-py2.py3-none-any.whl.metadata (2.6 kB)\nCollecting openai>=1.99.9 (from databricks-langchain==0.7.0)\n  Downloading openai-1.100.2-py3-none-any.whl.metadata (29 kB)\nCollecting unitycatalog-langchain>=0.2.0 (from unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n  Downloading unitycatalog_langchain-0.2.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.11/site-packages (from pgvector==0.2.5) (1.23.5)\nCollecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph==0.3.4)\n  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\nCollecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph==0.3.4)\n  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\nCollecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph==0.3.4)\n  Downloading langgraph_sdk-0.1.74-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: ipython<10,>=8 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk[notebook]) (8.25.0)\nCollecting ipywidgets<9,>=8 (from databricks-sdk[notebook])\n  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\nCollecting mlflow-skinny<4.0.0,>=3.1.0 (from databricks-agents==1.2.0)\n  Downloading mlflow_skinny-3.1.0-py3-none-any.whl.metadata (30 kB)\nCollecting Flask<4 (from mlflow<=3.1)\n  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting alembic!=1.10.0,<2 (from mlflow<=3.1)\n  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\nCollecting docker<8,>=4.0.0 (from mlflow<=3.1)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphene<4 (from mlflow<=3.1)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow<=3.1)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (3.7.2)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (1.5.3)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (1.3.0)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (1.11.1)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (8.0.4)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (3.0.0)\nCollecting fastapi<1 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (6.0.0)\nCollecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (0.5.1)\nCollecting uvicorn<1 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.11/site-packages (from mlflow[databricks]) (12.14.0)\nRequirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow[databricks]) (2.18.2)\nCollecting mcp~=1.0 (from langchain-mcp)\n  Downloading mcp-1.13.0-py3-none-any.whl.metadata (68 kB)\nCollecting typing-extensions>=4.7 (from langchain_core==0.3.74)\n  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting beautifulsoup4 (from bs4)\n  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\nCollecting python-dotenv (from dotenv)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow<=3.1)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: azure-core<2.0.0,>=1.28.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (1.32.0)\nRequirement already satisfied: azure-storage-blob<13.0.0,>=12.19.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (12.19.1)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (0.7.2)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.11/site-packages (from boto3>1->databricks-agents==1.2.0) (0.10.0)\nRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.11/site-packages (from boto3>1->databricks-agents==1.2.0) (0.10.3)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.11/site-packages (from botocore->databricks-agents==1.2.0) (2.8.2)\nCollecting urllib3>=2.0 (from databricks-agents==1.2.0)\n  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\nCollecting tabulate>=0.9.0 (from databricks-ai-bridge>=0.7.0->databricks-langchain==0.7.0)\n  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\nRequirement already satisfied: googleapis-common-protos>=1.56.4 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (1.65.0)\nRequirement already satisfied: grpcio-status>=1.59.3 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (1.69.0)\nRequirement already satisfied: grpcio>=1.59.3 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (1.69.0)\nRequirement already satisfied: py4j==0.10.9.7 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (0.10.9.7)\nRequirement already satisfied: setuptools>=68.0.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (75.1.0)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from databricks-connect->databricks-agents==1.2.0) (1.16.0)\nCollecting langchain-openai (from databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0)\n  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\nCollecting httpx (from databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting blinker>=1.9.0 (from Flask<4->mlflow<=3.1)\n  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting click<9,>=7.0 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\nCollecting itsdangerous>=2.2.0 (from Flask<4->mlflow<=3.1)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nCollecting markupsafe>=2.1.1 (from Flask<4->mlflow<=3.1)\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow<=3.1)\n  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.61.0) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.61.0) (4.9)\nRequirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.18.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.4.1)\nRequirement already satisfied: google-resumable-media>=2.7.2 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (1.6.0)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow<=3.1)\n  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow<=3.1)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: decorator in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.18.1)\nRequirement already satisfied: matplotlib-inline in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (3.0.48)\nRequirement already satisfied: pygments>=2.4.0 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (2.15.1)\nRequirement already satisfied: stack-data in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.2.0)\nRequirement already satisfied: traitlets>=5.13.0 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (5.13.0)\nRequirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (4.8.0)\nCollecting comm>=0.1.3 (from ipywidgets<9,>=8->databricks-sdk[notebook])\n  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\nCollecting widgetsnbextension~=4.0.14 (from ipywidgets<9,>=8->databricks-sdk[notebook])\n  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\nCollecting jupyterlab_widgets~=3.0.15 (from ipywidgets<9,>=8->databricks-sdk[notebook])\n  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\nCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core==0.3.74)\n  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\nCollecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph==0.3.4)\n  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\nCollecting orjson>=3.10.1 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.4)\n  Downloading orjson-3.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\nCollecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain==0.3.27)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.23.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (10.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (3.0.9)\nCollecting anyio>=4.5 (from mcp~=1.0->langchain-mcp)\n  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\nCollecting httpx-sse>=0.4 (from mcp~=1.0->langchain-mcp)\n  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting jsonschema>=4.20.0 (from mcp~=1.0->langchain-mcp)\n  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\nCollecting pydantic-settings>=2.5.2 (from mcp~=1.0->langchain-mcp)\n  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\nCollecting python-multipart>=0.0.9 (from mcp~=1.0->langchain-mcp)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nCollecting sse-starlette>=1.6.1 (from mcp~=1.0->langchain-mcp)\n  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\nCollecting starlette>=0.27 (from mcp~=1.0->langchain-mcp)\n  Downloading starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.9->databricks-langchain==0.7.0) (1.7.0)\nCollecting jiter<1,>=0.4.0 (from openai>=1.99.9->databricks-langchain==0.7.0)\n  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nCollecting sniffio (from openai>=1.99.9->databricks-langchain==0.7.0)\n  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas<3->mlflow<=3.1) (2022.7)\nCollecting annotated-types>=0.6.0 (from pydantic>=2->databricks-agents==1.2.0)\n  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\nCollecting pydantic-core==2.33.2 (from pydantic>=2->databricks-agents==1.2.0)\n  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic>=2->databricks-agents==1.2.0)\n  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (2023.7.22)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow<=3.1) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow<=3.1) (2.2.0)\nCollecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain==0.3.27)\n  Downloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nCollecting regex>=2022.1.18 (from tiktoken>=0.8.0->databricks-agents==1.2.0)\n  Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\nCollecting langchain-community>=0.2.0 (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\nCollecting unitycatalog-ai (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n  Downloading unitycatalog_ai-0.3.1-py3-none-any.whl.metadata (31 kB)\nCollecting soupsieve>1.2 (from beautifulsoup4->bs4)\n  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->databricks-agents==1.2.0)\n  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json->databricks-agents==1.2.0)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow[databricks]) (41.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (4.0.11)\nCollecting protobuf<6,>=3.12.0 (from databricks-vectorsearch==0.57)\n  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=1.30.0->mlflow[databricks]) (1.25.0)\nINFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\nCollecting grpcio-status>=1.59.3 (from databricks-connect->databricks-agents==1.2.0)\n  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\nINFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\nCollecting httpcore==1.* (from httpx->databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0)\n  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nCollecting h11>=0.16 (from httpcore==1.*->httpx->databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0)\n  Downl\n\n*** WARNING: max output size exceeded, skipping output. ***\n\ny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (3.11.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /databricks/python3/lib/python3.11/site-packages (from jedi>=0.16->ipython<10,>=8->databricks-sdk[notebook]) (0.8.3)\nCollecting attrs>=22.2.0 (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp)\n  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\nCollecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp)\n  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting referencing>=0.28.4 (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp)\n  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\nCollecting rpds-py>=0.7.1 (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp)\n  Downloading rpds_py-0.27.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nCollecting numpy (from pgvector==0.2.5)\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nCollecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: ptyprocess>=0.5 in /databricks/python3/lib/python3.11/site-packages (from pexpect>4.3->ipython<10,>=8->databricks-sdk[notebook]) (0.7.0)\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython<10,>=8->databricks-sdk[notebook]) (0.2.5)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk==0.61.0) (0.4.8)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->databricks-agents==1.2.0) (0.4.3)\nRequirement already satisfied: executing in /databricks/python3/lib/python3.11/site-packages (from stack-data->ipython<10,>=8->databricks-sdk[notebook]) (0.8.3)\nRequirement already satisfied: asttokens in /databricks/python3/lib/python3.11/site-packages (from stack-data->ipython<10,>=8->databricks-sdk[notebook]) (2.0.5)\nRequirement already satisfied: pure-eval in /databricks/python3/lib/python3.11/site-packages (from stack-data->ipython<10,>=8->databricks-sdk[notebook]) (0.2.2)\nRequirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.11/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (1.5.6)\nCollecting unitycatalog-client (from unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n  Downloading unitycatalog_client-0.3.0-py3-none-any.whl.metadata (7.8 kB)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow[databricks]) (1.15.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (5.0.1)\nCollecting aiohttp-retry>=2.8.3 (from unitycatalog-client->unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow[databricks]) (2.21)\nDownloading databricks_sdk-0.61.0-py3-none-any.whl (680 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/680.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m680.6/680.6 kB\u001B[0m \u001B[31m16.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading databricks_agents-1.2.0-py3-none-any.whl (195 kB)\nDownloading databricks_vectorsearch-0.57-py3-none-any.whl (16 kB)\nDownloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m37.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\nDownloading databricks_langchain-0.7.0-py3-none-any.whl (26 kB)\nDownloading psycopg2_binary-2.9.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m17.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pgvector-0.2.5-py2.py3-none-any.whl (9.6 kB)\nDownloading langgraph-0.3.4-py3-none-any.whl (131 kB)\nDownloading whenever-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (418 kB)\nDownloading mlflow-3.1.0-py3-none-any.whl (24.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/24.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.1/24.7 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m24.6/24.7 MB\u001B[0m \u001B[31m63.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.7/24.7 MB\u001B[0m \u001B[31m45.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_skinny-3.1.0-py3-none-any.whl (1.9 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.9 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m32.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_mcp-0.2.1-py3-none-any.whl (4.1 kB)\nDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\nDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\nDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\nDownloading databricks_ai_bridge-0.7.0-py3-none-any.whl (18 kB)\nDownloading databricks_connect-16.1.6-py2.py3-none-any.whl (2.4 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.4/2.4 MB\u001B[0m \u001B[31m83.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nDownloading docker-7.1.0-py3-none-any.whl (147 kB)\nDownloading flask-3.1.2-py3-none-any.whl (103 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\nDownloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\nDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\nDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\nDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\nDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\nDownloading langgraph_sdk-0.1.74-py3-none-any.whl (50 kB)\nDownloading langsmith-0.4.15-py3-none-any.whl (375 kB)\nDownloading mcp-1.13.0-py3-none-any.whl (160 kB)\nDownloading openai-1.100.2-py3-none-any.whl (787 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/787.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m787.8/787.8 kB\u001B[0m \u001B[31m16.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\nDownloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m50.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading sqlalchemy-2.0.43-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m33.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\nDownloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m55.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\nDownloading unitycatalog_langchain-0.2.0-py3-none-any.whl (5.4 kB)\nDownloading urllib3-2.0.7-py3-none-any.whl (124 kB)\nDownloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\nDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nDownloading anyio-4.10.0-py3-none-any.whl (107 kB)\nDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\nDownloading click-8.2.1-py3-none-any.whl (102 kB)\nDownloading comm-0.2.3-py3-none-any.whl (7.3 kB)\nDownloading fastapi-0.116.1-py3-none-any.whl (95 kB)\nDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\nDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\nDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (587 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/587.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m587.7/587.7 kB\u001B[0m \u001B[31m18.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\nDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\nDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\nDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\nDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\nDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\nDownloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\nDownloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\nDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m20.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/18.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m18.1/18.3 MB\u001B[0m \u001B[31m128.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.3/18.3 MB\u001B[0m \u001B[31m68.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\nDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\nDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\nDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\nDownloading orjson-3.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\nDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\nDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/798.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m798.9/798.9 kB\u001B[0m \u001B[31m33.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\nDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\nDownloading soupsieve-2.7-py3-none-any.whl (36 kB)\nDownloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\nDownloading starlette-0.47.2-py3-none-any.whl (72 kB)\nDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\nDownloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\nDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\nDownloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m34.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\nDownloading unitycatalog_ai-0.3.1-py3-none-any.whl (66 kB)\nDownloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m20.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading attrs-25.3.0-py3-none-any.whl (63 kB)\nDownloading h11-0.16.0-py3-none-any.whl (37 kB)\nDownloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\nDownloading referencing-0.36.2-py3-none-any.whl (26 kB)\nDownloading rpds_py-0.27.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\nDownloading unitycatalog_client-0.3.0-py3-none-any.whl (159 kB)\nDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\nDownloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\nDownloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\nDownloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\nInstalling collected packages: widgetsnbextension, whenever, urllib3, typing-extensions, tqdm, tenacity, tabulate, soupsieve, sniffio, rpds-py, regex, python-multipart, python-dotenv, psycopg2-binary, protobuf, propcache, ormsgpack, orjson, numpy, multidict, marshmallow, markupsafe, jupyterlab_widgets, jsonpointer, jiter, itsdangerous, httpx-sse, h11, gunicorn, greenlet, graphql-core, frozenlist, deprecation, comm, click, blinker, attrs, annotated-types, aiohappyeyeballs, yarl, werkzeug, uvicorn, typing-inspection, typing-inspect, SQLAlchemy, referencing, pydantic-core, pgvector, opentelemetry-api, Mako, jsonpatch, jinja2, httpcore, graphql-relay, dotenv, beautifulsoup4, anyio, aiosignal, tiktoken, starlette, sse-starlette, requests-toolbelt, pydantic, opentelemetry-semantic-conventions, jsonschema-specifications, httpx, grpcio-status, graphene, Flask, docker, dataclasses-json, databricks-sdk, bs4, alembic, aiohttp, pydantic-settings, opentelemetry-sdk, openai, langsmith, langgraph-sdk, jsonschema, ipywidgets, fastapi, databricks-connect, aiohttp-retry, unitycatalog-client, mlflow-skinny, mcp, langchain_core, unitycatalog-ai, mlflow, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langchain-mcp, databricks-vectorsearch, databricks-ai-bridge, langgraph-prebuilt, langchain, langgraph, langchain-community, databricks-agents, unitycatalog-langchain, databricks-langchain\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.16\n    Not uninstalling urllib3 at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'urllib3'. No files were found to uninstall.\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.10.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 8.2.2\n    Not uninstalling tenacity at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'tenacity'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 5.29.3\n    Not uninstalling protobuf at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'numpy'. No files were found to uninstall.\n  Attempting uninstall: comm\n    Found existing installation: comm 0.1.2\n    Not uninstalling comm at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'comm'. No files were found to uninstall.\n  Attempting uninstall: click\n    Found existing installation: click 8.0.4\n    Not uninstalling click at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'click'. No files were found to uninstall.\n  Attempting uninstall: blinker\n    Found existing installation: blinker 1.4\n    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'blinker'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.6\n    Not uninstalling pydantic at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'pydantic'. No files were found to uninstall.\n  Attempting uninstall: grpcio-status\n    Found existing installation: grpcio-status 1.69.0\n    Not uninstalling grpcio-status at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'grpcio-status'. No files were found to uninstall.\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.40.0\n    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n  Attempting uninstall: ipywidgets\n    Found existing installation: ipywidgets 7.7.2\n    Not uninstalling ipywidgets at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'ipywidgets'. No files were found to uninstall.\n  Attempting uninstall: databricks-connect\n    Found existing installation: databricks-connect 15.4.12\n    Not uninstalling databricks-connect at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'databricks-connect'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.11.4\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\nSuccessfully installed Flask-3.1.2 Mako-1.3.10 SQLAlchemy-2.0.43 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiohttp-retry-2.9.1 aiosignal-1.4.0 alembic-1.16.4 annotated-types-0.7.0 anyio-4.10.0 attrs-25.3.0 beautifulsoup4-4.13.4 blinker-1.9.0 bs4-0.0.2 click-8.2.1 comm-0.2.3 databricks-agents-1.2.0 databricks-ai-bridge-0.7.0 databricks-connect-16.1.6 databricks-langchain-0.7.0 databricks-sdk-0.61.0 databricks-vectorsearch-0.57 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 dotenv-0.9.9 fastapi-0.116.1 frozenlist-1.7.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 greenlet-3.2.4 grpcio-status-1.62.3 gunicorn-23.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.1 ipywidgets-8.1.7 itsdangerous-2.2.0 jinja2-3.1.6 jiter-0.10.0 jsonpatch-1.33 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.4.1 jupyterlab_widgets-3.0.15 langchain-0.3.27 langchain-community-0.3.27 langchain-mcp-0.2.1 langchain-openai-0.3.30 langchain-text-splitters-0.3.9 langchain_core-0.3.74 langgraph-0.3.4 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.74 langsmith-0.4.15 markupsafe-3.0.2 marshmallow-3.26.1 mcp-1.13.0 mlflow-3.1.0 mlflow-skinny-3.1.0 multidict-6.6.4 numpy-1.26.4 openai-1.100.2 opentelemetry-api-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 orjson-3.11.2 ormsgpack-1.10.0 pgvector-0.2.5 propcache-0.3.2 protobuf-4.25.8 psycopg2-binary-2.9.7 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 python-multipart-0.0.20 referencing-0.36.2 regex-2025.7.34 requests-toolbelt-1.0.0 rpds-py-0.27.0 sniffio-1.3.1 soupsieve-2.7 sse-starlette-3.0.2 starlette-0.47.2 tabulate-0.9.0 tenacity-9.1.2 tiktoken-0.11.0 tqdm-4.67.1 typing-extensions-4.14.1 typing-inspect-0.9.0 typing-inspection-0.4.1 unitycatalog-ai-0.3.1 unitycatalog-client-0.3.0 unitycatalog-langchain-0.2.0 urllib3-2.0.7 uvicorn-0.35.0 werkzeug-3.1.3 whenever-0.7.3 widgetsnbextension-4.0.14 yarl-1.20.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install 'databricks-sdk==0.61.0' 'pyarrow<20' 'databricks-sdk[notebook]' 'databricks-agents==1.2.0' 'mlflow<=3.1' 'mlflow[databricks]' 'databricks-vectorsearch==0.57' 'langchain==0.3.27' 'langchain-mcp' 'langchain_core==0.3.74' 'databricks-langchain==0.7.0' 'bs4' 'dotenv' 'psycopg2-binary==2.9.7' 'pgvector==0.2.5' 'langgraph==0.3.4'\n",
    "import os\n",
    "if os.environ.get(\"DATABRICKS_RUNTIME_VERSION\"):\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354c76a9-8be0-4c8d-9983-9e57eb961e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"embedding_model\", \"databricks-gte-large-en\")\n",
    "dbutils.widgets.text(\"database_instance_name\", \"tannerw-adtech-db\")\n",
    "dbutils.widgets.text(\"postgres_database_name\", \"databricks_postgres\")\n",
    "dbutils.widgets.text(\"llm_model_serving_endpoint_name\", \"databricks-claude-3-7-sonnet\")\n",
    "dbutils.widgets.text(\"target_catalog\", \"tanner_wendland\")\n",
    "dbutils.widgets.text(\"target_schema\", \"default\")\n",
    "dbutils.widgets.text(\"genie_space_id\", \"01efcca6fdc712d7be87a40ad4a2e33e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99bd51e-5492-4d5d-a971-0ddda14452cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = dbutils.widgets.get(\"embedding_model\")\n",
    "database_instance_name = dbutils.widgets.get(\"database_instance_name\")\n",
    "postgres_database_name = dbutils.widgets.get(\"postgres_database_name\")\n",
    "llm_model_serving_endpoint_name = dbutils.widgets.get(\"llm_model_serving_endpoint_name\")\n",
    "target_catalog = dbutils.widgets.get(\"target_catalog\")\n",
    "target_schema = dbutils.widgets.get(\"target_schema\")\n",
    "genie_space_id = dbutils.widgets.get(\"genie_space_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2750086a-7c5b-40aa-b5f6-28fec9147b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "from typing import Any, Generator, Literal, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "188fb341-aa4b-4717-b2f3-12ef5f50a65c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import os\n",
    "from typing import Optional\n",
    "import uuid\n",
    "\n",
    "host = os.environ.get(\"DATABRICKS_HOST\")\n",
    "token = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "workspace_client = WorkspaceClient(host=host, token=token)\n",
    "\n",
    "def get_postgres_connection(\n",
    "    client: WorkspaceClient,\n",
    "    db_name: str,\n",
    "    database_name: Optional[str] = \"databricks_postgres\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get PostgreSQL connection string using Databricks SDK.\n",
    "\n",
    "    Args:\n",
    "        client (WorkspaceClient): The Databricks workspace client.\n",
    "        db_name (str): The name of the database instance.\n",
    "        database_name (Optional[str], optional): The name of the database to connect to.\n",
    "            Defaults to \"databricks_postgres\".\n",
    "\n",
    "    Returns:\n",
    "        str: PostgreSQL connection string.\n",
    "    \"\"\"\n",
    "    database = client.database.get_database_instance(db_name)\n",
    "    credentials = client.database.generate_database_credential(\n",
    "        instance_names=[db_name],\n",
    "        request_id=str(uuid.uuid4())\n",
    "    )\n",
    "\n",
    "    # Use POSTGRES_GROUP env var as username if set, otherwise use current user\n",
    "    postgres_group = os.getenv('POSTGRES_GROUP')\n",
    "    username = postgres_group if postgres_group else client.current_user.me().user_name\n",
    "\n",
    "    database_info = {\n",
    "        \"host\": database.read_write_dns,\n",
    "        \"port\": \"5432\",\n",
    "        \"database\": database_name,\n",
    "        \"username\": username,\n",
    "        \"password\": credentials.token,\n",
    "        \"ssl_mode\": \"require\"\n",
    "    }\n",
    "\n",
    "    database_url = (\n",
    "        f\"postgresql://{database_info['username']}:{database_info['password']}\"\n",
    "        f\"@{database_info['host']}:{database_info['port']}/\"\n",
    "        f\"{database_info['database']}?sslmode={database_info['ssl_mode']}\"\n",
    "    )\n",
    "\n",
    "    return database_url\n",
    "\n",
    "database_url = get_postgres_connection(workspace_client, database_instance_name, postgres_database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d115d39-47b3-40a6-b143-e4ae3f30c1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.autolog()\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "585760d5-cc0a-4a30-b069-1b57188e8623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9d15d1-397a-4cdf-b948-41a6ad5555f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain_config = {\n",
    "    \"llm_model_serving_endpoint_name\": llm_model_serving_endpoint_name,\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"database_instance_name\": database_instance_name,\n",
    "    \"postgres_database_name\": postgres_database_name,\n",
    "    \"genie_space_id\": genie_space_id,\n",
    "    \"genie_agent_description\": \"You are a an agent that can invoke Genie, a powerful text-to-sql database assistant. You can use this tool to answer questions related to sales pipelines.\",\n",
    "    \"general_assistant_description\": \"The General Assistant agent is a helpful assistant that can answer any question.\",\n",
    "    \"code_agent_description\": \"The Coder agent specializes in solving programming challenges, generating code snippets, debugging issues, and explaining complex coding concepts.\",\n",
    "    \"llm_prompt_template\": \"\"\"You are an assistant that answers questions. You have access to a vector search tool that searches previous conversations, but you should primarily use the immediate conversation context provided. \n",
    "\n",
    "Current conversation context: {context}\n",
    "\n",
    "Instructions:\n",
    "1. First, use the current conversation context above to answer questions\n",
    "2. Only use the vector search tool if the question explicitly refers to information from previous conversations that is NOT in the current context\n",
    "3. If the current conversation context contains sufficient information to answer the question, do NOT use the vector search tool\n",
    "4. The vector search tool should be used sparingly, only when the user is clearly asking about something from their chat history that isn't in the current conversation\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c99472b6-acef-49b6-aafd-a5260d654768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Vector Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a545c6a-6753-4240-b196-2b284d419534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "retriever_config = {\n",
    "    \"parameters\": {\n",
    "        \"k\": 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fe9afa93-e6f1-4616-8b92-b99fa8f143af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "# combine dynamic and static filters for vector search\n",
    "def create_configurable_with_filters(input: Dict, retriever_config: Dict) -> Dict:\n",
    "   \"\"\"\n",
    "   create configurable object with filters.\n",
    "   Args:\n",
    "       input: The input data containing filters.\n",
    "   Returns:\n",
    "       A configurable object with filters added to the search_kwargs.\n",
    "   \"\"\"\n",
    "   if \"custom_inputs\" in input:\n",
    "       filters = input[\"custom_inputs\"][\"filters\"]\n",
    "   else:\n",
    "       filters = {}\n",
    "   print(filters)\n",
    "   configurable = {\n",
    "       \"configurable\": {\n",
    "           \"search_kwargs\": {\n",
    "               \"k\": retriever_config.get(\"parameters\")[\"k\"],\n",
    "               \"filter\": filters\n",
    "           }\n",
    "       }\n",
    "   }\n",
    "   return configurable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bfb5894-e178-45c4-8874-b882e3f46b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9c02e33-0b5b-402b-b659-041490a9c82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, List\n",
    "from databricks_langchain import DatabricksEmbeddings\n",
    "from sqlalchemy import create_engine, text, event\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "# Use psycopg2 driver explicitly for SQLAlchemy\n",
    "database_url_sqlalchemy = database_url.replace(\"postgresql://\", \"postgresql+psycopg2://\")\n",
    "\n",
    "engine = create_engine(database_url_sqlalchemy, pool_pre_ping=True)\n",
    "\n",
    "@event.listens_for(engine, \"connect\")\n",
    "def _register_vector(dbapi_connection, connection_record):\n",
    "    # Register pgvector adapter for psycopg2 so Python lists map to the vector type\n",
    "    register_vector(dbapi_connection)\n",
    "\n",
    "def pg_vector_similarity_search(\n",
    "    embeddings: DatabricksEmbeddings,\n",
    "    query: str,\n",
    "    k: int = 3,\n",
    "    filters: Optional[Dict[str, Any]] = None,\n",
    ") -> str:\n",
    "    # 1) Embed the query\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    # 2) Build where clause from filters\n",
    "    filters = filters or {}\n",
    "    where_conditions: List[str] = []\n",
    "    params: Dict[str, Any] = {}\n",
    "\n",
    "    if \"user_name\" in filters:\n",
    "        where_conditions.append(\"me.user_name = :user_name\")\n",
    "        params[\"user_name\"] = filters[\"user_name\"]\n",
    "\n",
    "    if \"chat_id\" in filters:\n",
    "        where_conditions.append(\"me.chat_id = :chat_id\")\n",
    "        params[\"chat_id\"] = filters[\"chat_id\"]\n",
    "\n",
    "    where_clause = \"\"\n",
    "    if where_conditions:\n",
    "        where_clause = \"WHERE \" + \" AND \".join(where_conditions)\n",
    "\n",
    "    # 3) Query using cosine distance operator (<=>); requires pgvector\n",
    "    sql = text(f\"\"\"\n",
    "        SELECT\n",
    "            ch.message_content,\n",
    "            me.user_name,\n",
    "            me.chat_id,\n",
    "            ch.message_type,\n",
    "            ch.created_at,\n",
    "            ch.message_order,\n",
    "            (me.embedding <=> CAST(:query_embedding AS vector)) AS distance\n",
    "        FROM message_embeddings me\n",
    "        JOIN chat_history ch ON me.message_id = ch.id\n",
    "        {where_clause}\n",
    "        ORDER BY me.embedding <=> CAST(:query_embedding AS vector)\n",
    "        LIMIT :k\n",
    "    \"\"\")\n",
    "\n",
    "    # 4) Execute\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(\n",
    "            sql, {\"query_embedding\": query_embedding, \"k\": k, **params}\n",
    "        ).fetchall()\n",
    "\n",
    "    # 5) Format context for the prompt\n",
    "    passages = [f\"Passage: {r.message_content}\" for r in rows]\n",
    "    return \"\\n\".join(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0de4ba-d9b3-4cef-8105-26490de4b9a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-25625ff6fe1c9e700295633d20499706\"",
      "text/plain": [
       "Trace(trace_id=tr-25625ff6fe1c9e700295633d20499706)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from databricks_langchain.chat_models import ChatDatabricks\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from databricks_langchain.genie import GenieAgent\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    DatabricksFunctionClient,\n",
    "    set_uc_function_client\n",
    ")\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "## Load the chain's configuration\n",
    "model_config = mlflow.models.ModelConfig(development_config=chain_config)\n",
    "\n",
    "def create_context_aware_vector_search_tool(state):\n",
    "    \"\"\"Create a vector search tool that has access to user context from state\"\"\"\n",
    "    \n",
    "    def filtered_vector_search(query: str) -> str:\n",
    "        # Extract user context from state\n",
    "        user_context = state.get(\"user_context\", {})\n",
    "        filters = user_context.get(\"filters\", {})\n",
    "        \n",
    "        # Use your existing pg_vector_similarity_search with filters\n",
    "        return pg_vector_similarity_search(\n",
    "            embeddings=embeddings, \n",
    "            query=query, \n",
    "            k=retriever_config[\"parameters\"][\"k\"], \n",
    "            filters=filters\n",
    "        )\n",
    "    \n",
    "    return Tool(\n",
    "        name=\"search_chat_history\",\n",
    "        description=\"Retrieve chat history from Postgres (pgvector) for the current user; use only if the immediate conversation context is insufficient. THe input to this function should be the user message.\",\n",
    "        func=filtered_vector_search,\n",
    "    )\n",
    "\n",
    "\n",
    "genie_agent_description = model_config.get('genie_agent_description')\n",
    "general_assistant_description = model_config.get('general_assistant_description')\n",
    "code_agent_description = model_config.get('code_agent_description')\n",
    "\n",
    "genie_agent = GenieAgent(\n",
    "    genie_space_id=model_config.get('genie_space_id'),\n",
    "    genie_agent_name=\"Genie\",\n",
    "    description=genie_agent_description,\n",
    "    client=workspace_client,\n",
    "    include_context=True,\n",
    ")\n",
    "\n",
    "# Max number of interactions between agents\n",
    "MAX_ITERATIONS = 3\n",
    "\n",
    "worker_descriptions = {\n",
    "    \"Genie\": genie_agent_description,\n",
    "    \"General\": general_assistant_description,\n",
    "    \"Coder\": code_agent_description,\n",
    "}\n",
    "\n",
    "formatted_descriptions = \"\\n\".join(\n",
    "    f\"- {name}: {desc}\" for name, desc in worker_descriptions.items()\n",
    ")\n",
    "\n",
    "system_prompt = f\"Decide between routing between the following workers or ending the conversation if an answer is provided. \\n{formatted_descriptions}\"\n",
    "options = [\"FINISH\"] + list(worker_descriptions.keys())\n",
    "FINISH = {\"next_node\": \"FINISH\"}\n",
    "\n",
    "# Updated prompt template for tool-calling agent -- TODO REMOVE\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", model_config.get('llm_prompt_template')),\n",
    "#     (\"user\", \"{question}\"),\n",
    "#     (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "# ])\n",
    "\n",
    "# Our foundation model answering the final prompt\n",
    "model = ChatDatabricks(\n",
    "    endpoint=model_config.get(\"llm_model_serving_endpoint_name\"),\n",
    "    extra_params={\"temperature\": 0.01, \"max_tokens\": 500}\n",
    ")\n",
    "\n",
    "tools = []\n",
    "\n",
    "# TODO if desired, add additional tools and update the description of this agent\n",
    "uc_tool_names = [\"system.ai.*\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools.extend(uc_toolkit.tools)\n",
    "\n",
    "embeddings = DatabricksEmbeddings(\n",
    "    endpoint=chain_config[\"embedding_model\"],\n",
    "    token=os.environ.get(\"DATABRICKS_TOKEN\")\n",
    ")\n",
    "\n",
    "def supervisor_agent(state):\n",
    "    count = state.get(\"iteration_count\", 0) + 1\n",
    "    if count > MAX_ITERATIONS:\n",
    "        return FINISH\n",
    "    \n",
    "    class nextNode(BaseModel):\n",
    "        next_node: Literal[tuple(options)]\n",
    "\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    )\n",
    "    supervisor_chain = preprocessor | model.with_structured_output(nextNode)\n",
    "    next_node = supervisor_chain.invoke(state).next_node\n",
    "    \n",
    "    # if routed back to the same node, exit the loop\n",
    "    if state.get(\"next_node\") == next_node:\n",
    "        return FINISH\n",
    "    return {\n",
    "        \"iteration_count\": count,\n",
    "        \"next_node\": next_node\n",
    "    }\n",
    "\n",
    "#######################################\n",
    "# Define our multiagent graph structure\n",
    "#######################################\n",
    "\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": result[\"messages\"][-1].content,\n",
    "                \"name\": name,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def final_answer(state):\n",
    "    prompt = \"Using only the content in the messages, respond to the previous user question using the answer given by the other assistant messages.\"\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: state[\"messages\"] + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    final_answer_chain = preprocessor | model\n",
    "    return {\"messages\": [final_answer_chain.invoke(state)]}\n",
    "\n",
    "\n",
    "def agent_node_with_context(state, agent, name):\n",
    "    \"\"\"Enhanced agent node that injects context-aware tools\"\"\"\n",
    "    \n",
    "    # Create the shared vector search tool with current state context\n",
    "    vector_search_tool = create_context_aware_vector_search_tool(state)\n",
    "    \n",
    "    if name == \"Genie\":\n",
    "        # Genie already has its tools, just add vector search\n",
    "        enhanced_agent = agent  # Genie agent already configured\n",
    "        # Note: GenieAgent might need special handling - see option below\n",
    "        \n",
    "    elif name == \"Coder\" or name == \"General\":\n",
    "        # Add vector search tool to Coder's existing UC tools\n",
    "        enhanced_tools = tools + [vector_search_tool]  # tools is your UC toolkit\n",
    "        enhanced_agent = create_react_agent(model, tools=[vector_search_tool])\n",
    "        \n",
    "    # Execute with enhanced agent\n",
    "    result = enhanced_agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": result[\"messages\"][-1].content,\n",
    "            \"name\": name,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "# Create enhanced agent nodes\n",
    "def enhanced_genie_node(state):\n",
    "    enhanced_agent = genie_agent\n",
    "    return agent_node_with_context(state, enhanced_agent, \"Genie\")\n",
    "\n",
    "def enhanced_coder_node(state):\n",
    "    return agent_node_with_context(state, None, \"Coder\")\n",
    "\n",
    "def enhanced_general_node(state):\n",
    "    return agent_node_with_context(state, None, \"General\")\n",
    "\n",
    "class AgentState(ChatAgentState):\n",
    "    next_node: str\n",
    "    iteration_count: int\n",
    "    user_context: Optional[Dict[str, Any]] = None\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "# Agent States\n",
    "workflow.add_node(\"Genie\", enhanced_genie_node)\n",
    "workflow.add_node(\"Coder\", enhanced_coder_node)\n",
    "workflow.add_node(\"General\", enhanced_general_node)\n",
    "# Supervisor States\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow.add_node(\"final_answer\", final_answer)\n",
    "\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "# We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "for worker in worker_descriptions.keys():\n",
    "    workflow.add_edge(worker, \"supervisor\")\n",
    "\n",
    "# Let the supervisor decide which next node to go\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda x: x[\"next_node\"],\n",
    "    {**{k: k for k in worker_descriptions.keys()}, \"FINISH\": \"final_answer\"},\n",
    ")\n",
    "workflow.add_edge(\"final_answer\", END)\n",
    "multi_agent = workflow.compile()\n",
    "\n",
    "###################################\n",
    "# Wrap our multi-agent in ChatAgent\n",
    "###################################\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "    self,\n",
    "    messages: list[ChatAgentMessage],\n",
    "    context: Optional[ChatContext] = None,\n",
    "    custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        # Extract user context from custom_inputs\n",
    "        user_context = {}\n",
    "        if custom_inputs and \"filters\" in custom_inputs:\n",
    "            user_context[\"filters\"] = custom_inputs[\"filters\"]\n",
    "        \n",
    "        request = {\n",
    "            \"messages\": [m.model_dump_compat(exclude_none=True) for m in messages],\n",
    "            \"user_context\": user_context  # Inject user context into state\n",
    "        }\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\n",
    "            \"messages\": [m.model_dump_compat(exclude_none=True) for m in messages]\n",
    "        }\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg})\n",
    "                    for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "AGENT = LangGraphChatAgent(multi_agent)\n",
    "# mlflow.models.set_model(AGENT)\n",
    "\n",
    "# def extract_user_query_string(chat_messages_array):\n",
    "#     return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "# def extract_context_string(chat_messages_array):\n",
    "#     return '\\n'.join([f\"Role: {message['role']} - Content: {message['content']}\" for message in chat_messages_array[:-1]])\n",
    "\n",
    "# # In your rag_chain_with_tool():\n",
    "# def rag_chain_with_tool(input_data: dict) -> str:\n",
    "#     user_query = extract_user_query_string(input_data[\"messages\"])\n",
    "#     def filtered_vector_search(q: str) -> str:\n",
    "#         return pg_vector_search_with_filters(embeddings, q, input_data)\n",
    "\n",
    "#     filtered_tool = Tool(\n",
    "#         name=\"search_chat_history\",\n",
    "#         description=\"Retreive chat history from this vector database for the current user, use this tool to answer questions that may refer to previous conversations\",\n",
    "#         func=filtered_vector_search,\n",
    "#     )\n",
    "\n",
    "#     filtered_agent = create_tool_calling_agent(model, [filtered_tool], prompt)\n",
    "#     filtered_agent_executor = AgentExecutor(agent=filtered_agent, tools=[filtered_tool], verbose=True)\n",
    "#     context = extract_context_string(input_data[\"messages\"])\n",
    "#     result = filtered_agent_executor.invoke({\"question\": user_query, \"context\": context})\n",
    "#     return result[\"output\"]\n",
    "\n",
    "# # Create a runnable version of the chain\n",
    "# chain = RunnableLambda(rag_chain_with_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4a985e5-e27b-41c8-a705-82ed8d1ebb9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Document Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5e1c30-cb1b-4436-9a03-01882af81c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[ChatAgentMessage(role='assistant', content='Yes, we have talked about Databricks before. Based on our previous conversations, we\\'ve discussed:\\n\\n1. You asked about \"Databricks Genie\" in a previous conversation\\n2. You inquired about \"Databricks Photon\" \\n\\nFor the Photon query, I provided you with detailed information about Databricks Photon, explaining that it\\'s a next-generation query engine developed by Databricks that accelerates SQL and DataFrame workloads on the Databricks Lakehouse Platform. I shared its key features including:\\n- It\\'s a C++ vectorized query engine compatible with Apache Spark APIs\\n- Delivers up to 12x faster SQL query performance compared to standard Spark SQL\\n- Requires no code changes to implement\\n- Is optimized for cloud infrastructure and modern CPU architectures\\n\\nIs there something specific about Databricks you\\'d like to discuss further?', name='General', id='0c99a75f-2465-431d-b3d3-a6f039f71b92', tool_calls=None, tool_call_id=None, attachments=None), ChatAgentMessage(role='assistant', content=\"I don't see any other assistant messages in our conversation that I could use to respond to your question about Databricks. There's only our direct exchange where I recalled our previous discussions about Databricks Genie and Databricks Photon. If you intended to reference other assistant messages that aren't visible in our current conversation thread, I don't have access to those.\", name=None, id='run--5282f0ba-434b-40aa-b60b-3168713834dc-0', tool_calls=None, tool_call_id=None, attachments=None)] finish_reason=None custom_outputs=None usage=None\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-db9d377e0b35746cca62a9d0581c7fa5\"",
      "text/plain": [
       "Trace(trace_id=tr-db9d377e0b35746cca62a9d0581c7fa5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_example_no_filter = {\"messages\": [ {\"role\": \"user\", \"content\": \"Have we talked about databricks before?\"}]}\n",
    "answer_no_filter = AGENT.predict(input_example_no_filter)\n",
    "print(answer_no_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d406781-de86-4f91-af3f-f2eb616290e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[ChatAgentMessage(role='assistant', content='|    |   avg_opportunity_amount |\\n|---:|-------------------------:|\\n|  0 |                  85339.4 |', name='Genie', id='1a20b2ed-5afc-479f-bc61-a1212a1cd24c', tool_calls=None, tool_call_id=None, attachments=None), ChatAgentMessage(role='assistant', content='Based on the data provided, your average active opportunity size is $85,339.40.', name=None, id='run--cd5923c3-7bc8-40dc-be8a-43943bdfdc62-0', tool_calls=None, tool_call_id=None, attachments=None)] finish_reason=None custom_outputs=None usage=None\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-b6664716e34c106f900ed552dd8aa89f\"",
      "text/plain": [
       "Trace(trace_id=tr-b6664716e34c106f900ed552dd8aa89f)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_example_no_filter = {\"messages\": [ {\"role\": \"user\", \"content\": \"What is my average active opportunity size?\"}]}\n",
    "answer_no_filter = AGENT.predict(input_example_no_filter)\n",
    "print(answer_no_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3300a0-9b8e-40a5-a8fd-fd3ae12ca393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[ChatAgentMessage(role='assistant', content='Based on searching your chat history, I don\\'t see a specific \"chat history idea\" that you previously mentioned. The search results show that we\\'ve had conversations about:\\n\\n1. AdTech and various databases used in that industry\\n2. A question about whether we had discussed \"Databricks Genie\" before\\n\\nIf you\\'re referring to a specific idea that isn\\'t showing up in these results, could you provide more details about what you\\'re looking for? Perhaps mentioning keywords related to the idea would help me find the relevant conversation in your chat history.', name='General', id='f3dad83f-8003-4c55-9ed2-740147ad0e3f', tool_calls=None, tool_call_id=None, attachments=None), ChatAgentMessage(role='assistant', content='I don\\'t have access to other assistant messages outside of our current conversation. Based solely on our exchange so far, I can see that you asked about your \"chat history idea,\" but I don\\'t have any information about what that idea was since it wasn\\'t mentioned in our previous messages. \\n\\nIf you\\'re referring to something discussed with another assistant in a different conversation, I wouldn\\'t be able to access that information. I can only reference what we\\'ve discussed directly in this conversation thread.', name=None, id='run--883eb6cd-4a72-49bc-870f-5bce4d4295ed-0', tool_calls=None, tool_call_id=None, attachments=None)] finish_reason=None custom_outputs=None usage=None\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-05aee7a2368648e609105113080fd9dc\"",
      "text/plain": [
       "Trace(trace_id=tr-05aee7a2368648e609105113080fd9dc)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_example = {\"messages\": [ {\"role\": \"user\", \"content\": \"What was my chat history idea?\"}], \"custom_inputs\": {\"filters\": {\"user_name\": \"tanner.wendland@databricks.com\"}}}\n",
    "answer = AGENT.predict(input_example)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "247bc64a-7331-4a60-97fe-52d0360bd338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e706a3d-0b01-44e6-95e1-fcaa22ffaf44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/20 21:32:48 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n\uD83D\uDD17 View Logged Model at: https://e2-demo-field-eng.cloud.databricks.com/ml/experiments/617970317606308/models/m-2eab50a445574dd9a45635fc391ed021?o=1444828305810485\n2025/08/20 21:33:03 WARNING mlflow.models.signature: Failed to infer model output schema from prediction result, setting output schema to AnyType. For full traceback, set logging level to debug.\n2025/08/20 21:33:05 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-2eab50a445574dd9a45635fc391ed021\n2025/08/20 21:33:05 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n2025/08/20 21:33:18 INFO mlflow.models.model: Found the following environment variables used during model inference: [DATABRICKS_HOST, DATABRICKS_TOKEN]. Please check if you need to set them when deploying the model. To disable this message, set environment variable `MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING` to `false`.\nRegistered model 'tanner_wendland.default.chat_history_agent_postgres_genie' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d7c822e48348518ca70f521b1f8e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e93884005a64a25a298dcfffb4654d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 Created version '1' of model 'tanner_wendland.default.chat_history_agent_postgres_genie': https://e2-demo-field-eng.cloud.databricks.com/explore/data/models/tanner_wendland/default/chat_history_agent_postgres_genie/version/1?o=1444828305810485\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models.resources import DatabricksVectorSearchIndex, DatabricksServingEndpoint, DatabricksGenieSpace\n",
    "\n",
    "chain_file_path = os.path.join(os.getcwd(), 'chain_postgres_genie.py')\n",
    "if not os.path.exists(chain_file_path):\n",
    "    raise FileNotFoundError(f\"Chain file not found at {chain_file_path}\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "os.environ['DATABRICKS_HOST'] = f\"https://{workspace_url}\"\n",
    "os.environ['DATABRICKS_TOKEN'] = dbutils.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=\"adtech_chat_history_agent_postgres_genie\"):\n",
    "  logged_chain_info = mlflow.langchain.log_model(\n",
    "          #Note: In classical ML, MLflow works by serializing the model object.  In generative AI, chains often include Python packages that do not serialize.  Here, we use MLflow's new code-based logging, where we saved our chain under the chain notebook and will use this code instead of trying to serialize the object.\n",
    "          lc_model=os.path.join(os.getcwd(), 'chain_postgres_genie.py'),  # Chain code file e.g., /path/to/the/chain.py \n",
    "          model_config=chain_config, # Chain configuration \n",
    "          artifact_path=\"chain_postgres_genie\", # Required by MLflow, the chain's code/config are saved in this directory\n",
    "          input_example=input_example,\n",
    "          # Specify resources for automatic authentication passthrough\n",
    "          resources=[\n",
    "            DatabricksServingEndpoint(endpoint_name=model_config.get(\"llm_model_serving_endpoint_name\")),\n",
    "            DatabricksGenieSpace(genie_space_id=model_config.get(\"genie_space_id\"))\n",
    "          ],\n",
    "          pip_requirements=[\n",
    "            \"mlflow==3.1.0\",\n",
    "            \"databricks-agents==1.2.0\",\n",
    "            \"databricks-langchain==0.7.0\",\n",
    "            \"langchain==0.3.27\",\n",
    "            \"pgvector==0.2.5\",\n",
    "            \"psycopg2-binary==2.9.7\",\n",
    "            \"pydantic==2.11.7\",\n",
    "            \"sqlalchemy==2.0.43\",\n",
    "            \"tornado==6.3.2\",\n",
    "            \"langgraph==0.3.4\"\n",
    "          ]\n",
    "      )\n",
    "\n",
    "model_name = \"chat_history_agent_postgres_genie\"\n",
    "MODEL_NAME_FQN = f\"{target_catalog}.{target_schema}.{model_name}\"\n",
    "# Register to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=MODEL_NAME_FQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0141fcd2-95ff-4d57-bfa4-3c55a38f6946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -617970317606329,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9639d4-1d86-40b3-87b4-b46c8b7be046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"secert_scope\", \"field-eng\", \"Secret Scope\")\n",
    "dbutils.widgets.text(\"secret_key\", \"app-secret\", \"Secret Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -617970317606329,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93d99ca7-c565-4092-9f28-aff21fcebfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REDACTED]\n"
     ]
    }
   ],
   "source": [
    "secret_scope = dbutils.widgets.get(\"secert_scope\")\n",
    "secret_key = dbutils.widgets.get(\"secret_key\")\n",
    "\n",
    "secret_value = dbutils.secrets.get(scope=secret_scope, key=secret_key)\n",
    "\n",
    "print(str(secret_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -617970317606329,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbed8a02-2931-4a9f-aa13-e3f2766bf1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating endpoint tanner_wendland-default-chat_history_agent_postgres_genie...\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedEntityInput\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "\n",
    "version = uc_registered_model_info.version\n",
    "serving_endpoint_name = MODEL_NAME_FQN.replace(\".\", \"-\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "config = {\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "                \"name\": serving_endpoint_name,\n",
    "                \"entity_name\": MODEL_NAME_FQN,\n",
    "                \"entity_version\": version,\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": True,\n",
    "                \"environment_vars\": {\n",
    "                    'DATABRICKS_HOST': workspace_url,\n",
    "                    'DATABRICKS_TOKEN': secret_value\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def does_endpoint_exists(endpoint_name):\n",
    "    try:\n",
    "        workspace_client.serving_endpoints.get(endpoint_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if not does_endpoint_exists(serving_endpoint_name):\n",
    "    print(f\"Creating endpoint {serving_endpoint_name}...\")\n",
    "    workspace_client.serving_endpoints.create_and_wait(\n",
    "        serving_endpoint_name,\n",
    "        config=EndpointCoreConfigInput.from_dict(config)\n",
    "    )\n",
    "else:\n",
    "    print(f\"Updating endpoint {serving_endpoint_name}...\")\n",
    "    workspace_client.serving_endpoints.update_config_and_wait(\n",
    "        serving_endpoint_name,\n",
    "        served_entities=[ServedEntityInput.from_dict(entity) for entity in config['served_entities']]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02.2-chat-history-agent-pg-genie",
   "widgets": {
    "database_instance_name": {
     "currentValue": "tannerw-adtech-db",
     "nuid": "5935864b-337f-4cea-978f-00fcf2a24646",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tannerw-adtech-db",
      "label": null,
      "name": "database_instance_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "tannerw-adtech-db",
      "label": null,
      "name": "database_instance_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "embedding_model": {
     "currentValue": "databricks-gte-large-en",
     "nuid": "96111585-d614-405c-960e-ae589ccc89fc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_model",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_model",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "genie_space_id": {
     "currentValue": "01efcca6fdc712d7be87a40ad4a2e33e",
     "nuid": "ac774ec2-f551-4a54-94f9-5c547df32515",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "01efcca6fdc712d7be87a40ad4a2e33e",
      "label": null,
      "name": "genie_space_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "01efcca6fdc712d7be87a40ad4a2e33e",
      "label": null,
      "name": "genie_space_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "llm_model_serving_endpoint_name": {
     "currentValue": "databricks-claude-3-7-sonnet",
     "nuid": "4e5299b7-86bb-4511-bd44-8d76be56893e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-claude-3-7-sonnet",
      "label": null,
      "name": "llm_model_serving_endpoint_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-claude-3-7-sonnet",
      "label": null,
      "name": "llm_model_serving_endpoint_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "postgres_database_name": {
     "currentValue": "databricks_postgres",
     "nuid": "e75a0daa-dd0b-412d-b393-29edb1e3b1f1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks_postgres",
      "label": null,
      "name": "postgres_database_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks_postgres",
      "label": null,
      "name": "postgres_database_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "secert_scope": {
     "currentValue": "field-eng",
     "nuid": "e227bce8-9119-4ce0-9bd1-7bb37e315a10",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "field-eng",
      "label": "Secret Scope",
      "name": "secert_scope",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "field-eng",
      "label": "Secret Scope",
      "name": "secert_scope",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "secret_key": {
     "currentValue": "app-secret",
     "nuid": "90c15fd3-85b5-4532-9fb1-cd73d8f32360",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "app-secret",
      "label": "Secret Key",
      "name": "secret_key",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "app-secret",
      "label": "Secret Key",
      "name": "secret_key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_catalog": {
     "currentValue": "tanner_wendland",
     "nuid": "9ce71cf8-98af-4e13-8cc0-71dc8c6cf387",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tanner_wendland",
      "label": null,
      "name": "target_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "tanner_wendland",
      "label": null,
      "name": "target_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_schema": {
     "currentValue": "default",
     "nuid": "eb61b685-72df-4bd5-bf4e-80e589c9adb5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "target_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": null,
      "name": "target_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}