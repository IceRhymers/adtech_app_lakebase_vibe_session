{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf63ecd-9099-40b4-aa3a-0700f2576e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks-sdk==0.61.0\n",
      "  Downloading databricks_sdk-0.61.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: pyarrow<20 in /databricks/python3/lib/python3.11/site-packages (14.0.1)\n",
      "Collecting databricks-agents==1.2.0\n",
      "  Downloading databricks_agents-1.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting mlflow<=3.1\n",
      "  Downloading mlflow-3.1.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting databricks-vectorsearch==0.57\n",
      "  Downloading databricks_vectorsearch-0.57-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting langchain==0.3.27\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-mcp\n",
      "  Downloading langchain_mcp-0.2.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting langchain_core==0.3.74\n",
      "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting databricks-langchain==0.7.0\n",
      "  Downloading databricks_langchain-0.7.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting psycopg2-binary==2.9.7\n",
      "  Downloading psycopg2_binary-2.9.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting pgvector==0.2.5\n",
      "  Downloading pgvector-0.2.5-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting langgraph==0.3.4\n",
      "  Downloading langgraph-0.3.4-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: requests<3,>=2.28.1 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk==0.61.0) (2.31.0)\n",
      "Requirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk==0.61.0) (2.35.0)\n",
      "Requirement already satisfied: databricks-connect in /databricks/python3/lib/python3.11/site-packages (from databricks-agents==1.2.0) (15.4.12)\n",
      "Collecting dataclasses-json (from databricks-agents==1.2.0)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jinja2>=3.0.0 (from databricks-agents==1.2.0)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting mlflow-skinny<4.0.0,>=3.1.0 (from databricks-agents==1.2.0)\n",
      "  Downloading mlflow_skinny-3.3.1-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting tenacity>=8.5 (from databricks-agents==1.2.0)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.8.0 (from databricks-agents==1.2.0)\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm (from databricks-agents==1.2.0)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting urllib3>=2.0 (from databricks-agents==1.2.0)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic>=2 (from databricks-agents==1.2.0)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting whenever==0.7.3 (from databricks-agents==1.2.0)\n",
      "  Downloading whenever-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: boto3>1 in /databricks/python3/lib/python3.11/site-packages (from databricks-agents==1.2.0) (1.34.39)\n",
      "Requirement already satisfied: botocore in /databricks/python3/lib/python3.11/site-packages (from databricks-agents==1.2.0) (1.34.39)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-vectorsearch==0.57) (5.29.3)\n",
      "Collecting deprecation>=2 (from databricks-vectorsearch==0.57)\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain==0.3.27)\n",
      "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain==0.3.27)\n",
      "  Downloading langsmith-0.4.15-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.3.27)\n",
      "  Downloading sqlalchemy-2.0.43-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.3.27) (6.0)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain_core==0.3.74)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /databricks/python3/lib/python3.11/site-packages (from langchain_core==0.3.74) (4.10.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /databricks/python3/lib/python3.11/site-packages (from langchain_core==0.3.74) (23.2)\n",
      "Collecting databricks-ai-bridge>=0.7.0 (from databricks-langchain==0.7.0)\n",
      "  Downloading databricks_ai_bridge-0.7.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting databricks-connect (from databricks-agents==1.2.0)\n",
      "  Downloading databricks_connect-16.1.6-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting openai>=1.99.9 (from databricks-langchain==0.7.0)\n",
      "  Downloading openai-1.100.2-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting unitycatalog-langchain>=0.2.0 (from unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n",
      "  Downloading unitycatalog_langchain-0.2.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy in /databricks/python3/lib/python3.11/site-packages (from pgvector==0.2.5) (1.23.5)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph==0.3.4)\n",
      "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph==0.3.4)\n",
      "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph==0.3.4)\n",
      "  Downloading langgraph_sdk-0.1.74-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: ipython<10,>=8 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk[notebook]) (8.25.0)\n",
      "Collecting ipywidgets<9,>=8 (from databricks-sdk[notebook])\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting mlflow-skinny<4.0.0,>=3.1.0 (from databricks-agents==1.2.0)\n",
      "  Downloading mlflow_skinny-3.1.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting Flask<4 (from mlflow<=3.1)\n",
      "  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow<=3.1)\n",
      "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow<=3.1)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow<=3.1)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow<=3.1)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (3.7.2)\n",
      "Requirement already satisfied: pandas<3 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (1.3.0)\n",
      "Requirement already satisfied: scipy<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow<=3.1) (1.11.1)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (8.0.4)\n",
      "Requirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (3.0.0)\n",
      "Collecting fastapi<1 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (3.1.43)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (6.0.0)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n",
      "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n",
      "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (0.5.1)\n",
      "Collecting uvicorn<1 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.11/site-packages (from mlflow[databricks]) (12.14.0)\n",
      "Requirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow[databricks]) (2.18.2)\n",
      "Collecting mcp~=1.0 (from langchain-mcp)\n",
      "  Downloading mcp-1.13.0-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting typing-extensions>=4.7 (from langchain_core==0.3.74)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow<=3.1)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (1.32.0)\n",
      "Requirement already satisfied: azure-storage-blob<13.0.0,>=12.19.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (12.19.1)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (0.7.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.11/site-packages (from boto3>1->databricks-agents==1.2.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.11/site-packages (from boto3>1->databricks-agents==1.2.0) (0.10.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.11/site-packages (from botocore->databricks-agents==1.2.0) (2.8.2)\n",
      "Collecting urllib3>=2.0 (from databricks-agents==1.2.0)\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting tabulate>=0.9.0 (from databricks-ai-bridge>=0.7.0->databricks-langchain==0.7.0)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.56.4 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (1.65.0)\n",
      "Requirement already satisfied: grpcio-status>=1.59.3 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (1.69.0)\n",
      "Requirement already satisfied: grpcio>=1.59.3 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (1.69.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (0.10.9.7)\n",
      "Requirement already satisfied: setuptools>=68.0.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect->databricks-agents==1.2.0) (75.1.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from databricks-connect->databricks-agents==1.2.0) (1.16.0)\n",
      "Collecting langchain-openai (from databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0)\n",
      "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpx (from databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow<=3.1)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting click<9,>=7.0 (from mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow<=3.1)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting markupsafe>=2.1.1 (from Flask<4->mlflow<=3.1)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting werkzeug>=3.1.0 (from Flask<4->mlflow<=3.1)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.61.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.61.0) (4.9)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.18.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (1.6.0)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow<=3.1)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow<=3.1)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: decorator in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (5.13.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.11/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (4.8.0)\n",
      "Collecting comm>=0.1.3 (from ipywidgets<9,>=8->databricks-sdk[notebook])\n",
      "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets<9,>=8->databricks-sdk[notebook])\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets<9,>=8->databricks-sdk[notebook])\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core==0.3.74)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph==0.3.4)\n",
      "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.4)\n",
      "  Downloading orjson-3.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain==0.3.27)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (10.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow<=3.1) (3.0.9)\n",
      "Collecting anyio>=4.5 (from mcp~=1.0->langchain-mcp)\n",
      "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting httpx-sse>=0.4 (from mcp~=1.0->langchain-mcp)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting jsonschema>=4.20.0 (from mcp~=1.0->langchain-mcp)\n",
      "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pydantic-settings>=2.5.2 (from mcp~=1.0->langchain-mcp)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from mcp~=1.0->langchain-mcp)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp~=1.0->langchain-mcp)\n",
      "  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting starlette>=0.27 (from mcp~=1.0->langchain-mcp)\n",
      "  Downloading starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.9->databricks-langchain==0.7.0) (1.7.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.99.9->databricks-langchain==0.7.0)\n",
      "  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai>=1.99.9->databricks-langchain==0.7.0)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas<3->mlflow<=3.1) (2022.7)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2->databricks-agents==1.2.0)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2->databricks-agents==1.2.0)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2->databricks-agents==1.2.0)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (2023.7.22)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow<=3.1) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow<=3.1) (2.2.0)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain==0.3.27)\n",
      "  Downloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken>=0.8.0->databricks-agents==1.2.0)\n",
      "  Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting langchain-community>=0.2.0 (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting unitycatalog-ai (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n",
      "  Downloading unitycatalog_ai-0.3.1-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->databricks-agents==1.2.0)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->databricks-agents==1.2.0)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow[databricks]) (41.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (4.0.11)\n",
      "Collecting protobuf<6,>=3.12.0 (from databricks-vectorsearch==0.57)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=1.30.0->mlflow[databricks]) (1.25.0)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status>=1.59.3 (from databricks-connect->databricks-agents==1.2.0)\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting httpcore==1.* (from httpx->databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx->databricks-sdk[openai]>=0.58.0->databricks-agents==1.2.0)\n",
      "  Downl\n",
      "\n",
      "*** WARNING: max output size exceeded, skipping output. ***\n",
      "\n",
      "y<4.0.0,>=3.1.0->databricks-agents==1.2.0) (3.11.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /databricks/python3/lib/python3.11/site-packages (from jedi>=0.16->ipython<10,>=8->databricks-sdk[notebook]) (0.8.3)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp)\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp)\n",
      "  Downloading rpds_py-0.27.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting numpy (from pgvector==0.2.5)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0)\n",
      "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /databricks/python3/lib/python3.11/site-packages (from pexpect>4.3->ipython<10,>=8->databricks-sdk[notebook]) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /databricks/python3/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython<10,>=8->databricks-sdk[notebook]) (0.2.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk==0.61.0) (0.4.8)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->databricks-agents==1.2.0) (0.4.3)\n",
      "Requirement already satisfied: executing in /databricks/python3/lib/python3.11/site-packages (from stack-data->ipython<10,>=8->databricks-sdk[notebook]) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /databricks/python3/lib/python3.11/site-packages (from stack-data->ipython<10,>=8->databricks-sdk[notebook]) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /databricks/python3/lib/python3.11/site-packages (from stack-data->ipython<10,>=8->databricks-sdk[notebook]) (0.2.2)\n",
      "Requirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.11/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0) (1.5.6)\n",
      "Collecting unitycatalog-client (from unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n",
      "  Downloading unitycatalog_client-0.3.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow[databricks]) (1.15.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.1.0->databricks-agents==1.2.0) (5.0.1)\n",
      "Collecting aiohttp-retry>=2.8.3 (from unitycatalog-client->unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.0)\n",
      "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow[databricks]) (2.21)\n",
      "Downloading databricks_sdk-0.61.0-py3-none-any.whl (680 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/680.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.6/680.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading databricks_agents-1.2.0-py3-none-any.whl (195 kB)\n",
      "Downloading databricks_vectorsearch-0.57-py3-none-any.whl (16 kB)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
      "Downloading databricks_langchain-0.7.0-py3-none-any.whl (26 kB)\n",
      "Downloading psycopg2_binary-2.9.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pgvector-0.2.5-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading langgraph-0.3.4-py3-none-any.whl (131 kB)\n",
      "Downloading whenever-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (418 kB)\n",
      "Downloading mlflow-3.1.0-py3-none-any.whl (24.7 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/24.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/24.7 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m24.6/24.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-3.1.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_mcp-0.2.1-py3-none-any.whl (4.1 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
      "Downloading databricks_ai_bridge-0.7.0-py3-none-any.whl (18 kB)\n",
      "Downloading databricks_connect-16.1.6-py2.py3-none-any.whl (2.4 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
      "Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
      "Downloading langgraph_sdk-0.1.74-py3-none-any.whl (50 kB)\n",
      "Downloading langsmith-0.4.15-py3-none-any.whl (375 kB)\n",
      "Downloading mcp-1.13.0-py3-none-any.whl (160 kB)\n",
      "Downloading openai-1.100.2-py3-none-any.whl (787 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/787.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m787.8/787.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.0.43-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading unitycatalog_langchain-0.2.0-py3-none-any.whl (5.4 kB)\n",
      "Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
      "Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (587 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/587.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/18.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m18.1/18.3 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Downloading orjson-3.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/798.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Downloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\n",
      "Downloading starlette-0.47.2-py3-none-any.whl (72 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading unitycatalog_ai-0.3.1-py3-none-any.whl (66 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.27.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n",
      "Downloading unitycatalog_client-0.3.0-py3-none-any.whl (159 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Installing collected packages: widgetsnbextension, whenever, urllib3, typing-extensions, tqdm, tenacity, tabulate, soupsieve, sniffio, rpds-py, regex, python-multipart, python-dotenv, psycopg2-binary, protobuf, propcache, ormsgpack, orjson, numpy, multidict, marshmallow, markupsafe, jupyterlab_widgets, jsonpointer, jiter, itsdangerous, httpx-sse, h11, gunicorn, greenlet, graphql-core, frozenlist, deprecation, comm, click, blinker, attrs, annotated-types, aiohappyeyeballs, yarl, werkzeug, uvicorn, typing-inspection, typing-inspect, SQLAlchemy, referencing, pydantic-core, pgvector, opentelemetry-api, Mako, jsonpatch, jinja2, httpcore, graphql-relay, dotenv, beautifulsoup4, anyio, aiosignal, tiktoken, starlette, sse-starlette, requests-toolbelt, pydantic, opentelemetry-semantic-conventions, jsonschema-specifications, httpx, grpcio-status, graphene, Flask, docker, dataclasses-json, databricks-sdk, bs4, alembic, aiohttp, pydantic-settings, opentelemetry-sdk, openai, langsmith, langgraph-sdk, jsonschema, ipywidgets, fastapi, databricks-connect, aiohttp-retry, unitycatalog-client, mlflow-skinny, mcp, langchain_core, unitycatalog-ai, mlflow, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langchain-mcp, databricks-vectorsearch, databricks-ai-bridge, langgraph-prebuilt, langchain, langgraph, langchain-community, databricks-agents, unitycatalog-langchain, databricks-langchain\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.16\n",
      "    Not uninstalling urllib3 at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'urllib3'. No files were found to uninstall.\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Not uninstalling typing-extensions at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.2\n",
      "    Not uninstalling tenacity at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'tenacity'. No files were found to uninstall.\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.3\n",
      "    Not uninstalling protobuf at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'protobuf'. No files were found to uninstall.\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Not uninstalling numpy at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'numpy'. No files were found to uninstall.\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.1.2\n",
      "    Not uninstalling comm at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'comm'. No files were found to uninstall.\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.4\n",
      "    Not uninstalling click at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'click'. No files were found to uninstall.\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.4\n",
      "    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'blinker'. No files were found to uninstall.\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.6\n",
      "    Not uninstalling pydantic at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'pydantic'. No files were found to uninstall.\n",
      "  Attempting uninstall: grpcio-status\n",
      "    Found existing installation: grpcio-status 1.69.0\n",
      "    Not uninstalling grpcio-status at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'grpcio-status'. No files were found to uninstall.\n",
      "  Attempting uninstall: databricks-sdk\n",
      "    Found existing installation: databricks-sdk 0.40.0\n",
      "    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 7.7.2\n",
      "    Not uninstalling ipywidgets at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'ipywidgets'. No files were found to uninstall.\n",
      "  Attempting uninstall: databricks-connect\n",
      "    Found existing installation: databricks-connect 15.4.12\n",
      "    Not uninstalling databricks-connect at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'databricks-connect'. No files were found to uninstall.\n",
      "  Attempting uninstall: mlflow-skinny\n",
      "    Found existing installation: mlflow-skinny 2.11.4\n",
      "    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1754f043-0a1f-48e7-b663-f290c7017076\n",
      "    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\n",
      "Successfully installed Flask-3.1.2 Mako-1.3.10 SQLAlchemy-2.0.43 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiohttp-retry-2.9.1 aiosignal-1.4.0 alembic-1.16.4 annotated-types-0.7.0 anyio-4.10.0 attrs-25.3.0 beautifulsoup4-4.13.4 blinker-1.9.0 bs4-0.0.2 click-8.2.1 comm-0.2.3 databricks-agents-1.2.0 databricks-ai-bridge-0.7.0 databricks-connect-16.1.6 databricks-langchain-0.7.0 databricks-sdk-0.61.0 databricks-vectorsearch-0.57 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 dotenv-0.9.9 fastapi-0.116.1 frozenlist-1.7.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 greenlet-3.2.4 grpcio-status-1.62.3 gunicorn-23.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.1 ipywidgets-8.1.7 itsdangerous-2.2.0 jinja2-3.1.6 jiter-0.10.0 jsonpatch-1.33 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.4.1 jupyterlab_widgets-3.0.15 langchain-0.3.27 langchain-community-0.3.27 langchain-mcp-0.2.1 langchain-openai-0.3.30 langchain-text-splitters-0.3.9 langchain_core-0.3.74 langgraph-0.3.4 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.74 langsmith-0.4.15 markupsafe-3.0.2 marshmallow-3.26.1 mcp-1.13.0 mlflow-3.1.0 mlflow-skinny-3.1.0 multidict-6.6.4 numpy-1.26.4 openai-1.100.2 opentelemetry-api-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 orjson-3.11.2 ormsgpack-1.10.0 pgvector-0.2.5 propcache-0.3.2 protobuf-4.25.8 psycopg2-binary-2.9.7 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 python-multipart-0.0.20 referencing-0.36.2 regex-2025.7.34 requests-toolbelt-1.0.0 rpds-py-0.27.0 sniffio-1.3.1 soupsieve-2.7 sse-starlette-3.0.2 starlette-0.47.2 tabulate-0.9.0 tenacity-9.1.2 tiktoken-0.11.0 tqdm-4.67.1 typing-extensions-4.14.1 typing-inspect-0.9.0 typing-inspection-0.4.1 unitycatalog-ai-0.3.1 unitycatalog-client-0.3.0 unitycatalog-langchain-0.2.0 urllib3-2.0.7 uvicorn-0.35.0 werkzeug-3.1.3 whenever-0.7.3 widgetsnbextension-4.0.14 yarl-1.20.1\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install 'databricks-sdk==0.61.0' 'pyarrow<20' 'databricks-sdk[notebook]' 'databricks-agents==1.2.0' 'mlflow<=3.1' 'mlflow[databricks]' 'databricks-vectorsearch==0.57' 'langchain==0.3.27' 'langchain-mcp' 'langchain_core==0.3.74' 'databricks-langchain==0.7.0' 'bs4' 'dotenv' 'psycopg2-binary==2.9.7' 'pgvector==0.2.5' 'langgraph==0.3.4'\n",
    "import os\n",
    "if os.environ.get(\"DATABRICKS_RUNTIME_VERSION\"):\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chain_postgres_genie.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chain_postgres_genie.py\n",
    "import functools\n",
    "import os\n",
    "import uuid\n",
    "from typing import Any, Generator, Literal, Optional, Dict, List, Optional\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import ModelConfig\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    DatabricksFunctionClient,\n",
    "    set_uc_function_client\n",
    ")\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client) \n",
    "from databricks_langchain.genie import GenieAgent\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "from sqlalchemy import create_engine, text, event\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import DatabricksEmbeddings\n",
    "from databricks_langchain.chat_models import ChatDatabricks\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# Enable MLflow Tracing for LangChain\n",
    "mlflow.autolog()\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Load chain configuration provided at logging/deployment time.\n",
    "# The config should include at least:\n",
    "# - \"llm_model_serving_endpoint_name\": str\n",
    "# - \"embedding_model\": str\n",
    "# - \"llm_prompt_template\": str (expects {context} and {question})\n",
    "model_config: ModelConfig = mlflow.models.ModelConfig()\n",
    "\n",
    "\n",
    "def _get_required_env(name: str) -> str:\n",
    "    value = os.environ.get(name)\n",
    "    if not value:\n",
    "        raise RuntimeError(f\"Missing required environment variable: {name}\")\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_postgres_connection(\n",
    "    client: WorkspaceClient,\n",
    "    db_instance_name: str,\n",
    "    database_name: Optional[str] = \"databricks_postgres\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build a PostgreSQL SQLAlchemy URL (psycopg2) using Databricks Database credentials.\n",
    "\n",
    "    Uses POSTGRES_GROUP env var as username if set; otherwise current user.\n",
    "    Always enforces sslmode=require.\n",
    "    \"\"\"\n",
    "    database = client.database.get_database_instance(db_instance_name)\n",
    "    credentials = client.database.generate_database_credential(\n",
    "        instance_names=[db_instance_name], request_id=str(uuid.uuid4())\n",
    "    )\n",
    "\n",
    "    postgres_group = os.getenv(\"POSTGRES_GROUP\")\n",
    "    username = (\n",
    "        postgres_group if postgres_group else client.current_user.me().user_name\n",
    "    )\n",
    "\n",
    "    host = database.read_write_dns\n",
    "    port = \"5432\"\n",
    "    password = credentials.token\n",
    "    db_name = database_name or \"databricks_postgres\"\n",
    "\n",
    "    # SQLAlchemy URL with psycopg2 driver\n",
    "    sqlalchemy_url = (\n",
    "        f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{db_name}?sslmode=require\"\n",
    "    )\n",
    "    return sqlalchemy_url\n",
    "\n",
    "\n",
    "# --- Databricks Auth (required for both embeddings and DB credentials) ---\n",
    "_DATABRICKS_HOST = _get_required_env(\"DATABRICKS_HOST\")\n",
    "_DATABRICKS_TOKEN = _get_required_env(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "workspace_client = WorkspaceClient(host=_DATABRICKS_HOST, token=_DATABRICKS_TOKEN)\n",
    "\n",
    "\n",
    "# --- Postgres Engine (pgvector) ---\n",
    "def _build_engine() -> Any:\n",
    "    # Allow configuration via model_config or environment variables\n",
    "    db_instance_name = (\n",
    "        os.environ.get(\"DATABASE_INSTANCE_NAME\")\n",
    "        or model_config.get(\"database_instance_name\")\n",
    "    )\n",
    "    if not db_instance_name:\n",
    "        raise RuntimeError(\n",
    "            \"A Postgres database instance name is required. Set env 'DATABASE_INSTANCE_NAME' \"\n",
    "            \"or include 'database_instance_name' in the model_config.\"\n",
    "        )\n",
    "\n",
    "    postgres_database_name = (\n",
    "        os.environ.get(\"POSTGRES_DATABASE_NAME\")\n",
    "        or model_config.get(\"postgres_database_name\")\n",
    "        or \"databricks_postgres\"\n",
    "    )\n",
    "\n",
    "    database_url = get_postgres_connection(\n",
    "        workspace_client, db_instance_name, postgres_database_name\n",
    "    )\n",
    "\n",
    "    engine = create_engine(database_url, pool_pre_ping=True)\n",
    "\n",
    "    @event.listens_for(engine, \"connect\")\n",
    "    def _register_vector(dbapi_connection, connection_record):  # noqa: ANN001\n",
    "        # Map Python lists to pgvector type for psycopg2\n",
    "        register_vector(dbapi_connection)\n",
    "\n",
    "    return engine\n",
    "\n",
    "\n",
    "engine = _build_engine()\n",
    "\n",
    "\n",
    "# --- Embeddings ---\n",
    "embeddings = DatabricksEmbeddings(\n",
    "    endpoint=model_config.get(\"embedding_model\"),\n",
    "    token=_DATABRICKS_TOKEN,\n",
    ")\n",
    "\n",
    "# --- Vector similarity search over Postgres (pgvector) ---\n",
    "def pg_vector_similarity_search(\n",
    "    query_text: str,\n",
    "    k: int = 3,\n",
    "    filters: Optional[Dict[str, Any]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Perform similarity search against message embeddings in Postgres (pgvector).\n",
    "\n",
    "    Schema expectations:\n",
    "    - message_embeddings(me: id, message_id, user_name, chat_id, embedding vector)\n",
    "    - chat_history(ch: id, message_content, message_type, created_at, message_order)\n",
    "    \"\"\"\n",
    "    filters = filters or {}\n",
    "\n",
    "    # 1) Embed the query\n",
    "    query_embedding = embeddings.embed_query(query_text)\n",
    "\n",
    "    # 2) WHERE clause from filters\n",
    "    where_conditions: List[str] = []\n",
    "    params: Dict[str, Any] = {}\n",
    "\n",
    "    if \"user_name\" in filters:\n",
    "        where_conditions.append(\"me.user_name = :user_name\")\n",
    "        params[\"user_name\"] = filters[\"user_name\"]\n",
    "\n",
    "    if \"chat_id\" in filters:\n",
    "        where_conditions.append(\"me.chat_id = :chat_id\")\n",
    "        params[\"chat_id\"] = filters[\"chat_id\"]\n",
    "\n",
    "    where_clause = \"\"\n",
    "    if where_conditions:\n",
    "        where_clause = \"WHERE \" + \" AND \".join(where_conditions)\n",
    "\n",
    "    # 3) Query using cosine distance operator (<=>) provided by pgvector\n",
    "    sql = text(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            ch.message_content,\n",
    "            me.user_name,\n",
    "            me.chat_id,\n",
    "            ch.message_type,\n",
    "            ch.created_at,\n",
    "            ch.message_order,\n",
    "            (me.embedding <=> CAST(:query_embedding AS vector)) AS distance\n",
    "        FROM message_embeddings me\n",
    "        JOIN chat_history ch ON me.message_id = ch.id\n",
    "        {where_clause}\n",
    "        ORDER BY me.embedding <=> CAST(:query_embedding AS vector)\n",
    "        LIMIT :k\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(\n",
    "            sql, {\"query_embedding\": query_embedding, \"k\": k, **params}\n",
    "        ).fetchall()\n",
    "\n",
    "    passages = [f\"Passage: {r.message_content}\" for r in rows]\n",
    "    return \"\\n\".join(passages)\n",
    "  \n",
    "\n",
    "def create_context_aware_vector_search_tool(state):\n",
    "  \"\"\"Create a vector search tool that has access to user context from state\"\"\"\n",
    "  \n",
    "  def filtered_vector_search(query: str) -> str:\n",
    "      # Extract user context from state\n",
    "      user_context = state.get(\"user_context\", {})\n",
    "      filters = user_context.get(\"filters\", {})\n",
    "      \n",
    "      # Use your existing pg_vector_similarity_search with filters\n",
    "      return pg_vector_similarity_search(\n",
    "          query_text=query, \n",
    "          k=model_config.get('k'), \n",
    "          filters=filters\n",
    "      )\n",
    "  \n",
    "  return Tool(\n",
    "      name=\"search_chat_history\",\n",
    "      description=\"Retrieve chat history from Postgres (pgvector) for the current user; use only if the immediate conversation context is insufficient. THe input to this function should be the user message.\",\n",
    "      func=filtered_vector_search,\n",
    "  )\n",
    "\n",
    "\n",
    "genie_agent_description = model_config.get('genie_agent_description')\n",
    "general_assistant_description = model_config.get('general_assistant_description')\n",
    "code_agent_description = model_config.get('code_agent_description')\n",
    "\n",
    "genie_agent = GenieAgent(\n",
    "    genie_space_id=model_config.get('genie_space_id'),\n",
    "    genie_agent_name=\"Genie\",\n",
    "    description=genie_agent_description,\n",
    "    client=workspace_client,\n",
    "    include_context=True,\n",
    ")\n",
    "\n",
    "# Max number of interactions between agents\n",
    "MAX_ITERATIONS = 3\n",
    "\n",
    "worker_descriptions = {\n",
    "    \"Genie\": genie_agent_description,\n",
    "    \"General\": general_assistant_description,\n",
    "    \"Coder\": code_agent_description,\n",
    "}\n",
    "\n",
    "formatted_descriptions = \"\\n\".join(\n",
    "    f\"- {name}: {desc}\" for name, desc in worker_descriptions.items()\n",
    ")\n",
    "\n",
    "system_prompt = f\"Decide between routing between the following workers or ending the conversation if an answer is provided. \\n{formatted_descriptions}\"\n",
    "options = [\"FINISH\"] + list(worker_descriptions.keys())\n",
    "FINISH = {\"next_node\": \"FINISH\"}\n",
    "\n",
    "# Our foundation model answering the final prompt\n",
    "model = ChatDatabricks(\n",
    "    endpoint=model_config.get(\"llm_model_serving_endpoint_name\"),\n",
    "    extra_params={\"temperature\": 0.01, \"max_tokens\": 500}\n",
    ")\n",
    "\n",
    "# Custom Static Tools\n",
    "tools = []\n",
    "\n",
    "def supervisor_agent(state):\n",
    "    count = state.get(\"iteration_count\", 0) + 1\n",
    "    if count > MAX_ITERATIONS:\n",
    "        return FINISH\n",
    "    \n",
    "    class nextNode(BaseModel):\n",
    "        next_node: Literal[tuple(options)]\n",
    "\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    )\n",
    "    supervisor_chain = preprocessor | model.with_structured_output(nextNode)\n",
    "    next_node = supervisor_chain.invoke(state).next_node\n",
    "    \n",
    "    # if routed back to the same node, exit the loop\n",
    "    if state.get(\"next_node\") == next_node:\n",
    "        return FINISH\n",
    "    return {\n",
    "        \"iteration_count\": count,\n",
    "        \"next_node\": next_node\n",
    "    }\n",
    "\n",
    "#######################################\n",
    "# Define our multiagent graph structure\n",
    "#######################################\n",
    "\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": result[\"messages\"][-1].content,\n",
    "                \"name\": name,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def final_answer(state):\n",
    "    prompt = \"Using only the content in the messages, respond to the previous user question using the answer given by the other assistant messages.\"\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: state[\"messages\"] + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    final_answer_chain = preprocessor | model\n",
    "    return {\"messages\": [final_answer_chain.invoke(state)]}\n",
    "\n",
    "\n",
    "def agent_node_with_context(state, agent, name):\n",
    "    \"\"\"Enhanced agent node that injects context-aware tools\"\"\"\n",
    "    \n",
    "    # Create the shared vector search tool with current state context\n",
    "    vector_search_tool = create_context_aware_vector_search_tool(state)\n",
    "    \n",
    "    if name == \"Genie\":\n",
    "        # Genie already has its tools, just add vector search\n",
    "        enhanced_agent = agent  # Genie agent already configured\n",
    "        # Note: GenieAgent might need special handling - see option below\n",
    "        \n",
    "    elif name == \"Coder\" or name == \"General\":\n",
    "        # Add vector search tool to Coder's existing UC tools\n",
    "        enhanced_tools = tools + [vector_search_tool]  # tools is your UC toolkit\n",
    "        enhanced_agent = create_react_agent(model, tools=enhanced_tools)\n",
    "        \n",
    "    # Execute with enhanced agent\n",
    "    result = enhanced_agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": result[\"messages\"][-1].content,\n",
    "            \"name\": name,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "# Create enhanced agent nodes\n",
    "def enhanced_genie_node(state):\n",
    "    enhanced_agent = genie_agent\n",
    "    return agent_node_with_context(state, enhanced_agent, \"Genie\")\n",
    "\n",
    "def enhanced_coder_node(state):\n",
    "    return agent_node_with_context(state, None, \"Coder\")\n",
    "\n",
    "def enhanced_general_node(state):\n",
    "    return agent_node_with_context(state, None, \"General\")\n",
    "\n",
    "class AgentState(ChatAgentState):\n",
    "    next_node: str\n",
    "    iteration_count: int\n",
    "    user_context: Optional[Dict[str, Any]] = None\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "# Agent States\n",
    "workflow.add_node(\"Genie\", enhanced_genie_node)\n",
    "workflow.add_node(\"Coder\", enhanced_coder_node)\n",
    "workflow.add_node(\"General\", enhanced_general_node)\n",
    "# Supervisor States\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow.add_node(\"final_answer\", final_answer)\n",
    "\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "# We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "for worker in worker_descriptions.keys():\n",
    "    workflow.add_edge(worker, \"supervisor\")\n",
    "\n",
    "# Let the supervisor decide which next node to go\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda x: x[\"next_node\"],\n",
    "    {**{k: k for k in worker_descriptions.keys()}, \"FINISH\": \"final_answer\"},\n",
    ")\n",
    "workflow.add_edge(\"final_answer\", END)\n",
    "multi_agent = workflow.compile()\n",
    "\n",
    "###################################\n",
    "# Wrap our multi-agent in ChatAgent\n",
    "###################################\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "    self,\n",
    "    messages: list[ChatAgentMessage],\n",
    "    context: Optional[ChatContext] = None,\n",
    "    custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        # Extract user context from custom_inputs\n",
    "        user_context = {}\n",
    "        if custom_inputs and \"filters\" in custom_inputs:\n",
    "            user_context[\"filters\"] = custom_inputs[\"filters\"]\n",
    "        \n",
    "        request = {\n",
    "            \"messages\": [m.model_dump_compat(exclude_none=True) for m in messages],\n",
    "            \"user_context\": user_context  # Inject user context into state\n",
    "        }\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\n",
    "            \"messages\": [m.model_dump_compat(exclude_none=True) for m in messages]\n",
    "        }\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg})\n",
    "                    for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "AGENT = LangGraphChatAgent(multi_agent)\n",
    "chain = RunnableLambda(lambda x: AGENT.predict(x))\n",
    "mlflow.models.set_model(model=chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354c76a9-8be0-4c8d-9983-9e57eb961e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"embedding_model\", \"databricks-gte-large-en\")\n",
    "dbutils.widgets.text(\"database_instance_name\", \"tannerw-adtech-db\")\n",
    "dbutils.widgets.text(\"postgres_database_name\", \"databricks_postgres\")\n",
    "dbutils.widgets.text(\"llm_model_serving_endpoint_name\", \"databricks-claude-3-7-sonnet\")\n",
    "dbutils.widgets.text(\"target_catalog\", \"tanner_wendland\")\n",
    "dbutils.widgets.text(\"target_schema\", \"default\")\n",
    "dbutils.widgets.text(\"genie_space_id\", \"01efcca6fdc712d7be87a40ad4a2e33e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99bd51e-5492-4d5d-a971-0ddda14452cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = dbutils.widgets.get(\"embedding_model\")\n",
    "database_instance_name = dbutils.widgets.get(\"database_instance_name\")\n",
    "postgres_database_name = dbutils.widgets.get(\"postgres_database_name\")\n",
    "llm_model_serving_endpoint_name = dbutils.widgets.get(\"llm_model_serving_endpoint_name\")\n",
    "target_catalog = dbutils.widgets.get(\"target_catalog\")\n",
    "target_schema = dbutils.widgets.get(\"target_schema\")\n",
    "genie_space_id = dbutils.widgets.get(\"genie_space_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2750086a-7c5b-40aa-b5f6-28fec9147b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "from typing import Any, Generator, Literal, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d115d39-47b3-40a6-b143-e4ae3f30c1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.autolog()\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "585760d5-cc0a-4a30-b069-1b57188e8623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9d15d1-397a-4cdf-b948-41a6ad5555f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain_config = {\n",
    "    \"llm_model_serving_endpoint_name\": llm_model_serving_endpoint_name,\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"database_instance_name\": database_instance_name,\n",
    "    \"postgres_database_name\": postgres_database_name,\n",
    "    \"genie_space_id\": genie_space_id,\n",
    "    \"k\": 3,\n",
    "    \"genie_agent_description\": \"You are a an agent that can invoke Genie, a powerful text-to-sql database assistant. You can use this tool to answer questions related to sales pipelines.\",\n",
    "    \"general_assistant_description\": \"The General Assistant agent is a helpful assistant that can answer any question.\",\n",
    "    \"code_agent_description\": \"The Coder agent specializes in solving programming challenges, generating code snippets, debugging issues, and explaining complex coding concepts.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "247bc64a-7331-4a60-97fe-52d0360bd338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e706a3d-0b01-44e6-95e1-fcaa22ffaf44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/20 21:32:48 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "🔗 View Logged Model at: https://e2-demo-field-eng.cloud.databricks.com/ml/experiments/617970317606308/models/m-2eab50a445574dd9a45635fc391ed021?o=1444828305810485\n",
      "2025/08/20 21:33:03 WARNING mlflow.models.signature: Failed to infer model output schema from prediction result, setting output schema to AnyType. For full traceback, set logging level to debug.\n",
      "2025/08/20 21:33:05 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-2eab50a445574dd9a45635fc391ed021\n",
      "2025/08/20 21:33:05 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n",
      "2025/08/20 21:33:18 INFO mlflow.models.model: Found the following environment variables used during model inference: [DATABRICKS_HOST, DATABRICKS_TOKEN]. Please check if you need to set them when deploying the model. To disable this message, set environment variable `MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING` to `false`.\n",
      "Registered model 'tanner_wendland.default.chat_history_agent_postgres_genie' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d7c822e48348518ca70f521b1f8e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e93884005a64a25a298dcfffb4654d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔗 Created version '1' of model 'tanner_wendland.default.chat_history_agent_postgres_genie': https://e2-demo-field-eng.cloud.databricks.com/explore/data/models/tanner_wendland/default/chat_history_agent_postgres_genie/version/1?o=1444828305810485\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models.resources import DatabricksVectorSearchIndex, DatabricksServingEndpoint, DatabricksGenieSpace\n",
    "\n",
    "chain_file_path = os.path.join(os.getcwd(), 'chain_postgres_genie.py')\n",
    "if not os.path.exists(chain_file_path):\n",
    "    raise FileNotFoundError(f\"Chain file not found at {chain_file_path}\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "os.environ['DATABRICKS_HOST'] = f\"https://{workspace_url}\"\n",
    "os.environ['DATABRICKS_TOKEN'] = dbutils.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "input_example = {\"messages\": [ {\"role\": \"user\", \"content\": \"What was my chat history idea?\"}], \"custom_inputs\": {\"filters\": {\"user_name\": \"tanner.wendland@databricks.com\"}}}\n",
    "\n",
    "model_config = mlflow.models.ModelConfig(development_config=chain_config)\n",
    "\n",
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=\"adtech_chat_history_agent_postgres_genie\"):\n",
    "  logged_chain_info = mlflow.langchain.log_model(\n",
    "          #Note: In classical ML, MLflow works by serializing the model object.  In generative AI, chains often include Python packages that do not serialize.  Here, we use MLflow's new code-based logging, where we saved our chain under the chain notebook and will use this code instead of trying to serialize the object.\n",
    "          lc_model=os.path.join(os.getcwd(), 'chain_postgres_genie.py'),  # Chain code file e.g., /path/to/the/chain.py \n",
    "          model_config=chain_config, # Chain configuration \n",
    "          artifact_path=\"chain_postgres_genie\", # Required by MLflow, the chain's code/config are saved in this directory\n",
    "          input_example=input_example,\n",
    "          # Specify resources for automatic authentication passthrough\n",
    "          resources=[\n",
    "            DatabricksServingEndpoint(endpoint_name=model_config.get(\"llm_model_serving_endpoint_name\")),\n",
    "            DatabricksGenieSpace(genie_space_id=model_config.get(\"genie_space_id\"))\n",
    "          ],\n",
    "          pip_requirements=[\n",
    "            \"mlflow==3.1.0\",\n",
    "            \"databricks-agents==1.2.0\",\n",
    "            \"databricks-langchain==0.7.0\",\n",
    "            \"langchain==0.3.27\",\n",
    "            \"pgvector==0.2.5\",\n",
    "            \"psycopg2-binary==2.9.7\",\n",
    "            \"pydantic==2.11.7\",\n",
    "            \"sqlalchemy==2.0.43\",\n",
    "            \"tornado==6.3.2\",\n",
    "            \"langgraph==0.3.4\"\n",
    "          ]\n",
    "      )\n",
    "\n",
    "model_name = \"chat_history_agent_postgres_genie\"\n",
    "MODEL_NAME_FQN = f\"{target_catalog}.{target_schema}.{model_name}\"\n",
    "# Register to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=MODEL_NAME_FQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT = mlflow.pyfunc.load_model(logged_chain_info.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4a985e5-e27b-41c8-a705-82ed8d1ebb9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Document Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3300a0-9b8e-40a5-a8fd-fd3ae12ca393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[ChatAgentMessage(role='assistant', content='Based on searching your chat history, I don\\'t see a specific \"chat history idea\" that you previously mentioned. The search results show that we\\'ve had conversations about:\\n\\n1. AdTech and various databases used in that industry\\n2. A question about whether we had discussed \"Databricks Genie\" before\\n\\nIf you\\'re referring to a specific idea that isn\\'t showing up in these results, could you provide more details about what you\\'re looking for? Perhaps mentioning keywords related to the idea would help me find the relevant conversation in your chat history.', name='General', id='f3dad83f-8003-4c55-9ed2-740147ad0e3f', tool_calls=None, tool_call_id=None, attachments=None), ChatAgentMessage(role='assistant', content='I don\\'t have access to other assistant messages outside of our current conversation. Based solely on our exchange so far, I can see that you asked about your \"chat history idea,\" but I don\\'t have any information about what that idea was since it wasn\\'t mentioned in our previous messages. \\n\\nIf you\\'re referring to something discussed with another assistant in a different conversation, I wouldn\\'t be able to access that information. I can only reference what we\\'ve discussed directly in this conversation thread.', name=None, id='run--883eb6cd-4a72-49bc-870f-5bce4d4295ed-0', tool_calls=None, tool_call_id=None, attachments=None)] finish_reason=None custom_outputs=None usage=None\n"
     ]
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "\"tr-05aee7a2368648e609105113080fd9dc\"",
      "text/plain": [
       "Trace(trace_id=tr-05aee7a2368648e609105113080fd9dc)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_example = {\"messages\": [ {\"role\": \"user\", \"content\": \"What was my chat history idea?\"}], \"custom_inputs\": {\"filters\": {\"user_name\": \"tanner.wendland@databricks.com\"}}}\n",
    "answer = AGENT.predict(input_example)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0141fcd2-95ff-4d57-bfa4-3c55a38f6946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -617970317606329,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9639d4-1d86-40b3-87b4-b46c8b7be046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"secert_scope\", \"field-eng\", \"Secret Scope\")\n",
    "dbutils.widgets.text(\"secret_key\", \"app-secret\", \"Secret Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -617970317606329,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93d99ca7-c565-4092-9f28-aff21fcebfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REDACTED]\n"
     ]
    }
   ],
   "source": [
    "secret_scope = dbutils.widgets.get(\"secert_scope\")\n",
    "secret_key = dbutils.widgets.get(\"secret_key\")\n",
    "\n",
    "secret_value = dbutils.secrets.get(scope=secret_scope, key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -617970317606329,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbed8a02-2931-4a9f-aa13-e3f2766bf1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating endpoint tanner_wendland-default-chat_history_agent_postgres_genie...\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedEntityInput, AiGatewayConfig\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "\n",
    "version = uc_registered_model_info.version\n",
    "serving_endpoint_name = MODEL_NAME_FQN.replace(\".\", \"-\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "config = {\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "                \"name\": serving_endpoint_name,\n",
    "                \"entity_name\": MODEL_NAME_FQN,\n",
    "                \"entity_version\": version,\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": True,\n",
    "                \"environment_vars\": {\n",
    "                    'DATABRICKS_HOST': workspace_url,\n",
    "                    'DATABRICKS_TOKEN': secret_value\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "ai_gateway_config = {\n",
    "        'inference_table_config': {\n",
    "            'enabled': True,\n",
    "            'catalog_name': target_catalog,\n",
    "            'schema_name': target_schema,\n",
    "            'table_name': \"chat_history_agent_postgres_genie_inference\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "def does_endpoint_exists(endpoint_name):\n",
    "    try:\n",
    "        workspace_client.serving_endpoints.get(endpoint_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if not does_endpoint_exists(serving_endpoint_name):\n",
    "    print(f\"Creating endpoint {serving_endpoint_name}...\")\n",
    "    workspace_client.serving_endpoints.create_and_wait(\n",
    "        serving_endpoint_name,\n",
    "        config=EndpointCoreConfigInput.from_dict(config),\n",
    "        ai_gateway=AiGatewayConfig.from_dict(ai_gateway_config)\n",
    "    )\n",
    "else:\n",
    "    print(f\"Updating endpoint {serving_endpoint_name}...\")\n",
    "    workspace_client.serving_endpoints.update_config_and_wait(\n",
    "        serving_endpoint_name,\n",
    "        served_entities=[ServedEntityInput.from_dict(entity) for entity in config['served_entities']]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02.2-chat-history-agent-pg-genie",
   "widgets": {
    "database_instance_name": {
     "currentValue": "tannerw-adtech-db",
     "nuid": "5935864b-337f-4cea-978f-00fcf2a24646",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tannerw-adtech-db",
      "label": null,
      "name": "database_instance_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "tannerw-adtech-db",
      "label": null,
      "name": "database_instance_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "embedding_model": {
     "currentValue": "databricks-gte-large-en",
     "nuid": "96111585-d614-405c-960e-ae589ccc89fc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_model",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_model",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "genie_space_id": {
     "currentValue": "01efcca6fdc712d7be87a40ad4a2e33e",
     "nuid": "ac774ec2-f551-4a54-94f9-5c547df32515",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "01efcca6fdc712d7be87a40ad4a2e33e",
      "label": null,
      "name": "genie_space_id",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "01efcca6fdc712d7be87a40ad4a2e33e",
      "label": null,
      "name": "genie_space_id",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "llm_model_serving_endpoint_name": {
     "currentValue": "databricks-claude-3-7-sonnet",
     "nuid": "4e5299b7-86bb-4511-bd44-8d76be56893e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-claude-3-7-sonnet",
      "label": null,
      "name": "llm_model_serving_endpoint_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "databricks-claude-3-7-sonnet",
      "label": null,
      "name": "llm_model_serving_endpoint_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "postgres_database_name": {
     "currentValue": "databricks_postgres",
     "nuid": "e75a0daa-dd0b-412d-b393-29edb1e3b1f1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks_postgres",
      "label": null,
      "name": "postgres_database_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "databricks_postgres",
      "label": null,
      "name": "postgres_database_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "secert_scope": {
     "currentValue": "field-eng",
     "nuid": "e227bce8-9119-4ce0-9bd1-7bb37e315a10",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "field-eng",
      "label": "Secret Scope",
      "name": "secert_scope",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "field-eng",
      "label": "Secret Scope",
      "name": "secert_scope",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "secret_key": {
     "currentValue": "app-secret",
     "nuid": "90c15fd3-85b5-4532-9fb1-cd73d8f32360",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "app-secret",
      "label": "Secret Key",
      "name": "secret_key",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "app-secret",
      "label": "Secret Key",
      "name": "secret_key",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "target_catalog": {
     "currentValue": "tanner_wendland",
     "nuid": "9ce71cf8-98af-4e13-8cc0-71dc8c6cf387",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tanner_wendland",
      "label": null,
      "name": "target_catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "tanner_wendland",
      "label": null,
      "name": "target_catalog",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "target_schema": {
     "currentValue": "default",
     "nuid": "eb61b685-72df-4bd5-bf4e-80e589c9adb5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "target_schema",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "default",
      "label": null,
      "name": "target_schema",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
