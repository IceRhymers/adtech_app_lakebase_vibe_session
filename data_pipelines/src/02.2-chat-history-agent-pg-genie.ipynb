{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf63ecd-9099-40b4-aa3a-0700f2576e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks-sdk==0.61.0\n  Using cached databricks_sdk-0.61.0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: databricks-connect==16.4.2 in /databricks/python3/lib/python3.12/site-packages (16.4.2)\nRequirement already satisfied: pyarrow<20 in /databricks/python3/lib/python3.12/site-packages (15.0.2)\nCollecting databricks-agents==1.4.0\n  Using cached databricks_agents-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting mlflow<=3.2\n  Downloading mlflow-3.2.0-py3-none-any.whl.metadata (29 kB)\nCollecting databricks-vectorsearch==0.57\n  Using cached databricks_vectorsearch-0.57-py3-none-any.whl.metadata (2.8 kB)\nCollecting langchain==0.3.27\n  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\nCollecting langchain-mcp\n  Using cached langchain_mcp-0.2.1-py3-none-any.whl.metadata (1.7 kB)\nCollecting langchain_core==0.3.74\n  Using cached langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\nCollecting databricks-langchain==0.7.1\n  Using cached databricks_langchain-0.7.1-py3-none-any.whl.metadata (2.7 kB)\nCollecting bs4\n  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\nCollecting dotenv\n  Using cached dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\nCollecting psycopg2-binary==2.9.7\n  Using cached psycopg2-binary-2.9.7.tar.gz (383 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting pgvector==0.2.5\n  Using cached pgvector-0.2.5-py2.py3-none-any.whl.metadata (9.9 kB)\nCollecting langgraph==0.3.4\n  Using cached langgraph-0.3.4-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: requests<3,>=2.28.1 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk==0.61.0) (2.32.2)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk==0.61.0) (2.38.0)\nRequirement already satisfied: googleapis-common-protos>=1.56.4 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (1.69.2)\nRequirement already satisfied: grpcio-status>=1.59.3 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (1.71.0)\nRequirement already satisfied: grpcio>=1.59.3 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (1.71.0)\nRequirement already satisfied: numpy<2,>=1.15 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (1.26.4)\nRequirement already satisfied: packaging>=23.2 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (24.1)\nRequirement already satisfied: pandas>=1.0.5 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (1.5.3)\nRequirement already satisfied: py4j==0.10.9.9 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (0.10.9.9)\nRequirement already satisfied: setuptools>=68.0.0 in /usr/local/lib/python3.12/dist-packages (from databricks-connect==16.4.2) (75.8.0)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from databricks-connect==16.4.2) (1.16.0)\nCollecting dataclasses-json (from databricks-agents==1.4.0)\n  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nCollecting jinja2>=3.0.0 (from databricks-agents==1.4.0)\n  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\nCollecting mlflow-skinny<4.0.0,>=3.1.4 (from databricks-agents==1.4.0)\n  Using cached mlflow_skinny-3.3.2-py3-none-any.whl.metadata (31 kB)\nCollecting tenacity>=8.5 (from databricks-agents==1.4.0)\n  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\nCollecting tiktoken>=0.8.0 (from databricks-agents==1.4.0)\n  Using cached tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting tqdm (from databricks-agents==1.4.0)\n  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nRequirement already satisfied: urllib3>=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-agents==1.4.0) (2.2.2)\nRequirement already satisfied: pydantic>=2 in /databricks/python3/lib/python3.12/site-packages (from databricks-agents==1.4.0) (2.8.2)\nCollecting whenever==0.7.3 (from databricks-agents==1.4.0)\n  Using cached whenever-0.7.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: boto3>1 in /databricks/python3/lib/python3.12/site-packages (from databricks-agents==1.4.0) (1.34.69)\nRequirement already satisfied: botocore in /databricks/python3/lib/python3.12/site-packages (from databricks-agents==1.4.0) (1.34.69)\nRequirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch==0.57) (5.29.4)\nCollecting deprecation>=2 (from databricks-vectorsearch==0.57)\n  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain==0.3.27)\n  Using cached langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\nCollecting langsmith>=0.1.17 (from langchain==0.3.27)\n  Using cached langsmith-0.4.21-py3-none-any.whl.metadata (14 kB)\nCollecting SQLAlchemy<3,>=1.4 (from langchain==0.3.27)\n  Using cached sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.12/site-packages (from langchain==0.3.27) (6.0.1)\nCollecting jsonpatch<2.0,>=1.33 (from langchain_core==0.3.74)\n  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /databricks/python3/lib/python3.12/site-packages (from langchain_core==0.3.74) (4.11.0)\nCollecting databricks-ai-bridge>=0.7.1 (from databricks-langchain==0.7.1)\n  Using cached databricks_ai_bridge-0.7.1-py3-none-any.whl.metadata (6.2 kB)\nCollecting openai>=1.99.9 (from databricks-langchain==0.7.1)\n  Using cached openai-1.104.0-py3-none-any.whl.metadata (29 kB)\nCollecting pydantic>=2 (from databricks-agents==1.4.0)\n  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\nCollecting unitycatalog-langchain>=0.2.0 (from unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Using cached unitycatalog_langchain-0.2.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph==0.3.4)\n  Using cached langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\nCollecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph==0.3.4)\n  Using cached langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\nCollecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph==0.3.4)\n  Using cached langgraph_sdk-0.1.74-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: ipython<10,>=8 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk[notebook]) (8.32.0)\nCollecting ipywidgets<9,>=8 (from databricks-sdk[notebook])\n  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\nCollecting mlflow-skinny<4.0.0,>=3.1.4 (from databricks-agents==1.4.0)\n  Downloading mlflow_skinny-3.2.0-py3-none-any.whl.metadata (30 kB)\nCollecting mlflow-tracing==3.2.0 (from mlflow<=3.2)\n  Downloading mlflow_tracing-3.2.0-py3-none-any.whl.metadata (19 kB)\nCollecting Flask<4 (from mlflow<=3.2)\n  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting alembic!=1.10.0,<2 (from mlflow<=3.2)\n  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\nCollecting docker<8,>=4.0.0 (from mlflow<=3.2)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphene<4 (from mlflow<=3.2)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow<=3.2)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow<=3.2) (3.8.4)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow<=3.2) (1.4.2)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow<=3.2) (1.13.1)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (5.3.3)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (3.0.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (0.115.12)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (3.1.37)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (7.0.1)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (1.31.1)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (1.31.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (0.5.3)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (0.34.0)\nRequirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.12/site-packages (from mlflow[databricks]) (12.17.0)\nRequirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow[databricks]) (3.1.0)\nCollecting mcp~=1.0 (from langchain-mcp)\n  Downloading mcp-1.13.1-py3-none-any.whl.metadata (74 kB)\nCollecting typing-extensions>=4.7 (from langchain_core==0.3.74)\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting beautifulsoup4 (from bs4)\n  Downloading beautifulsoup4-4.13.5-py3-none-any.whl.metadata (3.8 kB)\nCollecting python-dotenv (from dotenv)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow<=3.2)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: azure-core>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (1.33.0)\nRequirement already satisfied: azure-storage-blob>=12.23.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (12.23.0)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (0.7.2)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from boto3>1->databricks-agents==1.4.0) (1.0.1)\nRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.12/site-packages (from boto3>1->databricks-agents==1.4.0) (0.10.4)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.12/site-packages (from botocore->databricks-agents==1.4.0) (2.9.0.post0)\nCollecting tabulate>=0.9.0 (from databricks-ai-bridge>=0.7.1->databricks-langchain==0.7.1)\n  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\nCollecting langchain-openai (from databricks-sdk[openai]>=0.58.0->databricks-agents==1.4.0)\n  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\nCollecting httpx (from databricks-sdk[openai]>=0.58.0->databricks-agents==1.4.0)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting blinker>=1.9.0 (from Flask<4->mlflow<=3.2)\n  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting itsdangerous>=2.2.0 (from Flask<4->mlflow<=3.2)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nCollecting markupsafe>=2.1.1 (from Flask<4->mlflow<=3.2)\n  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow<=3.2)\n  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk==0.61.0) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk==0.61.0) (4.9)\nRequirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.20.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.4.2 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.4.3)\nRequirement already satisfied: google-resumable-media>=2.7.2 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (1.7.1)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow<=3.2)\n  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow<=3.2)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: decorator in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.1.6)\nRequirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (4.8.0)\nRequirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (3.0.43)\nRequirement already satisfied: pygments>=2.4.0 in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (2.15.1)\nRequirement already satisfied: stack_data in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.2.0)\nRequirement already satisfied: traitlets>=5.13.0 in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (5.14.3)\nRequirement already satisfied: comm>=0.1.3 in /databricks/python3/lib/python3.12/site-packages (from ipywidgets<9,>=8->databricks-sdk[notebook]) (0.2.1)\nCollecting widgetsnbextension~=4.0.14 (from ipywidgets<9,>=8->databricks-sdk[notebook])\n  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\nCollecting jupyterlab_widgets~=3.0.15 (from ipywidgets<9,>=8->databricks-sdk[notebook])\n  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\nCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core==0.3.74)\n  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\nINFO: pip is looking at multiple versions of langchain-text-splitters to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain==0.3.27)\n  Downloading langchain_text_splitters-0.3.10-py3-none-any.whl.metadata (1.9 kB)\n  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\nCollecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph==0.3.4)\n  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\nCollecting orjson>=3.10.1 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.4)\n  Downloading orjson-3.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\nCollecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain==0.3.27)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.23.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow<=3.2) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow<=3.2) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow<=3.2) (4.51.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow<=3.2) (1.4.4)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow<=3.2) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow<=3.2) (3.0.9)\nCollecting anyio>=4.5 (from mcp~=1.0->langchain-mcp)\n  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\nCollecting httpx-sse>=0.4 (from mcp~=1.0->langchain-mcp)\n  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting jsonschema>=4.20.0 (from mcp~=1.0->langchain-mcp)\n  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\nCollecting pydantic-settings>=2.5.2 (from mcp~=1.0->langchain-mcp)\n  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\nCollecting python-multipart>=0.0.9 (from mcp~=1.0->langchain-mcp)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nCollecting sse-starlette>=1.6.1 (from mcp~=1.0->langchain-mcp)\n  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: starlette>=0.27 in /databricks/python3/lib/python3.12/site-packages (from mcp~=1.0->langchain-mcp) (0.46.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.9->databricks-langchain==0.7.1) (1.9.0)\nCollecting jiter<1,>=0.4.0 (from openai>=1.99.9->databricks-langchain==0.7.1)\n  Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from openai>=1.99.9->databricks-langchain==0.7.1) (1.3.0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas>=1.0.5->databricks-connect==16.4.2) (2024.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=2->databricks-agents==1.4.0) (0.7.0)\nCollecting pydantic-core==2.33.2 (from pydantic>=2->databricks-agents==1.4.0)\n  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic>=2->databricks-agents==1.4.0)\n  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (2024.6.2)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow<=3.2) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow<=3.2) (2.2.0)\nCollecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain==0.3.27)\n  Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nCollecting regex>=2022.1.18 (from tiktoken>=0.8.0->databricks-agents==1.4.0)\n  Downloading regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\nCollecting langchain-community>=0.2.0 (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\nCollecting unitycatalog-ai (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Downloading unitycatalog_ai-0.3.2-py3-none-any.whl.metadata (31 kB)\nCollecting soupsieve>1.2 (from beautifulsoup4->bs4)\n  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->databricks-agents==1.4.0)\n  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json->databricks-agents==1.4.0)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow[databricks]) (42.0.5)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (4.0.11)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.12/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=1.30.0->mlflow[databricks]) (1.26.1)\nCollecting httpcore==1.* (from httpx->databricks-sdk[openai]>=0.58.0->databricks-agents==1.4.0)\n  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nCollecting h11>=0.16 (from httpcore==1.*->httpx->databricks-sdk[openai]>=0.58.0->databricks-agents==1.4.0)\n  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (3.17.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /databricks/python3/lib/python3.12/site-packages (from jedi>=0.16->ipython<10,>=8->databricks-sdk[notebook]) (0.8.3)\nCollecting attrs>=22.2.0 (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp)\n  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\nCollecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp)\n  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting referencing>=0.28.4 (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp)\n  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\nCollecting rpds-py>=0.7.1 (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp)\n  Downloading rpds_py-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-community>=0.2.0 (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Downloading langchain_community-0.3.28-py3-none-any.whl.metadata (2.9 kB)\nCollecting requests<3,>=2.28.1 (from databricks-sdk==0.61.0)\n  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (1.2.18)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (0.52b1)\nRequirement already satisfied: ptyprocess>=0.5 in /databricks/python3/lib/python3.12/site-packages (from pexpect>4.3->ipython<10,>=8->databricks-sdk[notebook]) (0.7.0)\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython<10,>=8->databricks-sdk[notebook]) (0.2.5)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk==0.61.0) (0.4.8)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->databricks-agents==1.4.0) (1.0.0)\nRequirement already satisfied: executing in /databricks/python3/lib/python3.12/site-packages (from stack_data->ipython<10,>=8->databricks-sdk[notebook]) (0.8.3)\nRequirement already satisfied: asttokens in /databricks/python3/lib/python3.12/site-packages (from stack_data->ipython<10,>=8->databricks-sdk[notebook]) (2.0.5)\nRequirement already satisfied: pure-eval in /databricks/python3/lib/python3.12/site-packages (from stack_data->ipython<10,>=8->databricks-sdk[notebook]) (0.2.2)\nRequirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.12/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (1.6.0)\nCollecting unitycatalog-client (from unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Downloading unitycatalog_client-0.3.0-py3-none-any.whl.metadata (7.8 kB)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Downloading multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow[databricks]) (1.16.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (1.14.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (5.0.0)\nCollecting aiohttp-retry>=2.8.3 (from unitycatalog-client->unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1)\n  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow[databricks]) (2.21)\nDownloading databricks_sdk-0.61.0-py3-none-any.whl (680 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/680.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m680.6/680.6 kB\u001B[0m \u001B[31m29.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading databricks_agents-1.4.0-py3-none-any.whl (198 kB)\nDownloading databricks_vectorsearch-0.57-py3-none-any.whl (16 kB)\nDownloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m30.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\nDownloading databricks_langchain-0.7.1-py3-none-any.whl (26 kB)\nDownloading pgvector-0.2.5-py2.py3-none-any.whl (9.6 kB)\nDownloading langgraph-0.3.4-py3-none-any.whl (131 kB)\nDownloading whenever-0.7.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (419 kB)\nDownloading mlflow-3.2.0-py3-none-any.whl (25.8 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/25.8 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m25.8/25.8 MB\u001B[0m \u001B[31m151.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_skinny-3.2.0-py3-none-any.whl (2.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m44.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_tracing-3.2.0-py3-none-any.whl (1.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m74.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_mcp-0.2.1-py3-none-any.whl (4.1 kB)\nDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\nDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\nDownloading alembic-1.16.5-py3-none-any.whl (247 kB)\nDownloading databricks_ai_bridge-0.7.1-py3-none-any.whl (18 kB)\nDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nDownloading docker-7.1.0-py3-none-any.whl (147 kB)\nDownloading flask-3.1.2-py3-none-any.whl (103 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\nDownloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\nDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\nDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\nDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\nDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\nDownloading langgraph_sdk-0.1.74-py3-none-any.whl (50 kB)\nDownloading langsmith-0.4.21-py3-none-any.whl (378 kB)\nDownloading mcp-1.13.1-py3-none-any.whl (161 kB)\nDownloading openai-1.104.0-py3-none-any.whl (926 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/926.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m926.7/926.7 kB\u001B[0m \u001B[31m63.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\nDownloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m92.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m82.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\nDownloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m86.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\nDownloading unitycatalog_langchain-0.2.0-py3-none-any.whl (5.4 kB)\nDownloading beautifulsoup4-4.13.5-py3-none-any.whl (105 kB)\nDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nDownloading anyio-4.10.0-py3-none-any.whl (107 kB)\nDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\nDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\nDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (607 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/607.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m607.6/607.6 kB\u001B[0m \u001B[31m29.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\nDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\nDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\nDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\nDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\nDownloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\nDownloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\nDownloading langchain_community-0.3.28-py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m84.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\nDownloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\nDownloading orjson-3.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\nDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\nDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/802.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m802.0/802.0 kB\u001B[0m \u001B[31m44.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\nDownloading soupsieve-2.8-py3-none-any.whl (36 kB)\nDownloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\nDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\nDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\nDownloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m88.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_openai-0.3.32-py3-none-any.whl (74 kB)\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\nDownloading unitycatalog_ai-0.3.2-py3-none-any.whl (66 kB)\nDownloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m68.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading attrs-25.3.0-py3-none-any.whl (63 kB)\nDownloading h11-0.16.0-py3-none-any.whl (37 kB)\nDownloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\nDownloading referencing-0.36.2-py3-none-any.whl (26 kB)\nDownloading rpds_py-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (386 kB)\nDownloading unitycatalog_client-0.3.0-py3-none-any.whl (159 kB)\nDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\nDownloading multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\nDownloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\nDownloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\nBuilding wheels for collected packages: psycopg2-binary\n  Building wheel for psycopg2-binary (setup.py): started\n  Building wheel for psycopg2-binary (setup.py): finished with status 'done'\n  Created wheel for psycopg2-binary: filename=psycopg2_binary-2.9.7-cp312-cp312-linux_x86_64.whl size=529643 sha256=3ce29a15d2bc57cf0fc05106ab0efaed6385a1eaafe76fee79a41540681124d5\n  Stored in directory: /home/spark-ef41dd14-648a-488c-8839-50/.cache/pip/wheels/06/98/07/c82f54ee8a265a952c7faf644da48da839c70f7fa6cec261a7\nSuccessfully built psycopg2-binary\nInstalling collected packages: widgetsnbextension, whenever, typing-extensions, tqdm, tenacity, tabulate, soupsieve, rpds-py, requests, regex, python-multipart, python-dotenv, psycopg2-binary, propcache, pgvector, ormsgpack, orjson, multidict, marshmallow, markupsafe, jupyterlab_widgets, jsonpointer, jiter, itsdangerous, httpx-sse, h11, gunicorn, greenlet, graphql-core, frozenlist, deprecation, blinker, attrs, aiohappyeyeballs, yarl, werkzeug, typing-inspection, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, referencing, pydantic-core, Mako, jsonpatch, jinja2, httpcore, graphql-relay, dotenv, docker, beautifulsoup4, anyio, aiosignal, sse-starlette, pydantic, jsonschema-specifications, httpx, graphene, Flask, dataclasses-json, databricks-sdk, bs4, alembic, aiohttp, pydantic-settings, openai, langsmith, langgraph-sdk, jsonschema, ipywidgets, aiohttp-retry, unitycatalog-client, mlflow-tracing, mlflow-skinny, mcp, langchain_core, unitycatalog-ai, mlflow, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langchain-mcp, databricks-vectorsearch, databricks-ai-bridge, langgraph-prebuilt, langchain, langgraph, langchain-community, databricks-agents, unitycatalog-langchain, databricks-langchain\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.11.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 8.2.2\n    Not uninstalling tenacity at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b\n    Can't uninstall 'tenacity'. No files were found to uninstall.\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.2\n    Not uninstalling requests at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b\n    Can't uninstall 'requests'. No files were found to uninstall.\n  Attempting uninstall: h11\n    Found existing installation: h11 0.14.0\n    Not uninstalling h11 at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b\n    Can't uninstall 'h11'. No files were found to uninstall.\n  Attempting uninstall: blinker\n    Found existing installation: blinker 1.7.0\n    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b\n    Can't uninstall 'blinker'. No files were found to uninstall.\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.20.1\n    Not uninstalling pydantic-core at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b\n    Can't uninstall 'pydantic_core'. No files were found to uninstall.\n  Attempting uninstall: anyio\n    Found existing installation: anyio 4.2.0\n    Not uninstalling anyio at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b\n    Can't uninstall 'anyio'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.8.2\n    Not uninstalling pydantic at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b\n    Can't uninstall 'pydantic'. No files were found to uninstall.\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.49.0\n    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b\n    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n  Attempting uninstall: ipywidgets\n    Found existing installation: ipywidgets 7.7.2\n    Not uninstalling ipywidgets at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b\n    Can't uninstall 'ipywidgets'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.21.3\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\nSuccessfully installed Flask-3.1.2 Mako-1.3.10 SQLAlchemy-2.0.43 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiohttp-retry-2.9.1 aiosignal-1.4.0 alembic-1.16.5 anyio-4.10.0 attrs-25.3.0 beautifulsoup4-4.13.5 blinker-1.9.0 bs4-0.0.2 databricks-agents-1.4.0 databricks-ai-bridge-0.7.1 databricks-langchain-0.7.1 databricks-sdk-0.61.0 databricks-vectorsearch-0.57 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 dotenv-0.9.9 frozenlist-1.7.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 greenlet-3.2.4 gunicorn-23.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.1 ipywidgets-8.1.7 itsdangerous-2.2.0 jinja2-3.1.6 jiter-0.10.0 jsonpatch-1.33 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.4.1 jupyterlab_widgets-3.0.15 langchain-0.3.27 langchain-community-0.3.28 langchain-mcp-0.2.1 langchain-openai-0.3.32 langchain-text-splitters-0.3.9 langchain_core-0.3.74 langgraph-0.3.4 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.74 langsmith-0.4.21 markupsafe-3.0.2 marshmallow-3.26.1 mcp-1.13.1 mlflow-3.2.0 mlflow-skinny-3.2.0 mlflow-tracing-3.2.0 multidict-6.6.4 openai-1.104.0 orjson-3.11.3 ormsgpack-1.10.0 pgvector-0.2.5 propcache-0.3.2 psycopg2-binary-2.9.7 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 python-multipart-0.0.20 referencing-0.36.2 regex-2025.9.1 requests-2.32.5 requests-toolbelt-1.0.0 rpds-py-0.27.1 soupsieve-2.8 sse-starlette-3.0.2 tabulate-0.9.0 tenacity-9.1.2 tiktoken-0.11.0 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.1 unitycatalog-ai-0.3.2 unitycatalog-client-0.3.0 unitycatalog-langchain-0.2.0 werkzeug-3.1.3 whenever-0.7.3 widgetsnbextension-4.0.14 yarl-1.20.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install 'databricks-sdk==0.61.0' 'databricks-connect==16.4.2' 'pyarrow<20' 'databricks-sdk[notebook]' 'databricks-agents==1.4.0' 'mlflow<=3.2' 'mlflow[databricks]' 'databricks-vectorsearch==0.57' 'langchain==0.3.27' 'langchain-mcp' 'langchain_core==0.3.74' 'databricks-langchain==0.7.1' 'bs4' 'dotenv' 'psycopg2-binary==2.9.7' 'pgvector==0.2.5' 'langgraph==0.3.4'\n",
    "import os\n",
    "if os.environ.get(\"DATABRICKS_RUNTIME_VERSION\"):\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7aed6b4a-e10c-4d2f-9359-bddb7e481a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chain_postgres_genie.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chain_postgres_genie.py\n",
    "import functools\n",
    "import os\n",
    "import uuid\n",
    "from typing import Any, Generator, Literal, Optional, Dict, List\n",
    "\n",
    "import mlflow\n",
    "import pydantic\n",
    "from mlflow.models import ModelConfig\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    DatabricksFunctionClient,\n",
    "    set_uc_function_client\n",
    ")\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client) \n",
    "from databricks_langchain.genie import GenieAgent\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "from sqlalchemy import create_engine, text, event\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import DatabricksEmbeddings\n",
    "from databricks_langchain.chat_models import ChatDatabricks\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from mlflow.entities import SpanType, Document\n",
    "\n",
    "# Enable MLflow Tracing for LangChain\n",
    "mlflow.autolog()\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Load chain configuration provided at logging/deployment time.\n",
    "model_config: ModelConfig = mlflow.models.ModelConfig()\n",
    "\n",
    "# Pydantic models for input validation\n",
    "class Message(pydantic.BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "    name: Optional[str] = None\n",
    "\n",
    "class Filters(pydantic.BaseModel):\n",
    "    user_name: str  # Required\n",
    "    chat_id: Optional[str] = None\n",
    "\n",
    "class CustomInputs(pydantic.BaseModel):\n",
    "    filters: Filters\n",
    "    k: Optional[int] = None  # Optional, will default to model_config value\n",
    "\n",
    "class ChatRequest(pydantic.BaseModel):\n",
    "    messages: List[Message]\n",
    "    custom_inputs: Optional[CustomInputs] = None\n",
    "\n",
    "class ChatResponse(pydantic.BaseModel):\n",
    "    messages: List[Message]\n",
    "    finish_reason: Optional[str] = None\n",
    "\n",
    "\n",
    "def _get_required_env(name: str) -> str:\n",
    "    value = os.environ.get(name)\n",
    "    if not value:\n",
    "        raise RuntimeError(f\"Missing required environment variable: {name}\")\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_postgres_connection(\n",
    "    client: WorkspaceClient,\n",
    "    db_instance_name: str,\n",
    "    database_name: Optional[str] = \"databricks_postgres\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build a PostgreSQL SQLAlchemy URL (psycopg2) using Databricks Database credentials.\n",
    "\n",
    "    Uses POSTGRES_GROUP env var as username if set; otherwise current user.\n",
    "    Always enforces sslmode=require.\n",
    "    \"\"\"\n",
    "    database = client.database.get_database_instance(db_instance_name)\n",
    "    credentials = client.database.generate_database_credential(\n",
    "        instance_names=[db_instance_name], request_id=str(uuid.uuid4())\n",
    "    )\n",
    "\n",
    "    postgres_group = os.getenv(\"POSTGRES_GROUP\")\n",
    "    username = (\n",
    "        postgres_group if postgres_group else client.current_user.me().user_name\n",
    "    )\n",
    "\n",
    "    host = database.read_write_dns\n",
    "    port = \"5432\"\n",
    "    password = credentials.token\n",
    "    db_name = database_name or \"databricks_postgres\"\n",
    "\n",
    "    # SQLAlchemy URL with psycopg2 driver\n",
    "    sqlalchemy_url = (\n",
    "        f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{db_name}?sslmode=require\"\n",
    "    )\n",
    "    return sqlalchemy_url\n",
    "\n",
    "\n",
    "# --- Databricks Auth (required for both embeddings and DB credentials) ---\n",
    "_DATABRICKS_HOST = _get_required_env(\"DATABRICKS_HOST\")\n",
    "_DATABRICKS_TOKEN = _get_required_env(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "workspace_client = WorkspaceClient(host=_DATABRICKS_HOST, token=_DATABRICKS_TOKEN)\n",
    "\n",
    "\n",
    "# --- Postgres Engine (pgvector) ---\n",
    "def _build_engine() -> Any:\n",
    "    # Allow configuration via model_config or environment variables\n",
    "    db_instance_name = (\n",
    "        os.environ.get(\"DATABASE_INSTANCE_NAME\")\n",
    "        or model_config.get(\"database_instance_name\")\n",
    "    )\n",
    "    if not db_instance_name:\n",
    "        raise RuntimeError(\n",
    "            \"A Postgres database instance name is required. Set env 'DATABASE_INSTANCE_NAME' \"\n",
    "            \"or include 'database_instance_name' in the model_config.\"\n",
    "        )\n",
    "\n",
    "    postgres_database_name = (\n",
    "        os.environ.get(\"POSTGRES_DATABASE_NAME\")\n",
    "        or model_config.get(\"postgres_database_name\")\n",
    "        or \"databricks_postgres\"\n",
    "    )\n",
    "\n",
    "    database_url = get_postgres_connection(\n",
    "        workspace_client, db_instance_name, postgres_database_name\n",
    "    )\n",
    "\n",
    "    engine = create_engine(database_url, pool_pre_ping=True)\n",
    "\n",
    "    @event.listens_for(engine, \"connect\")\n",
    "    def _register_vector(dbapi_connection, connection_record):  # noqa: ANN001\n",
    "        # Map Python lists to pgvector type for psycopg2\n",
    "        register_vector(dbapi_connection)\n",
    "\n",
    "    return engine\n",
    "\n",
    "\n",
    "engine = _build_engine()\n",
    "\n",
    "\n",
    "# --- Embeddings ---\n",
    "embeddings = DatabricksEmbeddings(\n",
    "    endpoint=model_config.get(\"embedding_model\"),\n",
    "    token=_DATABRICKS_TOKEN,\n",
    ")\n",
    "\n",
    "# --- Vector similarity search over Postgres (pgvector) ---\n",
    "@mlflow.trace\n",
    "def pg_vector_similarity_search(\n",
    "    query_text: str,\n",
    "    k: int = 3,\n",
    "    filters: Optional[Dict[str, Any]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Perform similarity search against message embeddings in Postgres (pgvector).\n",
    "\n",
    "    Schema expectations:\n",
    "    - message_embeddings(me: id, message_id, user_name, chat_id, embedding vector)\n",
    "    - chat_history(ch: id, message_content, message_type, created_at, message_order)\n",
    "    \"\"\"\n",
    "    filters = filters or {}\n",
    "\n",
    "    # 1) Embed the query\n",
    "    query_embedding = embeddings.embed_query(query_text)\n",
    "\n",
    "    # 2) WHERE clause from filters\n",
    "    where_conditions: List[str] = []\n",
    "    params: Dict[str, Any] = {}\n",
    "\n",
    "    if \"user_name\" in filters:\n",
    "        where_conditions.append(\"me.user_name = :user_name\")\n",
    "        params[\"user_name\"] = filters[\"user_name\"]\n",
    "\n",
    "    where_clause = \"\"\n",
    "    if where_conditions:\n",
    "        where_clause = \"WHERE \" + \" AND \".join(where_conditions)\n",
    "\n",
    "    # 3) Query using cosine distance operator (<=>) provided by pgvector\n",
    "    sql = text(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            ch.message_content,\n",
    "            me.user_name,\n",
    "            me.chat_id,\n",
    "            ch.message_type,\n",
    "            ch.created_at,\n",
    "            ch.message_order,\n",
    "            (me.embedding <=> CAST(:query_embedding AS vector)) AS distance\n",
    "        FROM message_embeddings me\n",
    "        JOIN chat_history ch ON me.message_id = ch.id\n",
    "        {where_clause}\n",
    "        ORDER BY me.embedding <=> CAST(:query_embedding AS vector)\n",
    "        LIMIT :k\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_outputs([Document(page_content=sql)])\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(\n",
    "            sql, {\"query_embedding\": query_embedding, \"k\": k, **params}\n",
    "        ).fetchall()\n",
    "\n",
    "    passages = [f\"Passage: {r.message_content}\" for r in rows]\n",
    "    return \"\\n\".join(passages)\n",
    "  \n",
    "\n",
    "def create_context_aware_vector_search_tool(state, custom_k: Optional[int] = None):\n",
    "    \"\"\"Create a vector search tool that has access to user context from state\"\"\"\n",
    "    \n",
    "    def filtered_vector_search(query: str) -> str:\n",
    "        # Extract user context from state\n",
    "        user_context = state.get(\"user_context\", {})\n",
    "        filters = user_context.get(\"filters\", {})\n",
    "        \n",
    "        # Use custom k if provided, otherwise fall back to model_config default\n",
    "        k = custom_k if custom_k is not None else model_config.get('k')\n",
    "        \n",
    "        # Use your existing pg_vector_similarity_search with filters and custom k\n",
    "        return pg_vector_similarity_search(\n",
    "            query_text=query, \n",
    "            k=k, \n",
    "            filters=filters\n",
    "        )\n",
    "    \n",
    "    return Tool(\n",
    "        name=\"search_chat_history\",\n",
    "        description=\"Retrieve chat history from Postgres (pgvector) for the current user; use only if the immediate conversation context is insufficient. The input to this function should be the user message.\",\n",
    "        func=filtered_vector_search,\n",
    "    )\n",
    "\n",
    "\n",
    "genie_agent_description = model_config.get('genie_agent_description')\n",
    "general_assistant_description = model_config.get('general_assistant_description')\n",
    "code_agent_description = model_config.get('code_agent_description')\n",
    "\n",
    "genie_agent = GenieAgent(\n",
    "    genie_space_id=model_config.get('genie_space_id'),\n",
    "    genie_agent_name=\"Genie\",\n",
    "    description=genie_agent_description,\n",
    "    client=workspace_client,\n",
    "    include_context=True,\n",
    ")\n",
    "\n",
    "# Max number of interactions between agents\n",
    "MAX_ITERATIONS = 3\n",
    "\n",
    "worker_descriptions = {\n",
    "    \"Genie\": genie_agent_description,\n",
    "    \"General\": general_assistant_description,\n",
    "    \"Coder\": code_agent_description,\n",
    "}\n",
    "\n",
    "formatted_descriptions = \"\\n\".join(\n",
    "    f\"- {name}: {desc}\" for name, desc in worker_descriptions.items()\n",
    ")\n",
    "\n",
    "system_prompt = f\"Decide between routing between the following workers or ending the conversation if an answer is provided. \\n{formatted_descriptions}\"\n",
    "options = [\"FINISH\"] + list(worker_descriptions.keys())\n",
    "FINISH = {\"next_node\": \"FINISH\"}\n",
    "\n",
    "# Our foundation model answering the final prompt\n",
    "model = ChatDatabricks(\n",
    "    endpoint=model_config.get(\"llm_model_serving_endpoint_name\"),\n",
    "    extra_params={\"temperature\": 0.01, \"max_tokens\": 500}\n",
    ")\n",
    "\n",
    "# Custom Static Tools\n",
    "tools = []\n",
    "\n",
    "def supervisor_agent(state):\n",
    "    count = state.get(\"iteration_count\", 0) + 1\n",
    "    if count > MAX_ITERATIONS:\n",
    "        return FINISH\n",
    "    \n",
    "    class nextNode(BaseModel):\n",
    "        next_node: Literal[tuple(options)]\n",
    "\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    )\n",
    "    supervisor_chain = preprocessor | model.with_structured_output(nextNode)\n",
    "    next_node = supervisor_chain.invoke(state).next_node\n",
    "    \n",
    "    # if routed back to the same node, exit the loop\n",
    "    if state.get(\"next_node\") == next_node:\n",
    "        return FINISH\n",
    "    return {\n",
    "        \"iteration_count\": count,\n",
    "        \"next_node\": next_node\n",
    "    }\n",
    "\n",
    "#######################################\n",
    "# Define our multiagent graph structure\n",
    "#######################################\n",
    "\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": result[\"messages\"][-1].content,\n",
    "                \"name\": name,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def final_answer(state):\n",
    "    prompt = \"Using only the content in the messages, respond to the previous user question using the answer given by the other assistant messages.\"\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: state[\"messages\"] + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    final_answer_chain = preprocessor | model\n",
    "    return {\"messages\": [final_answer_chain.invoke(state)]}\n",
    "\n",
    "\n",
    "def agent_node_with_context(state, agent, name, custom_k: Optional[int] = None):\n",
    "    \"\"\"Enhanced agent node that injects context-aware tools\"\"\"\n",
    "    \n",
    "    # Create the shared vector search tool with current state context and custom k\n",
    "    vector_search_tool = create_context_aware_vector_search_tool(state, custom_k)\n",
    "    \n",
    "    if name == \"Genie\":\n",
    "        # Genie already has its tools, just add vector search\n",
    "        enhanced_agent = agent  # Genie agent already configured\n",
    "        \n",
    "    elif name == \"Coder\" or name == \"General\":\n",
    "        # Add vector search tool to other agents' tools\n",
    "        enhanced_tools = tools + [vector_search_tool]\n",
    "        enhanced_agent = create_react_agent(model, tools=enhanced_tools)\n",
    "        \n",
    "    # Execute with enhanced agent\n",
    "    result = enhanced_agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": result[\"messages\"][-1].content,\n",
    "            \"name\": name,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "class AgentState(ChatAgentState):\n",
    "    next_node: str\n",
    "    iteration_count: int\n",
    "    user_context: Optional[Dict[str, Any]] = None\n",
    "    custom_k: Optional[int] = None\n",
    "\n",
    "# Create enhanced agent nodes\n",
    "def enhanced_genie_node(state):\n",
    "    custom_k = state.get(\"custom_k\")\n",
    "    return agent_node_with_context(state, genie_agent, \"Genie\", custom_k)\n",
    "\n",
    "def enhanced_coder_node(state):\n",
    "    custom_k = state.get(\"custom_k\")\n",
    "    return agent_node_with_context(state, None, \"Coder\", custom_k)\n",
    "\n",
    "def enhanced_general_node(state):\n",
    "    custom_k = state.get(\"custom_k\")\n",
    "    return agent_node_with_context(state, None, \"General\", custom_k)\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "# Agent States\n",
    "workflow.add_node(\"Genie\", enhanced_genie_node)\n",
    "workflow.add_node(\"Coder\", enhanced_coder_node)\n",
    "workflow.add_node(\"General\", enhanced_general_node)\n",
    "# Supervisor States\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow.add_node(\"final_answer\", final_answer)\n",
    "\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "# We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "for worker in worker_descriptions.keys():\n",
    "    workflow.add_edge(worker, \"supervisor\")\n",
    "\n",
    "# Let the supervisor decide which next node to go\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda x: x[\"next_node\"],\n",
    "    {**{k: k for k in worker_descriptions.keys()}, \"FINISH\": \"final_answer\"},\n",
    ")\n",
    "workflow.add_edge(\"final_answer\", END)\n",
    "multi_agent = workflow.compile()\n",
    "\n",
    "###################################\n",
    "# Pydantic-based PythonModel\n",
    "###################################\n",
    "\n",
    "class ChatAgentPythonModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self):\n",
    "        self.agent = multi_agent\n",
    "        \n",
    "    def predict(self, context, model_input: List[ChatRequest]) -> List[ChatResponse]:\n",
    "        \"\"\"\n",
    "        Predict method with pydantic type hints for automatic schema generation.\n",
    "        Note: MLflow expects List[InputType] -> List[OutputType] for pyfunc models.\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        for request in model_input:\n",
    "            # Extract user context from request\n",
    "            user_context = {}\n",
    "            custom_k = None\n",
    "            \n",
    "            if request.custom_inputs:\n",
    "                user_context[\"filters\"] = request.custom_inputs.filters.model_dump()\n",
    "                custom_k = request.custom_inputs.k\n",
    "            \n",
    "            # Convert pydantic messages to dict format for langgraph\n",
    "            messages_dict = [msg.model_dump(exclude_none=True) for msg in request.messages]\n",
    "            \n",
    "            agent_request = {\n",
    "                \"messages\": messages_dict,\n",
    "                \"user_context\": user_context,\n",
    "                \"custom_k\": custom_k\n",
    "            }\n",
    "\n",
    "            response_messages = []\n",
    "            for event in self.agent.stream(agent_request, stream_mode=\"updates\"):\n",
    "                for node_data in event.values():\n",
    "                    for msg in node_data.get(\"messages\", []):\n",
    "                        response_messages.append(Message(**msg))\n",
    "            \n",
    "            responses.append(ChatResponse(messages=response_messages))\n",
    "        \n",
    "        return responses\n",
    "\n",
    "# Create the model instance and set it for MLflow\n",
    "model_instance = ChatAgentPythonModel()\n",
    "mlflow.models.set_model(model=model_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354c76a9-8be0-4c8d-9983-9e57eb961e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"embedding_model\", \"databricks-gte-large-en\")\n",
    "dbutils.widgets.text(\"database_instance_name\", \"tannerw-adtech-db\")\n",
    "dbutils.widgets.text(\"postgres_database_name\", \"databricks_postgres\")\n",
    "dbutils.widgets.text(\"llm_model_serving_endpoint_name\", \"databricks-claude-3-7-sonnet\")\n",
    "dbutils.widgets.text(\"target_catalog\", \"tanner_wendland\")\n",
    "dbutils.widgets.text(\"target_schema\", \"default\")\n",
    "dbutils.widgets.text(\"genie_space_id\", \"01efcca6fdc712d7be87a40ad4a2e33e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99bd51e-5492-4d5d-a971-0ddda14452cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = dbutils.widgets.get(\"embedding_model\")\n",
    "database_instance_name = dbutils.widgets.get(\"database_instance_name\")\n",
    "postgres_database_name = dbutils.widgets.get(\"postgres_database_name\")\n",
    "llm_model_serving_endpoint_name = dbutils.widgets.get(\"llm_model_serving_endpoint_name\")\n",
    "target_catalog = dbutils.widgets.get(\"target_catalog\")\n",
    "target_schema = dbutils.widgets.get(\"target_schema\")\n",
    "genie_space_id = dbutils.widgets.get(\"genie_space_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2750086a-7c5b-40aa-b5f6-28fec9147b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "from typing import Any, Generator, Literal, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d115d39-47b3-40a6-b143-e4ae3f30c1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/02 20:00:40 WARNING mlflow.tracking.fluent: Exception raised while enabling autologging for pyspark: MLflow Spark dataset autologging is not supported on Databricks shared clusters or Databricks serverless clusters.\n2025/09/02 20:00:40 WARNING mlflow.tracking.fluent: Exception raised while enabling autologging for pyspark.ml: [JVM_ATTRIBUTE_NOT_SUPPORTED] Attribute `sparkContext` is not supported in Spark Connect as it depends on the JVM. If you need to use this attribute, do not use Spark Connect when creating your session. Visit https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession for creating regular Spark Session in detail.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.autolog()\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "585760d5-cc0a-4a30-b069-1b57188e8623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9d15d1-397a-4cdf-b948-41a6ad5555f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain_config = {\n",
    "    \"llm_model_serving_endpoint_name\": llm_model_serving_endpoint_name,\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"database_instance_name\": database_instance_name,\n",
    "    \"postgres_database_name\": postgres_database_name,\n",
    "    \"genie_space_id\": genie_space_id,\n",
    "    \"k\": 3,\n",
    "    \"genie_agent_description\": \"You are a an agent that can invoke Genie, a powerful text-to-sql database assistant. You can use this tool to answer questions related to sales pipelines.\",\n",
    "    \"general_assistant_description\": \"The General Assistant agent is a helpful assistant that can answer any question.\",\n",
    "    \"code_agent_description\": \"The Coder agent specializes in solving programming challenges, generating code snippets, debugging issues, and explaining complex coding concepts.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "247bc64a-7331-4a60-97fe-52d0360bd338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e706a3d-0b01-44e6-95e1-fcaa22ffaf44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace URL: dbc-daae04e5-a828.cloud.databricks.com\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/02 20:06:39 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n\uD83D\uDD17 View Logged Model at: https://dbc-daae04e5-a828.cloud.databricks.com/ml/experiments/3857563691880669/models/m-f220d15e7cba49d29437b5777d580e22?o=2811078624112887\n/databricks/python/lib/python3.12/site-packages/databricks/connect/session.py:475: UserWarning: Ignoring the default notebook Spark session and creating a new Spark Connect session. To use the default notebook Spark session, use DatabricksSession.builder.getOrCreate() with no additional parameters.\n  warnings.warn(new_notebook_session_msg)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPermissionDenied\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/models/utils.py:1960\u001B[0m, in \u001B[0;36m_load_model_code_path\u001B[0;34m(code_path, model_config)\u001B[0m\n",
       "\u001B[1;32m   1959\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _mock_dbutils(module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m):\n",
       "\u001B[0;32m-> 1960\u001B[0m         spec\u001B[38;5;241m.\u001B[39mloader\u001B[38;5;241m.\u001B[39mexec_module(module)\n",
       "\u001B[1;32m   1961\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "File \u001B[0;32m<frozen importlib._bootstrap_external>:995\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
       "\n",
       "File \u001B[0;32m<frozen importlib._bootstrap>:488\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/Workspace/Users/tanner.wendland@databricks.com/.bundle/tannerw_adtech_demo/dev/files/data_pipelines/src/chain_postgres_genie.py:253\u001B[0m\n",
       "\u001B[1;32m    251\u001B[0m code_agent_description \u001B[38;5;241m=\u001B[39m model_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcode_agent_description\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[0;32m--> 253\u001B[0m genie_agent \u001B[38;5;241m=\u001B[39m GenieAgent(\n",
       "\u001B[1;32m    254\u001B[0m     genie_space_id\u001B[38;5;241m=\u001B[39mmodel_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgenie_space_id\u001B[39m\u001B[38;5;124m'\u001B[39m),\n",
       "\u001B[1;32m    255\u001B[0m     genie_agent_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenie\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    256\u001B[0m     description\u001B[38;5;241m=\u001B[39mgenie_agent_description,\n",
       "\u001B[1;32m    257\u001B[0m     client\u001B[38;5;241m=\u001B[39mworkspace_client,\n",
       "\u001B[1;32m    258\u001B[0m     include_context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n",
       "\u001B[1;32m    259\u001B[0m )\n",
       "\u001B[1;32m    261\u001B[0m \u001B[38;5;66;03m# Max number of interactions between agents\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/tracing/fluent.py:246\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[0;32m--> 246\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _WrappingContext(fn, args, kwargs) \u001B[38;5;28;01mas\u001B[39;00m wrapping_coro:\n",
       "\u001B[1;32m    247\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m wrapping_coro\u001B[38;5;241m.\u001B[39msend(fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/tracing/fluent.py:235\u001B[0m, in \u001B[0;36m_wrap_function.<locals>._WrappingContext.__exit__\u001B[0;34m(self, exc_type, exc_value, traceback)\u001B[0m\n",
       "\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m exc_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m--> 235\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoro\u001B[38;5;241m.\u001B[39mthrow(exc_type, exc_value, traceback)\n",
       "\u001B[1;32m    236\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoro\u001B[38;5;241m.\u001B[39mclose()\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/tracing/fluent.py:214\u001B[0m, in \u001B[0;36m_wrap_function.<locals>._WrappingContext._wrapping_logic\u001B[0;34m(fn, args, kwargs)\u001B[0m\n",
       "\u001B[1;32m    213\u001B[0m span\u001B[38;5;241m.\u001B[39mset_inputs(inputs)\n",
       "\u001B[0;32m--> 214\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01myield\u001B[39;00m  \u001B[38;5;66;03m# sync/async function output to be sent here\u001B[39;00m\n",
       "\u001B[1;32m    215\u001B[0m span\u001B[38;5;241m.\u001B[39mset_outputs(result)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/tracing/fluent.py:247\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _WrappingContext(fn, args, kwargs) \u001B[38;5;28;01mas\u001B[39;00m wrapping_coro:\n",
       "\u001B[0;32m--> 247\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m wrapping_coro\u001B[38;5;241m.\u001B[39msend(fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks_langchain/genie.py:116\u001B[0m, in \u001B[0;36mGenieAgent\u001B[0;34m(genie_space_id, genie_agent_name, description, include_context, message_processor, client)\u001B[0m\n",
       "\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrunnables\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RunnableLambda\n",
       "\u001B[0;32m--> 116\u001B[0m genie \u001B[38;5;241m=\u001B[39m Genie(genie_space_id, client\u001B[38;5;241m=\u001B[39mclient)\n",
       "\u001B[1;32m    118\u001B[0m \u001B[38;5;66;03m# Create a partial function with the genie_space_id pre-filled\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks_ai_bridge/genie.py:111\u001B[0m, in \u001B[0;36mGenie.__init__\u001B[0;34m(self, space_id, client, truncate_results)\u001B[0m\n",
       "\u001B[1;32m    110\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenie \u001B[38;5;241m=\u001B[39m workspace_client\u001B[38;5;241m.\u001B[39mgenie\n",
       "\u001B[0;32m--> 111\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdescription \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenie\u001B[38;5;241m.\u001B[39mget_space(space_id)\u001B[38;5;241m.\u001B[39mdescription\n",
       "\u001B[1;32m    112\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheaders \u001B[38;5;241m=\u001B[39m {\n",
       "\u001B[1;32m    113\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccept\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    114\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContent-Type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    115\u001B[0m }\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks/sdk/service/dashboards.py:1802\u001B[0m, in \u001B[0;36mGenieAPI.get_space\u001B[0;34m(self, space_id)\u001B[0m\n",
       "\u001B[1;32m   1798\u001B[0m headers \u001B[38;5;241m=\u001B[39m {\n",
       "\u001B[1;32m   1799\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccept\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1800\u001B[0m }\n",
       "\u001B[0;32m-> 1802\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api\u001B[38;5;241m.\u001B[39mdo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGET\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/api/2.0/genie/spaces/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mspace_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, headers\u001B[38;5;241m=\u001B[39mheaders)\n",
       "\u001B[1;32m   1803\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m GenieSpace\u001B[38;5;241m.\u001B[39mfrom_dict(res)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks/sdk/core.py:85\u001B[0m, in \u001B[0;36mApiClient.do\u001B[0;34m(self, method, path, url, query, headers, body, raw, files, data, auth, response_headers)\u001B[0m\n",
       "\u001B[1;32m     84\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cfg\u001B[38;5;241m.\u001B[39mhost\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api_client\u001B[38;5;241m.\u001B[39mdo(\n",
       "\u001B[1;32m     86\u001B[0m     method\u001B[38;5;241m=\u001B[39mmethod,\n",
       "\u001B[1;32m     87\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n",
       "\u001B[1;32m     88\u001B[0m     query\u001B[38;5;241m=\u001B[39mquery,\n",
       "\u001B[1;32m     89\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n",
       "\u001B[1;32m     90\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n",
       "\u001B[1;32m     91\u001B[0m     raw\u001B[38;5;241m=\u001B[39mraw,\n",
       "\u001B[1;32m     92\u001B[0m     files\u001B[38;5;241m=\u001B[39mfiles,\n",
       "\u001B[1;32m     93\u001B[0m     data\u001B[38;5;241m=\u001B[39mdata,\n",
       "\u001B[1;32m     94\u001B[0m     auth\u001B[38;5;241m=\u001B[39mauth,\n",
       "\u001B[1;32m     95\u001B[0m     response_headers\u001B[38;5;241m=\u001B[39mresponse_headers,\n",
       "\u001B[1;32m     96\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks/sdk/_base_client.py:196\u001B[0m, in \u001B[0;36m_BaseClient.do\u001B[0;34m(self, method, url, query, headers, body, raw, files, data, auth, response_headers)\u001B[0m\n",
       "\u001B[1;32m    194\u001B[0m     call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_perform\n",
       "\u001B[0;32m--> 196\u001B[0m response \u001B[38;5;241m=\u001B[39m call(\n",
       "\u001B[1;32m    197\u001B[0m     method,\n",
       "\u001B[1;32m    198\u001B[0m     url,\n",
       "\u001B[1;32m    199\u001B[0m     query\u001B[38;5;241m=\u001B[39mquery,\n",
       "\u001B[1;32m    200\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n",
       "\u001B[1;32m    201\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n",
       "\u001B[1;32m    202\u001B[0m     raw\u001B[38;5;241m=\u001B[39mraw,\n",
       "\u001B[1;32m    203\u001B[0m     files\u001B[38;5;241m=\u001B[39mfiles,\n",
       "\u001B[1;32m    204\u001B[0m     data\u001B[38;5;241m=\u001B[39mdata,\n",
       "\u001B[1;32m    205\u001B[0m     auth\u001B[38;5;241m=\u001B[39mauth,\n",
       "\u001B[1;32m    206\u001B[0m )\n",
       "\u001B[1;32m    208\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks/sdk/retries.py:57\u001B[0m, in \u001B[0;36mretried.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retry_reason \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m     56\u001B[0m     \u001B[38;5;66;03m# raise if exception is not retryable\u001B[39;00m\n",
       "\u001B[0;32m---> 57\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n",
       "\u001B[1;32m     59\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetrying: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mretry_reason\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (sleeping ~\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msleep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks/sdk/retries.py:36\u001B[0m, in \u001B[0;36mretried.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks/sdk/_base_client.py:298\u001B[0m, in \u001B[0;36m_BaseClient._perform\u001B[0;34m(self, method, url, query, headers, body, raw, files, data, auth)\u001B[0m\n",
       "\u001B[1;32m    297\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m--> 298\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
       "\n",
       "\u001B[0;31mPermissionDenied\u001B[0m: You need \"Can View\" permission to perform this action. Config: host=https://dbc-daae04e5-a828.cloud.databricks.com, token=***, auth_type=pat. Env: DATABRICKS_HOST, DATABRICKS_TOKEN\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mMlflowException\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5737137237918700>, line 25\u001B[0m\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Use pyfunc log_model with the file path (models-from-code approach)\u001B[39;00m\n",
       "\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mstart_run(run_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madtech_chat_history_agent_postgres_genie_pydantic\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[0;32m---> 25\u001B[0m     logged_chain_info \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mpyfunc\u001B[38;5;241m.\u001B[39mlog_model(\n",
       "\u001B[1;32m     26\u001B[0m         python_model\u001B[38;5;241m=\u001B[39mchain_file_path,  \u001B[38;5;66;03m# Path to the chain file\u001B[39;00m\n",
       "\u001B[1;32m     27\u001B[0m         model_config\u001B[38;5;241m=\u001B[39mchain_config,\n",
       "\u001B[1;32m     28\u001B[0m         artifact_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchat_agent_pydantic\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     29\u001B[0m         input_example\u001B[38;5;241m=\u001B[39minput_example,  \u001B[38;5;66;03m# Schema auto-inferred from List[ChatRequest] type hint\u001B[39;00m\n",
       "\u001B[1;32m     30\u001B[0m         \u001B[38;5;66;03m# Specify resources for automatic authentication passthrough\u001B[39;00m\n",
       "\u001B[1;32m     31\u001B[0m         resources\u001B[38;5;241m=\u001B[39m[\n",
       "\u001B[1;32m     32\u001B[0m             DatabricksServingEndpoint(endpoint_name\u001B[38;5;241m=\u001B[39mmodel_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllm_model_serving_endpoint_name\u001B[39m\u001B[38;5;124m\"\u001B[39m)),\n",
       "\u001B[1;32m     33\u001B[0m             DatabricksGenieSpace(genie_space_id\u001B[38;5;241m=\u001B[39mmodel_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenie_space_id\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m     34\u001B[0m         ],\n",
       "\u001B[1;32m     35\u001B[0m         pip_requirements\u001B[38;5;241m=\u001B[39m[\n",
       "\u001B[1;32m     36\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlflow==3.2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     37\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatabricks-agents==1.4.0\u001B[39m\u001B[38;5;124m\"\u001B[39m, \n",
       "\u001B[1;32m     38\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatabricks-langchain==0.7.1\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     39\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlangchain==0.3.27\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     40\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpgvector==0.2.5\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     41\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpsycopg2-binary==2.9.7\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     42\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpydantic==2.11.7\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     43\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqlalchemy==2.0.43\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     44\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtornado==6.3.2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     45\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlanggraph==0.3.4\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     46\u001B[0m         ]\n",
       "\u001B[1;32m     47\u001B[0m     )\n",
       "\u001B[1;32m     49\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchat_history_agent_postgres_genie\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     50\u001B[0m MODEL_NAME_FQN \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget_catalog\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget_schema\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/tracing/provider.py:465\u001B[0m, in \u001B[0;36mtrace_disabled.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    464\u001B[0m     is_func_called \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[0;32m--> 465\u001B[0m     result \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    466\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m    467\u001B[0m     enable()\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/pyfunc/__init__.py:3602\u001B[0m, in \u001B[0;36mlog_model\u001B[0;34m(artifact_path, loader_module, data_path, code_paths, infer_code_paths, conda_env, python_model, artifacts, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, metadata, model_config, streamable, resources, auth_policy, prompts, name, params, tags, model_type, step, model_id)\u001B[0m\n",
       "\u001B[1;32m   3371\u001B[0m \u001B[38;5;129m@format_docstring\u001B[39m(LOG_MODEL_PARAM_DOCS\u001B[38;5;241m.\u001B[39mformat(package_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscikit-learn\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m   3372\u001B[0m \u001B[38;5;129m@trace_disabled\u001B[39m  \u001B[38;5;66;03m# Suppress traces for internal predict calls while logging model\u001B[39;00m\n",
       "\u001B[1;32m   3373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlog_model\u001B[39m(\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3399\u001B[0m     model_id: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m   3400\u001B[0m ):\n",
       "\u001B[1;32m   3401\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   3402\u001B[0m \u001B[38;5;124;03m    Log a Pyfunc model with custom inference logic and optional data dependencies as an MLflow\u001B[39;00m\n",
       "\u001B[1;32m   3403\u001B[0m \u001B[38;5;124;03m    artifact for the current run.\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3600\u001B[0m \u001B[38;5;124;03m        metadata of the logged model.\u001B[39;00m\n",
       "\u001B[1;32m   3601\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Model\u001B[38;5;241m.\u001B[39mlog(\n",
       "\u001B[1;32m   3603\u001B[0m         artifact_path\u001B[38;5;241m=\u001B[39martifact_path,\n",
       "\u001B[1;32m   3604\u001B[0m         name\u001B[38;5;241m=\u001B[39mname,\n",
       "\u001B[1;32m   3605\u001B[0m         flavor\u001B[38;5;241m=\u001B[39mmlflow\u001B[38;5;241m.\u001B[39mpyfunc,\n",
       "\u001B[1;32m   3606\u001B[0m         loader_module\u001B[38;5;241m=\u001B[39mloader_module,\n",
       "\u001B[1;32m   3607\u001B[0m         data_path\u001B[38;5;241m=\u001B[39mdata_path,\n",
       "\u001B[1;32m   3608\u001B[0m         code_paths\u001B[38;5;241m=\u001B[39mcode_paths,\n",
       "\u001B[1;32m   3609\u001B[0m         python_model\u001B[38;5;241m=\u001B[39mpython_model,\n",
       "\u001B[1;32m   3610\u001B[0m         artifacts\u001B[38;5;241m=\u001B[39martifacts,\n",
       "\u001B[1;32m   3611\u001B[0m         conda_env\u001B[38;5;241m=\u001B[39mconda_env,\n",
       "\u001B[1;32m   3612\u001B[0m         registered_model_name\u001B[38;5;241m=\u001B[39mregistered_model_name,\n",
       "\u001B[1;32m   3613\u001B[0m         signature\u001B[38;5;241m=\u001B[39msignature,\n",
       "\u001B[1;32m   3614\u001B[0m         input_example\u001B[38;5;241m=\u001B[39minput_example,\n",
       "\u001B[1;32m   3615\u001B[0m         await_registration_for\u001B[38;5;241m=\u001B[39mawait_registration_for,\n",
       "\u001B[1;32m   3616\u001B[0m         pip_requirements\u001B[38;5;241m=\u001B[39mpip_requirements,\n",
       "\u001B[1;32m   3617\u001B[0m         extra_pip_requirements\u001B[38;5;241m=\u001B[39mextra_pip_requirements,\n",
       "\u001B[1;32m   3618\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n",
       "\u001B[1;32m   3619\u001B[0m         prompts\u001B[38;5;241m=\u001B[39mprompts,\n",
       "\u001B[1;32m   3620\u001B[0m         model_config\u001B[38;5;241m=\u001B[39mmodel_config,\n",
       "\u001B[1;32m   3621\u001B[0m         streamable\u001B[38;5;241m=\u001B[39mstreamable,\n",
       "\u001B[1;32m   3622\u001B[0m         resources\u001B[38;5;241m=\u001B[39mresources,\n",
       "\u001B[1;32m   3623\u001B[0m         infer_code_paths\u001B[38;5;241m=\u001B[39minfer_code_paths,\n",
       "\u001B[1;32m   3624\u001B[0m         auth_policy\u001B[38;5;241m=\u001B[39mauth_policy,\n",
       "\u001B[1;32m   3625\u001B[0m         params\u001B[38;5;241m=\u001B[39mparams,\n",
       "\u001B[1;32m   3626\u001B[0m         tags\u001B[38;5;241m=\u001B[39mtags,\n",
       "\u001B[1;32m   3627\u001B[0m         model_type\u001B[38;5;241m=\u001B[39mmodel_type,\n",
       "\u001B[1;32m   3628\u001B[0m         step\u001B[38;5;241m=\u001B[39mstep,\n",
       "\u001B[1;32m   3629\u001B[0m         model_id\u001B[38;5;241m=\u001B[39mmodel_id,\n",
       "\u001B[1;32m   3630\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/models/model.py:1214\u001B[0m, in \u001B[0;36mModel.log\u001B[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, name, model_type, params, tags, step, model_id, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1202\u001B[0m     prompts \u001B[38;5;241m=\u001B[39m [pr\u001B[38;5;241m.\u001B[39muri \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pr, PromptVersion) \u001B[38;5;28;01melse\u001B[39;00m pr \u001B[38;5;28;01mfor\u001B[39;00m pr \u001B[38;5;129;01min\u001B[39;00m prompts]\n",
       "\u001B[1;32m   1204\u001B[0m mlflow_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\n",
       "\u001B[1;32m   1205\u001B[0m     artifact_path\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39martifact_location,\n",
       "\u001B[1;32m   1206\u001B[0m     model_uuid\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mmodel_id,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1212\u001B[0m     model_id\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mmodel_id,\n",
       "\u001B[1;32m   1213\u001B[0m )\n",
       "\u001B[0;32m-> 1214\u001B[0m flavor\u001B[38;5;241m.\u001B[39msave_model(path\u001B[38;5;241m=\u001B[39mlocal_path, mlflow_model\u001B[38;5;241m=\u001B[39mmlflow_model, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1215\u001B[0m \u001B[38;5;66;03m# `save_model` calls `load_model` to infer the model requirements, which may result\u001B[39;00m\n",
       "\u001B[1;32m   1216\u001B[0m \u001B[38;5;66;03m# in __pycache__ directories being created in the model directory.\u001B[39;00m\n",
       "\u001B[1;32m   1217\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pycache \u001B[38;5;129;01min\u001B[39;00m Path(local_path)\u001B[38;5;241m.\u001B[39mrglob(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__pycache__\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/tracing/provider.py:470\u001B[0m, in \u001B[0;36mtrace_disabled.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    468\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    469\u001B[0m         is_func_called \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[0;32m--> 470\u001B[0m         result \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    471\u001B[0m \u001B[38;5;66;03m# We should only catch the exception from disable() and enable()\u001B[39;00m\n",
       "\u001B[1;32m    472\u001B[0m \u001B[38;5;66;03m# and let other exceptions propagate.\u001B[39;00m\n",
       "\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m MlflowTracingException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/pyfunc/__init__.py:3056\u001B[0m, in \u001B[0;36msave_model\u001B[0;34m(path, loader_module, data_path, code_paths, infer_code_paths, conda_env, mlflow_model, python_model, artifacts, signature, input_example, pip_requirements, extra_pip_requirements, metadata, model_config, streamable, resources, auth_policy, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   3054\u001B[0m     model_code_path \u001B[38;5;241m=\u001B[39m _validate_and_get_model_code_path(python_model, temp_dir)\n",
       "\u001B[1;32m   3055\u001B[0m     _validate_and_copy_file_to_directory(model_code_path, path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcode\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m-> 3056\u001B[0m     python_model \u001B[38;5;241m=\u001B[39m _load_model_code_path(model_code_path, model_config)\n",
       "\u001B[1;32m   3058\u001B[0m _validate_function_python_model(python_model)\n",
       "\u001B[1;32m   3059\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(python_model) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\n",
       "\u001B[1;32m   3060\u001B[0m     a \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m (input_example, pip_requirements, extra_pip_requirements)\n",
       "\u001B[1;32m   3061\u001B[0m ):\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/models/utils.py:1966\u001B[0m, in \u001B[0;36m_load_model_code_path\u001B[0;34m(code_path, model_config)\u001B[0m\n",
       "\u001B[1;32m   1962\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n",
       "\u001B[1;32m   1963\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import code model from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcode_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m!s}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1964\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
       "\u001B[1;32m   1965\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[0;32m-> 1966\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n",
       "\u001B[1;32m   1967\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to run user code from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcode_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1968\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m!s}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1969\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReview the stack trace for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1970\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
       "\u001B[1;32m   1972\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m__mlflow_model__ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1973\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n",
       "\u001B[1;32m   1974\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf the model is logged as code, ensure the model is set using \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1975\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlflow.models.set_model() within the code file code file.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1976\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mMlflowException\u001B[0m: Failed to run user code from /Workspace/Users/tanner.wendland@databricks.com/.bundle/tannerw_adtech_demo/dev/files/data_pipelines/src/chain_postgres_genie.py. Error: You need \"Can View\" permission to perform this action. Config: host=https://dbc-daae04e5-a828.cloud.databricks.com, token=***, auth_type=pat. Env: DATABRICKS_HOST, DATABRICKS_TOKEN. Review the stack trace for more information."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "MlflowException",
        "evalue": "Failed to run user code from /Workspace/Users/tanner.wendland@databricks.com/.bundle/tannerw_adtech_demo/dev/files/data_pipelines/src/chain_postgres_genie.py. Error: You need \"Can View\" permission to perform this action. Config: host=https://dbc-daae04e5-a828.cloud.databricks.com, token=***, auth_type=pat. Env: DATABRICKS_HOST, DATABRICKS_TOKEN. Review the stack trace for more information."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>MlflowException</span>: Failed to run user code from /Workspace/Users/tanner.wendland@databricks.com/.bundle/tannerw_adtech_demo/dev/files/data_pipelines/src/chain_postgres_genie.py. Error: You need \"Can View\" permission to perform this action. Config: host=https://dbc-daae04e5-a828.cloud.databricks.com, token=***, auth_type=pat. Env: DATABRICKS_HOST, DATABRICKS_TOKEN. Review the stack trace for more information.\n[Trace ID: 00-b9e9b67f118296f7c7f7b6bc234035c1-d3a21e27bea287c0-00]"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPermissionDenied\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/models/utils.py:1960\u001B[0m, in \u001B[0;36m_load_model_code_path\u001B[0;34m(code_path, model_config)\u001B[0m\n\u001B[1;32m   1959\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _mock_dbutils(module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m):\n\u001B[0;32m-> 1960\u001B[0m         spec\u001B[38;5;241m.\u001B[39mloader\u001B[38;5;241m.\u001B[39mexec_module(module)\n\u001B[1;32m   1961\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
        "File \u001B[0;32m<frozen importlib._bootstrap_external>:995\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
        "File \u001B[0;32m<frozen importlib._bootstrap>:488\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
        "File \u001B[0;32m/Workspace/Users/tanner.wendland@databricks.com/.bundle/tannerw_adtech_demo/dev/files/data_pipelines/src/chain_postgres_genie.py:253\u001B[0m\n\u001B[1;32m    251\u001B[0m code_agent_description \u001B[38;5;241m=\u001B[39m model_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcode_agent_description\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 253\u001B[0m genie_agent \u001B[38;5;241m=\u001B[39m GenieAgent(\n\u001B[1;32m    254\u001B[0m     genie_space_id\u001B[38;5;241m=\u001B[39mmodel_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgenie_space_id\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m    255\u001B[0m     genie_agent_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenie\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    256\u001B[0m     description\u001B[38;5;241m=\u001B[39mgenie_agent_description,\n\u001B[1;32m    257\u001B[0m     client\u001B[38;5;241m=\u001B[39mworkspace_client,\n\u001B[1;32m    258\u001B[0m     include_context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    259\u001B[0m )\n\u001B[1;32m    261\u001B[0m \u001B[38;5;66;03m# Max number of interactions between agents\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/tracing/fluent.py:246\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 246\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _WrappingContext(fn, args, kwargs) \u001B[38;5;28;01mas\u001B[39;00m wrapping_coro:\n\u001B[1;32m    247\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m wrapping_coro\u001B[38;5;241m.\u001B[39msend(fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/tracing/fluent.py:235\u001B[0m, in \u001B[0;36m_wrap_function.<locals>._WrappingContext.__exit__\u001B[0;34m(self, exc_type, exc_value, traceback)\u001B[0m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m exc_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 235\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoro\u001B[38;5;241m.\u001B[39mthrow(exc_type, exc_value, traceback)\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoro\u001B[38;5;241m.\u001B[39mclose()\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/tracing/fluent.py:214\u001B[0m, in \u001B[0;36m_wrap_function.<locals>._WrappingContext._wrapping_logic\u001B[0;34m(fn, args, kwargs)\u001B[0m\n\u001B[1;32m    213\u001B[0m span\u001B[38;5;241m.\u001B[39mset_inputs(inputs)\n\u001B[0;32m--> 214\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01myield\u001B[39;00m  \u001B[38;5;66;03m# sync/async function output to be sent here\u001B[39;00m\n\u001B[1;32m    215\u001B[0m span\u001B[38;5;241m.\u001B[39mset_outputs(result)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/tracing/fluent.py:247\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _WrappingContext(fn, args, kwargs) \u001B[38;5;28;01mas\u001B[39;00m wrapping_coro:\n\u001B[0;32m--> 247\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m wrapping_coro\u001B[38;5;241m.\u001B[39msend(fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks_langchain/genie.py:116\u001B[0m, in \u001B[0;36mGenieAgent\u001B[0;34m(genie_space_id, genie_agent_name, description, include_context, message_processor, client)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrunnables\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RunnableLambda\n\u001B[0;32m--> 116\u001B[0m genie \u001B[38;5;241m=\u001B[39m Genie(genie_space_id, client\u001B[38;5;241m=\u001B[39mclient)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;66;03m# Create a partial function with the genie_space_id pre-filled\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks_ai_bridge/genie.py:111\u001B[0m, in \u001B[0;36mGenie.__init__\u001B[0;34m(self, space_id, client, truncate_results)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenie \u001B[38;5;241m=\u001B[39m workspace_client\u001B[38;5;241m.\u001B[39mgenie\n\u001B[0;32m--> 111\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdescription \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenie\u001B[38;5;241m.\u001B[39mget_space(space_id)\u001B[38;5;241m.\u001B[39mdescription\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheaders \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccept\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContent-Type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    115\u001B[0m }\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks/sdk/service/dashboards.py:1802\u001B[0m, in \u001B[0;36mGenieAPI.get_space\u001B[0;34m(self, space_id)\u001B[0m\n\u001B[1;32m   1798\u001B[0m headers \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   1799\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccept\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1800\u001B[0m }\n\u001B[0;32m-> 1802\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api\u001B[38;5;241m.\u001B[39mdo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGET\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/api/2.0/genie/spaces/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mspace_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, headers\u001B[38;5;241m=\u001B[39mheaders)\n\u001B[1;32m   1803\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m GenieSpace\u001B[38;5;241m.\u001B[39mfrom_dict(res)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks/sdk/core.py:85\u001B[0m, in \u001B[0;36mApiClient.do\u001B[0;34m(self, method, path, url, query, headers, body, raw, files, data, auth, response_headers)\u001B[0m\n\u001B[1;32m     84\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cfg\u001B[38;5;241m.\u001B[39mhost\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api_client\u001B[38;5;241m.\u001B[39mdo(\n\u001B[1;32m     86\u001B[0m     method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[1;32m     87\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m     88\u001B[0m     query\u001B[38;5;241m=\u001B[39mquery,\n\u001B[1;32m     89\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m     90\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m     91\u001B[0m     raw\u001B[38;5;241m=\u001B[39mraw,\n\u001B[1;32m     92\u001B[0m     files\u001B[38;5;241m=\u001B[39mfiles,\n\u001B[1;32m     93\u001B[0m     data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[1;32m     94\u001B[0m     auth\u001B[38;5;241m=\u001B[39mauth,\n\u001B[1;32m     95\u001B[0m     response_headers\u001B[38;5;241m=\u001B[39mresponse_headers,\n\u001B[1;32m     96\u001B[0m )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks/sdk/_base_client.py:196\u001B[0m, in \u001B[0;36m_BaseClient.do\u001B[0;34m(self, method, url, query, headers, body, raw, files, data, auth, response_headers)\u001B[0m\n\u001B[1;32m    194\u001B[0m     call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_perform\n\u001B[0;32m--> 196\u001B[0m response \u001B[38;5;241m=\u001B[39m call(\n\u001B[1;32m    197\u001B[0m     method,\n\u001B[1;32m    198\u001B[0m     url,\n\u001B[1;32m    199\u001B[0m     query\u001B[38;5;241m=\u001B[39mquery,\n\u001B[1;32m    200\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    201\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m    202\u001B[0m     raw\u001B[38;5;241m=\u001B[39mraw,\n\u001B[1;32m    203\u001B[0m     files\u001B[38;5;241m=\u001B[39mfiles,\n\u001B[1;32m    204\u001B[0m     data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[1;32m    205\u001B[0m     auth\u001B[38;5;241m=\u001B[39mauth,\n\u001B[1;32m    206\u001B[0m )\n\u001B[1;32m    208\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks/sdk/retries.py:57\u001B[0m, in \u001B[0;36mretried.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retry_reason \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;66;03m# raise if exception is not retryable\u001B[39;00m\n\u001B[0;32m---> 57\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m     59\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetrying: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mretry_reason\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (sleeping ~\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msleep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks/sdk/retries.py:36\u001B[0m, in \u001B[0;36mretried.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/databricks/sdk/_base_client.py:298\u001B[0m, in \u001B[0;36m_BaseClient._perform\u001B[0;34m(self, method, url, query, headers, body, raw, files, data, auth)\u001B[0m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 298\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
        "\u001B[0;31mPermissionDenied\u001B[0m: You need \"Can View\" permission to perform this action. Config: host=https://dbc-daae04e5-a828.cloud.databricks.com, token=***, auth_type=pat. Env: DATABRICKS_HOST, DATABRICKS_TOKEN",
        "\nThe above exception was the direct cause of the following exception:\n",
        "\u001B[0;31mMlflowException\u001B[0m                           Traceback (most recent call last)",
        "File \u001B[0;32m<command-5737137237918700>, line 25\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Use pyfunc log_model with the file path (models-from-code approach)\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mstart_run(run_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madtech_chat_history_agent_postgres_genie_pydantic\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m---> 25\u001B[0m     logged_chain_info \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mpyfunc\u001B[38;5;241m.\u001B[39mlog_model(\n\u001B[1;32m     26\u001B[0m         python_model\u001B[38;5;241m=\u001B[39mchain_file_path,  \u001B[38;5;66;03m# Path to the chain file\u001B[39;00m\n\u001B[1;32m     27\u001B[0m         model_config\u001B[38;5;241m=\u001B[39mchain_config,\n\u001B[1;32m     28\u001B[0m         artifact_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchat_agent_pydantic\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     29\u001B[0m         input_example\u001B[38;5;241m=\u001B[39minput_example,  \u001B[38;5;66;03m# Schema auto-inferred from List[ChatRequest] type hint\u001B[39;00m\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;66;03m# Specify resources for automatic authentication passthrough\u001B[39;00m\n\u001B[1;32m     31\u001B[0m         resources\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m     32\u001B[0m             DatabricksServingEndpoint(endpoint_name\u001B[38;5;241m=\u001B[39mmodel_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllm_model_serving_endpoint_name\u001B[39m\u001B[38;5;124m\"\u001B[39m)),\n\u001B[1;32m     33\u001B[0m             DatabricksGenieSpace(genie_space_id\u001B[38;5;241m=\u001B[39mmodel_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenie_space_id\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     34\u001B[0m         ],\n\u001B[1;32m     35\u001B[0m         pip_requirements\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m     36\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlflow==3.2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     37\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatabricks-agents==1.4.0\u001B[39m\u001B[38;5;124m\"\u001B[39m, \n\u001B[1;32m     38\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatabricks-langchain==0.7.1\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     39\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlangchain==0.3.27\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     40\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpgvector==0.2.5\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     41\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpsycopg2-binary==2.9.7\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     42\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpydantic==2.11.7\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     43\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqlalchemy==2.0.43\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     44\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtornado==6.3.2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     45\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlanggraph==0.3.4\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     46\u001B[0m         ]\n\u001B[1;32m     47\u001B[0m     )\n\u001B[1;32m     49\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchat_history_agent_postgres_genie\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     50\u001B[0m MODEL_NAME_FQN \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget_catalog\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget_schema\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/tracing/provider.py:465\u001B[0m, in \u001B[0;36mtrace_disabled.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    464\u001B[0m     is_func_called \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 465\u001B[0m     result \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    466\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    467\u001B[0m     enable()\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/pyfunc/__init__.py:3602\u001B[0m, in \u001B[0;36mlog_model\u001B[0;34m(artifact_path, loader_module, data_path, code_paths, infer_code_paths, conda_env, python_model, artifacts, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, metadata, model_config, streamable, resources, auth_policy, prompts, name, params, tags, model_type, step, model_id)\u001B[0m\n\u001B[1;32m   3371\u001B[0m \u001B[38;5;129m@format_docstring\u001B[39m(LOG_MODEL_PARAM_DOCS\u001B[38;5;241m.\u001B[39mformat(package_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscikit-learn\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   3372\u001B[0m \u001B[38;5;129m@trace_disabled\u001B[39m  \u001B[38;5;66;03m# Suppress traces for internal predict calls while logging model\u001B[39;00m\n\u001B[1;32m   3373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlog_model\u001B[39m(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3399\u001B[0m     model_id: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   3400\u001B[0m ):\n\u001B[1;32m   3401\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3402\u001B[0m \u001B[38;5;124;03m    Log a Pyfunc model with custom inference logic and optional data dependencies as an MLflow\u001B[39;00m\n\u001B[1;32m   3403\u001B[0m \u001B[38;5;124;03m    artifact for the current run.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3600\u001B[0m \u001B[38;5;124;03m        metadata of the logged model.\u001B[39;00m\n\u001B[1;32m   3601\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Model\u001B[38;5;241m.\u001B[39mlog(\n\u001B[1;32m   3603\u001B[0m         artifact_path\u001B[38;5;241m=\u001B[39martifact_path,\n\u001B[1;32m   3604\u001B[0m         name\u001B[38;5;241m=\u001B[39mname,\n\u001B[1;32m   3605\u001B[0m         flavor\u001B[38;5;241m=\u001B[39mmlflow\u001B[38;5;241m.\u001B[39mpyfunc,\n\u001B[1;32m   3606\u001B[0m         loader_module\u001B[38;5;241m=\u001B[39mloader_module,\n\u001B[1;32m   3607\u001B[0m         data_path\u001B[38;5;241m=\u001B[39mdata_path,\n\u001B[1;32m   3608\u001B[0m         code_paths\u001B[38;5;241m=\u001B[39mcode_paths,\n\u001B[1;32m   3609\u001B[0m         python_model\u001B[38;5;241m=\u001B[39mpython_model,\n\u001B[1;32m   3610\u001B[0m         artifacts\u001B[38;5;241m=\u001B[39martifacts,\n\u001B[1;32m   3611\u001B[0m         conda_env\u001B[38;5;241m=\u001B[39mconda_env,\n\u001B[1;32m   3612\u001B[0m         registered_model_name\u001B[38;5;241m=\u001B[39mregistered_model_name,\n\u001B[1;32m   3613\u001B[0m         signature\u001B[38;5;241m=\u001B[39msignature,\n\u001B[1;32m   3614\u001B[0m         input_example\u001B[38;5;241m=\u001B[39minput_example,\n\u001B[1;32m   3615\u001B[0m         await_registration_for\u001B[38;5;241m=\u001B[39mawait_registration_for,\n\u001B[1;32m   3616\u001B[0m         pip_requirements\u001B[38;5;241m=\u001B[39mpip_requirements,\n\u001B[1;32m   3617\u001B[0m         extra_pip_requirements\u001B[38;5;241m=\u001B[39mextra_pip_requirements,\n\u001B[1;32m   3618\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n\u001B[1;32m   3619\u001B[0m         prompts\u001B[38;5;241m=\u001B[39mprompts,\n\u001B[1;32m   3620\u001B[0m         model_config\u001B[38;5;241m=\u001B[39mmodel_config,\n\u001B[1;32m   3621\u001B[0m         streamable\u001B[38;5;241m=\u001B[39mstreamable,\n\u001B[1;32m   3622\u001B[0m         resources\u001B[38;5;241m=\u001B[39mresources,\n\u001B[1;32m   3623\u001B[0m         infer_code_paths\u001B[38;5;241m=\u001B[39minfer_code_paths,\n\u001B[1;32m   3624\u001B[0m         auth_policy\u001B[38;5;241m=\u001B[39mauth_policy,\n\u001B[1;32m   3625\u001B[0m         params\u001B[38;5;241m=\u001B[39mparams,\n\u001B[1;32m   3626\u001B[0m         tags\u001B[38;5;241m=\u001B[39mtags,\n\u001B[1;32m   3627\u001B[0m         model_type\u001B[38;5;241m=\u001B[39mmodel_type,\n\u001B[1;32m   3628\u001B[0m         step\u001B[38;5;241m=\u001B[39mstep,\n\u001B[1;32m   3629\u001B[0m         model_id\u001B[38;5;241m=\u001B[39mmodel_id,\n\u001B[1;32m   3630\u001B[0m     )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/models/model.py:1214\u001B[0m, in \u001B[0;36mModel.log\u001B[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, name, model_type, params, tags, step, model_id, **kwargs)\u001B[0m\n\u001B[1;32m   1202\u001B[0m     prompts \u001B[38;5;241m=\u001B[39m [pr\u001B[38;5;241m.\u001B[39muri \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pr, PromptVersion) \u001B[38;5;28;01melse\u001B[39;00m pr \u001B[38;5;28;01mfor\u001B[39;00m pr \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m   1204\u001B[0m mlflow_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\n\u001B[1;32m   1205\u001B[0m     artifact_path\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39martifact_location,\n\u001B[1;32m   1206\u001B[0m     model_uuid\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mmodel_id,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1212\u001B[0m     model_id\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mmodel_id,\n\u001B[1;32m   1213\u001B[0m )\n\u001B[0;32m-> 1214\u001B[0m flavor\u001B[38;5;241m.\u001B[39msave_model(path\u001B[38;5;241m=\u001B[39mlocal_path, mlflow_model\u001B[38;5;241m=\u001B[39mmlflow_model, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1215\u001B[0m \u001B[38;5;66;03m# `save_model` calls `load_model` to infer the model requirements, which may result\u001B[39;00m\n\u001B[1;32m   1216\u001B[0m \u001B[38;5;66;03m# in __pycache__ directories being created in the model directory.\u001B[39;00m\n\u001B[1;32m   1217\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pycache \u001B[38;5;129;01min\u001B[39;00m Path(local_path)\u001B[38;5;241m.\u001B[39mrglob(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__pycache__\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/tracing/provider.py:470\u001B[0m, in \u001B[0;36mtrace_disabled.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    468\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    469\u001B[0m         is_func_called \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 470\u001B[0m         result \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    471\u001B[0m \u001B[38;5;66;03m# We should only catch the exception from disable() and enable()\u001B[39;00m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;66;03m# and let other exceptions propagate.\u001B[39;00m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m MlflowTracingException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/pyfunc/__init__.py:3056\u001B[0m, in \u001B[0;36msave_model\u001B[0;34m(path, loader_module, data_path, code_paths, infer_code_paths, conda_env, mlflow_model, python_model, artifacts, signature, input_example, pip_requirements, extra_pip_requirements, metadata, model_config, streamable, resources, auth_policy, **kwargs)\u001B[0m\n\u001B[1;32m   3054\u001B[0m     model_code_path \u001B[38;5;241m=\u001B[39m _validate_and_get_model_code_path(python_model, temp_dir)\n\u001B[1;32m   3055\u001B[0m     _validate_and_copy_file_to_directory(model_code_path, path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcode\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 3056\u001B[0m     python_model \u001B[38;5;241m=\u001B[39m _load_model_code_path(model_code_path, model_config)\n\u001B[1;32m   3058\u001B[0m _validate_function_python_model(python_model)\n\u001B[1;32m   3059\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(python_model) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\n\u001B[1;32m   3060\u001B[0m     a \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m (input_example, pip_requirements, extra_pip_requirements)\n\u001B[1;32m   3061\u001B[0m ):\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-ef41dd14-648a-488c-8839-507026b1674b/lib/python3.12/site-packages/mlflow/models/utils.py:1966\u001B[0m, in \u001B[0;36m_load_model_code_path\u001B[0;34m(code_path, model_config)\u001B[0m\n\u001B[1;32m   1962\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n\u001B[1;32m   1963\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import code model from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcode_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m!s}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1964\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1965\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1966\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n\u001B[1;32m   1967\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to run user code from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcode_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1968\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m!s}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1969\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReview the stack trace for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1970\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1972\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m__mlflow_model__ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1973\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n\u001B[1;32m   1974\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf the model is logged as code, ensure the model is set using \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1975\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlflow.models.set_model() within the code file code file.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1976\u001B[0m     )\n",
        "\u001B[0;31mMlflowException\u001B[0m: Failed to run user code from /Workspace/Users/tanner.wendland@databricks.com/.bundle/tannerw_adtech_demo/dev/files/data_pipelines/src/chain_postgres_genie.py. Error: You need \"Can View\" permission to perform this action. Config: host=https://dbc-daae04e5-a828.cloud.databricks.com, token=***, auth_type=pat. Env: DATABRICKS_HOST, DATABRICKS_TOKEN. Review the stack trace for more information."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlflow.models.resources import DatabricksVectorSearchIndex, DatabricksServingEndpoint, DatabricksGenieSpace\n",
    "\n",
    "chain_file_path = os.path.join(os.getcwd(), 'chain_postgres_genie.py')\n",
    "if not os.path.exists(chain_file_path):\n",
    "    raise FileNotFoundError(f\"Chain file not found at {chain_file_path}\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "print(f\"Workspace URL: {workspace_url}\")\n",
    "os.environ['DATABRICKS_HOST'] = f\"https://{workspace_url}\"\n",
    "os.environ['DATABRICKS_TOKEN'] = dbutils.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# Pydantic-based input example - schema will be auto-inferred from type hints\n",
    "# Note: MLflow pyfunc expects List[InputType], so wrap in a list\n",
    "input_example = [\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What was my chat history idea?\"}],\n",
    "        # No custom_inputs to keep it optional\n",
    "    }\n",
    "]\n",
    "\n",
    "model_config = mlflow.models.ModelConfig(development_config=chain_config)\n",
    "\n",
    "# Use pyfunc log_model with the file path (models-from-code approach)\n",
    "with mlflow.start_run(run_name=\"adtech_chat_history_agent_postgres_genie_pydantic\"):\n",
    "    logged_chain_info = mlflow.pyfunc.log_model(\n",
    "        python_model=chain_file_path,  # Path to the chain file\n",
    "        model_config=chain_config,\n",
    "        artifact_path=\"chat_agent_pydantic\",\n",
    "        input_example=input_example,  # Schema auto-inferred from List[ChatRequest] type hint\n",
    "        # Specify resources for automatic authentication passthrough\n",
    "        resources=[\n",
    "            DatabricksServingEndpoint(endpoint_name=model_config.get(\"llm_model_serving_endpoint_name\")),\n",
    "            DatabricksGenieSpace(genie_space_id=model_config.get(\"genie_space_id\"))\n",
    "        ],\n",
    "        pip_requirements=[\n",
    "            \"mlflow==3.2.0\",\n",
    "            \"databricks-agents==1.4.0\", \n",
    "            \"databricks-langchain==0.7.1\",\n",
    "            \"langchain==0.3.27\",\n",
    "            \"pgvector==0.2.5\",\n",
    "            \"psycopg2-binary==2.9.7\",\n",
    "            \"pydantic==2.11.7\",\n",
    "            \"sqlalchemy==2.0.43\",\n",
    "            \"tornado==6.3.2\",\n",
    "            \"langgraph==0.3.4\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model_name = \"chat_history_agent_postgres_genie\"\n",
    "MODEL_NAME_FQN = f\"{target_catalog}.{target_schema}.{model_name}\"\n",
    "# Register to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=MODEL_NAME_FQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d1e4f2-7b64-4eb4-97af-4f10df379539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AGENT = mlflow.pyfunc.load_model(logged_chain_info.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4a985e5-e27b-41c8-a705-82ed8d1ebb9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Document Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3300a0-9b8e-40a5-a8fd-fd3ae12ca393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test with custom k parameter using pydantic structure\n",
    "# Note: Wrap in list because MLflow pyfunc expects List[ChatRequest]\n",
    "test_input_custom_k = [{\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What was my chat history idea?\"}],\n",
    "    \"custom_inputs\": {\n",
    "        \"filters\": {\"user_name\": \"tanner.wendland@databricks.com\"},\n",
    "        \"k\": 10  # Request 10 records instead of default 3\n",
    "    }\n",
    "}]\n",
    "\n",
    "answer = AGENT.predict(test_input_custom_k)\n",
    "print(\"=== Test with custom k=10 ===\")\n",
    "print(answer)\n",
    "\n",
    "# Test with default k (no custom_inputs provided)\n",
    "test_input_default = [{\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What was my chat history idea?\"}]\n",
    "    # No custom_inputs - should use defaults\n",
    "}]\n",
    "\n",
    "answer_default = AGENT.predict(test_input_default)\n",
    "print(\"\\n=== Test with no custom_inputs (should use defaults) ===\")\n",
    "print(answer_default)\n",
    "\n",
    "# Test with required user_name but no k (should use default k)\n",
    "test_input_required_user = [{\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What was my chat history idea?\"}],\n",
    "    \"custom_inputs\": {\n",
    "        \"filters\": {\"user_name\": \"tanner.wendland@databricks.com\"}\n",
    "        # No k specified - should use default from model_config\n",
    "    }\n",
    "}]\n",
    "\n",
    "answer_required = AGENT.predict(test_input_required_user)\n",
    "print(\"\\n=== Test with required user_name but default k ===\")\n",
    "print(answer_required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0141fcd2-95ff-4d57-bfa4-3c55a38f6946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9639d4-1d86-40b3-87b4-b46c8b7be046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.widgets.text(\"secert_scope\", \"field-eng\", \"Secret Scope\")\n",
    "dbutils.widgets.text(\"secret_key\", \"app-secret\", \"Secret Key\")\n",
    "dbutils.widgets.text(\"permission_group\", \"Tanner W Adtech DB Access Role\", \"Permission Group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93d99ca7-c565-4092-9f28-aff21fcebfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "secret_scope = dbutils.widgets.get(\"secert_scope\")\n",
    "secret_key = dbutils.widgets.get(\"secret_key\")\n",
    "permission_group = dbutils.widgets.get(\"permission_group\")\n",
    "\n",
    "secret_value = dbutils.secrets.get(scope=secret_scope, key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbed8a02-2931-4a9f-aa13-e3f2766bf1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedEntityInput, AiGatewayConfig, ServingEndpointAccessControlRequest, ServingEndpointPermissionLevel\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "\n",
    "version = uc_registered_model_info.version\n",
    "serving_endpoint_name = model_name.replace(\".\", \"-\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "config = {\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "                \"name\": serving_endpoint_name,\n",
    "                \"entity_name\": MODEL_NAME_FQN,\n",
    "                \"entity_version\": version,\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": True,\n",
    "                \"environment_vars\": {\n",
    "                    'DATABRICKS_HOST': workspace_url,\n",
    "                    'DATABRICKS_TOKEN': secret_value\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "ai_gateway_config = {\n",
    "        'inference_table_config': {\n",
    "            'enabled': True,\n",
    "            'catalog_name': target_catalog,\n",
    "            'schema_name': target_schema,\n",
    "            'table_name': \"chat_history_agent_postgres_genie_inference\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "def does_endpoint_exists(endpoint_name):\n",
    "    try:\n",
    "        workspace_client.serving_endpoints.get(endpoint_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "        \n",
    "endpoint = None\n",
    "if not does_endpoint_exists(serving_endpoint_name):\n",
    "    print(f\"Creating endpoint {serving_endpoint_name}...\")\n",
    "    endpoint = workspace_client.serving_endpoints.create_and_wait(\n",
    "        serving_endpoint_name,\n",
    "        config=EndpointCoreConfigInput.from_dict(config),\n",
    "        ai_gateway=AiGatewayConfig.from_dict(ai_gateway_config)\n",
    "    )\n",
    "else:\n",
    "    print(f\"Updating endpoint {serving_endpoint_name}...\")\n",
    "    endpoint = workspace_client.serving_endpoints.update_config_and_wait(\n",
    "        serving_endpoint_name,\n",
    "        served_entities=[ServedEntityInput.from_dict(entity) for entity in config['served_entities']]\n",
    "    )\n",
    "\n",
    "# Grant permissions to the specified group after endpoint deployment\n",
    "print(f\"Granting permissions to group: {permission_group}\")\n",
    "try:\n",
    "    workspace_client.serving_endpoints.set_permissions(\n",
    "        serving_endpoint_id=endpoint.id,\n",
    "        access_control_list=[\n",
    "            ServingEndpointAccessControlRequest(group_name=permission_group, permission_level=ServingEndpointPermissionLevel.CAN_QUERY)\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Successfully granted CAN_QUERY permission to group: {permission_group}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to grant permissions to group {permission_group}: {str(e)}\")\n",
    "    print(\"Please manually grant permissions to the endpoint if needed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02.2-chat-history-agent-pg-genie",
   "widgets": {
    "database_instance_name": {
     "currentValue": "tannerw-adtech-db",
     "nuid": "5935864b-337f-4cea-978f-00fcf2a24646",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tannerw-adtech-db",
      "label": null,
      "name": "database_instance_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "tannerw-adtech-db",
      "label": null,
      "name": "database_instance_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "embedding_model": {
     "currentValue": "databricks-gte-large-en",
     "nuid": "96111585-d614-405c-960e-ae589ccc89fc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_model",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_model",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "genie_space_id": {
     "currentValue": "01efcca6fdc712d7be87a40ad4a2e33e",
     "nuid": "ac774ec2-f551-4a54-94f9-5c547df32515",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "01efcca6fdc712d7be87a40ad4a2e33e",
      "label": null,
      "name": "genie_space_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "01efcca6fdc712d7be87a40ad4a2e33e",
      "label": null,
      "name": "genie_space_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "llm_model_serving_endpoint_name": {
     "currentValue": "databricks-claude-3-7-sonnet",
     "nuid": "4e5299b7-86bb-4511-bd44-8d76be56893e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-claude-3-7-sonnet",
      "label": null,
      "name": "llm_model_serving_endpoint_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-claude-3-7-sonnet",
      "label": null,
      "name": "llm_model_serving_endpoint_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "permission_group": {
     "currentValue": "Tanner W Adtech DB Access Role",
     "nuid": "fc8d7e9a-8c23-41e1-affe-706c0f79c5e7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Tanner W Adtech DB Access Role",
      "label": "Permission Group",
      "name": "permission_group",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Tanner W Adtech DB Access Role",
      "label": "Permission Group",
      "name": "permission_group",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "postgres_database_name": {
     "currentValue": "databricks_postgres",
     "nuid": "e75a0daa-dd0b-412d-b393-29edb1e3b1f1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks_postgres",
      "label": null,
      "name": "postgres_database_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks_postgres",
      "label": null,
      "name": "postgres_database_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "secert_scope": {
     "currentValue": "field-eng",
     "nuid": "e227bce8-9119-4ce0-9bd1-7bb37e315a10",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "field-eng",
      "label": "Secret Scope",
      "name": "secert_scope",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "field-eng",
      "label": "Secret Scope",
      "name": "secert_scope",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "secret_key": {
     "currentValue": "app-secret",
     "nuid": "90c15fd3-85b5-4532-9fb1-cd73d8f32360",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "app-secret",
      "label": "Secret Key",
      "name": "secret_key",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "app-secret",
      "label": "Secret Key",
      "name": "secret_key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_catalog": {
     "currentValue": "tanner_wendland",
     "nuid": "9ce71cf8-98af-4e13-8cc0-71dc8c6cf387",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tanner_wendland",
      "label": null,
      "name": "target_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "tanner_wendland",
      "label": null,
      "name": "target_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_schema": {
     "currentValue": "default",
     "nuid": "eb61b685-72df-4bd5-bf4e-80e589c9adb5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "target_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": null,
      "name": "target_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}